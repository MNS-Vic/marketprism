# MarketPrism 架构调整计划
*制定时间：2025年1月*

## 📋 项目背景

### 已完成成果
通过4阶段代码整合，MarketPrism已成功完成核心组件统一：
- **代码减少**：4756行 → 2200+行 (-2300+行，-51%)
- **文件减少**：15个 → 3个 (-80%)
- **重复代码消除**：~90%
- **测试验证**：49/49个测试用例全部通过 (100%成功率)

### 统一核心组件
- `core/networking/unified_session_manager.py` - 统一会话管理
- `core/storage/unified_clickhouse_writer.py` - 统一ClickHouse写入
- `core/storage/unified_storage_manager.py` - 统一存储管理

## 🎯 目标架构

### 微服务架构设计

#### 核心业务服务 (3个)
1. **market-data-collector** - 数据采集与标准化
   - 职责：多交易所API数据采集、格式标准化、实时推送
   - 技术：异步HTTP客户端、WebSocket、数据验证

2. **data-storage-service** - 统一存储服务
   - 职责：热冷数据管理、查询路由、数据生命周期
   - 技术：ClickHouse集群、存储分层、查询优化

3. **api-gateway-service** - 统一API网关
   - 职责：外部API入口、认证授权、路由负载均衡
   - 技术：智能路由、限流熔断、监控告警

#### 基础设施服务 (3个)  
4. **monitoring-service** - 独立监控平台
   - 职责：指标收集、告警管理、可视化仪表板
   - 技术：Prometheus、Grafana、AlertManager

5. **message-broker-service** - 消息中间件
   - 职责：异步消息传递、事件总线、消息持久化
   - 技术：NATS/Kafka、消息路由、故障恢复

6. **scheduler-service** - 分布式任务调度
   - 职责：定时任务、数据归档、清理维护、系统检查
   - 技术：分布式锁、任务队列、故障转移

### 混合云存储架构
- **云端部署**：热数据存储 (ClickHouse集群)
- **本地NAS**：冷数据存储 (成本优化76%)
- **连接方式**：VPN隧道 (WireGuard) + 智能数据迁移
- **查询路由**：热数据→云端，冷数据→本地，混合→组合查询

## 🚀 实施计划

### Phase 1: 基础服务重构 (2周)
**目标**：建立核心微服务框架

#### Week 1: 服务框架搭建
- [ ] 创建服务目录结构
  ```
  services/
  ├── market-data-collector/
  ├── data-storage-service/  
  ├── api-gateway-service/
  ├── monitoring-service/
  ├── message-broker-service/
  └── scheduler-service/
  ```
- [ ] 统一服务基础框架
  - 服务注册发现
  - 健康检查接口
  - 配置管理集成
  - 日志结构化
- [ ] 建立服务间通信协议
  - REST API规范
  - 消息队列协议  
  - 错误处理标准

#### Week 2: 核心服务迁移
- [ ] **data-storage-service** 优先级最高
  - 基于 `unified_storage_manager.py`
  - 集成热冷数据管理
  - 实现查询路由逻辑
- [ ] **scheduler-service** 
  - 迁移 `services/data_archiver` 调度逻辑
  - 实现分布式任务协调
  - 集成监控和故障恢复
- [ ] 服务间集成测试

### Phase 2: 数据采集与网关 (2周)

#### Week 3: 数据采集服务
- [ ] **market-data-collector** 
  - 迁移现有采集器逻辑
  - 实现多交易所适配器
  - 集成 `unified_session_manager.py`
  - 数据标准化管道
- [ ] 数据流测试验证

#### Week 4: API网关服务  
- [ ] **api-gateway-service**
  - 实现智能路由
  - 集成认证授权
  - 限流熔断机制
  - 监控埋点
- [ ] 端到端API测试

### Phase 3: 基础设施与监控 (2周)

#### Week 5: 消息与监控
- [ ] **message-broker-service**
  - NATS集群部署
  - 消息路由配置
  - 故障恢复机制
- [ ] **monitoring-service**  
  - Prometheus集群
  - Grafana仪表板
  - 告警规则配置

#### Week 6: 混合云存储
- [ ] VPN隧道建立 (WireGuard)
- [ ] 本地NAS存储配置
- [ ] 数据迁移策略实现
- [ ] 智能查询路由测试

### Phase 4: 优化与上线 (1周)

#### Week 7: 性能优化与部署
- [ ] 性能基准测试
- [ ] 容器化部署 (Docker)
- [ ] CI/CD管道建立
- [ ] 生产环境部署
- [ ] 监控告警验证

## 📊 预期收益

### 技术收益
- **可维护性**：单一职责，模块化设计
- **可扩展性**：水平扩展，按需伸缩  
- **可靠性**：故障隔离，自动恢复
- **性能**：分布式处理，并行优化

### 成本收益  
- **存储成本**：76%降低 (¥3000→¥800/月)
- **运维成本**：自动化程度提升80%
- **开发效率**：重复代码减少90%

### 业务收益
- **数据处理能力**：10倍提升
- **查询响应时间**：60%缩短
- **系统可用性**：99.9%+
- **扩展新交易所**：开发时间减少70%

## 🎛️ 风险控制

### 技术风险
- **服务拆分复杂度**：渐进式迁移，保持向后兼容
- **网络延迟**：智能缓存，本地降级
- **数据一致性**：事务补偿，幂等设计

### 运维风险  
- **部署复杂度**：容器化，自动化部署
- **监控盲区**：全链路监控，主动告警
- **故障恢复**：自动故障转移，人工介入备案

### 业务风险
- **服务中断**：蓝绿部署，灰度发布
- **数据丢失**：多副本备份，实时同步验证
- **性能下降**：基准测试，性能监控

## 📋 关键里程碑

| 时间节点 | 里程碑 | 成功标准 |
|---------|--------|----------|
| Week 2  | 核心存储服务上线 | 数据读写功能正常，性能基准达标 |
| Week 4  | API网关服务上线 | 外部API访问正常，限流熔断生效 |
| Week 6  | 混合云存储就绪 | 热冷数据迁移验证，查询路由正常 |
| Week 7  | 完整系统上线 | 所有服务健康，监控告警正常 |

## 🔧 配套工具

### 开发工具
- **服务框架**：FastAPI + Pydantic
- **消息队列**：NATS Streaming  
- **服务发现**：Consul
- **配置管理**：统一配置中心

### 运维工具
- **容器化**：Docker + Docker Compose
- **编排**：Kubernetes (可选)
- **CI/CD**：GitHub Actions
- **监控**：Prometheus + Grafana + AlertManager

### 测试工具
- **单元测试**：pytest + coverage
- **集成测试**：testcontainers
- **性能测试**：locust
- **端到端测试**：自动化测试套件

---

## ✅ 下一步行动

1. **立即开始Phase 1**：创建服务目录结构
2. **团队沟通**：架构评审，确认实施细节  
3. **环境准备**：开发测试环境搭建
4. **监控建立**：项目进度跟踪机制

*此计划将根据实施过程中的反馈和发现持续优化调整。*