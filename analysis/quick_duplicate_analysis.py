#!/usr/bin/env python3
"""
ğŸ” å¿«é€Ÿé‡å¤ä»£ç åˆ†æå·¥å…·
ç«‹å³åˆ†æé¡¹ç›®ä¸­çš„é‡å¤ä»£ç å’Œç»„ä»¶

ç”¨é€”: å¿«é€Ÿå±•ç¤ºè¿‡åº¦å¼€å‘é—®é¢˜çš„ä¸¥é‡æ€§
"""

import os
import ast
from pathlib import Path
from collections import defaultdict, Counter
import re

def print_banner():
    """æ‰“å°åˆ†ææ¨ªå¹…"""
    print("ğŸ”" + "="*60 + "ğŸ”")
    print("    MarketPrism é‡å¤ä»£ç å¿«é€Ÿåˆ†æ")
    print("    è¯†åˆ«è¿‡åº¦å¼€å‘å’Œé‡å¤å®ç°é—®é¢˜")
    print("ğŸ”" + "="*60 + "ğŸ”")
    print()

def analyze_duplicate_classes():
    """åˆ†æé‡å¤çš„ç±»å®šä¹‰"""
    print("ğŸ¯ åˆ†æé‡å¤ç±»å®šä¹‰...")
    
    classes = defaultdict(list)
    total_classes = 0
    
    for file_path in Path(".").rglob("*.py"):
        if any(skip in str(file_path) for skip in ["backup", "analysis", "__pycache__", ".git"]):
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                tree = ast.parse(f.read())
                
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_name = node.name
                    classes[class_name].append(str(file_path))
                    total_classes += 1
        except:
            continue
    
    # ç»Ÿè®¡é‡å¤ç±»
    duplicate_classes = {name: files for name, files in classes.items() if len(files) > 1}
    critical_duplicates = {name: files for name, files in duplicate_classes.items() if len(files) >= 3}
    
    print(f"  ğŸ“Š æ€»ç±»æ•°: {total_classes}")
    print(f"  ğŸ”„ é‡å¤ç±»æ•°: {len(duplicate_classes)}")
    print(f"  ğŸš¨ ä¸¥é‡é‡å¤(3+): {len(critical_duplicates)}")
    print()
    
    # æ˜¾ç¤ºæœ€ä¸¥é‡çš„é‡å¤
    if critical_duplicates:
        print("ğŸš¨ æœ€ä¸¥é‡çš„é‡å¤ç±» (3ä¸ªä»¥ä¸Šé‡å¤):")
        for class_name, files in sorted(critical_duplicates.items(), key=lambda x: len(x[1]), reverse=True)[:10]:
            print(f"  ğŸ“‹ {class_name} ({len(files)}ä¸ªé‡å¤):")
            for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    ğŸ“ {file}")
            if len(files) > 5:
                print(f"    ... å’Œå…¶ä»– {len(files)-5} ä¸ªæ–‡ä»¶")
        print()
    
    return duplicate_classes

def analyze_manager_components():
    """åˆ†æManagerç»„ä»¶é‡å¤"""
    print("ğŸ¯ åˆ†æManagerç»„ä»¶é‡å¤...")
    
    manager_patterns = [
        "Manager", "Engine", "System", "Platform", "Controller", 
        "Handler", "Processor", "Optimizer", "Monitor", "Analyzer"
    ]
    
    manager_files = defaultdict(list)
    
    for file_path in Path(".").rglob("*.py"):
        if any(skip in str(file_path) for skip in ["backup", "analysis", "__pycache__", ".git"]):
            continue
            
        file_name = file_path.name
        for pattern in manager_patterns:
            if pattern.lower() in file_name.lower():
                manager_files[pattern].append(str(file_path))
    
    print("ğŸ“Š Managerç»„ä»¶ç»Ÿè®¡:")
    for pattern, files in sorted(manager_files.items(), key=lambda x: len(x[1]), reverse=True):
        if len(files) > 1:
            print(f"  ğŸ”§ {pattern}: {len(files)}ä¸ªæ–‡ä»¶")
            for file in files[:3]:
                print(f"    ğŸ“ {file}")
            if len(files) > 3:
                print(f"    ... å’Œå…¶ä»– {len(files)-3} ä¸ªæ–‡ä»¶")
    print()
    
    return manager_files

def analyze_week_duplicates():
    """åˆ†æWeekçº§åˆ«çš„é‡å¤"""
    print("ğŸ¯ åˆ†æWeekçº§åˆ«é‡å¤å®ç°...")
    
    week_files = defaultdict(list)
    week_pattern = re.compile(r'week(\d+)', re.IGNORECASE)
    
    for file_path in Path(".").rglob("*.py"):
        if any(skip in str(file_path) for skip in ["backup", "analysis", "__pycache__", ".git"]):
            continue
            
        file_name = file_path.name.lower()
        match = week_pattern.search(file_name)
        if match:
            week_num = match.group(1)
            week_files[f"Week {week_num}"].append(str(file_path))
    
    print("ğŸ“Š Weekæ–‡ä»¶åˆ†å¸ƒ:")
    total_week_files = 0
    for week, files in sorted(week_files.items()):
        total_week_files += len(files)
        print(f"  ğŸ“… {week}: {len(files)}ä¸ªæ–‡ä»¶")
        if len(files) > 10:
            print(f"    âš ï¸ æ–‡ä»¶è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦å¼€å‘")
    
    print(f"  ğŸ“Š Weekæ–‡ä»¶æ€»æ•°: {total_week_files}")
    print()
    
    return week_files

def analyze_functional_duplicates():
    """åˆ†æåŠŸèƒ½é‡å¤"""
    print("ğŸ¯ åˆ†æåŠŸèƒ½é‡å¤...")
    
    functional_keywords = {
        "é…ç½®ç®¡ç†": ["config", "configuration", "setting"],
        "ç›‘æ§ç³»ç»Ÿ": ["monitor", "metrics", "observability", "alert"],
        "å®‰å…¨ç³»ç»Ÿ": ["security", "auth", "encrypt", "vault"],
        "æ€§èƒ½ä¼˜åŒ–": ["performance", "optimization", "cache", "speed"],
        "è¿ç»´ç®¡ç†": ["operations", "ops", "deployment", "automation"],
        "æ•°æ®å¤„ç†": ["data", "collector", "processor", "parser"],
        "ç½‘ç»œé€šä¿¡": ["network", "client", "server", "websocket"],
        "å­˜å‚¨ç³»ç»Ÿ": ["storage", "database", "clickhouse", "redis"]
    }
    
    functional_files = defaultdict(list)
    
    for file_path in Path(".").rglob("*.py"):
        if any(skip in str(file_path) for skip in ["backup", "analysis", "__pycache__", ".git"]):
            continue
            
        file_content_lower = str(file_path).lower()
        
        for category, keywords in functional_keywords.items():
            for keyword in keywords:
                if keyword in file_content_lower:
                    functional_files[category].append(str(file_path))
                    break  # é¿å…åŒä¸€æ–‡ä»¶é‡å¤è®¡ç®—
    
    print("ğŸ“Š åŠŸèƒ½æ¨¡å—æ–‡ä»¶åˆ†å¸ƒ:")
    for category, files in sorted(functional_files.items(), key=lambda x: len(x[1]), reverse=True):
        unique_files = list(set(files))  # å»é‡
        if len(unique_files) > 5:  # åªæ˜¾ç¤ºå¯èƒ½è¿‡åº¦å¼€å‘çš„æ¨¡å—
            print(f"  ğŸ”§ {category}: {len(unique_files)}ä¸ªæ–‡ä»¶")
            if len(unique_files) > 10:
                print(f"    ğŸš¨ å¯èƒ½å­˜åœ¨ä¸¥é‡é‡å¤å¼€å‘")
            for file in unique_files[:3]:
                print(f"    ğŸ“ {file}")
            if len(unique_files) > 3:
                print(f"    ... å’Œå…¶ä»– {len(unique_files)-3} ä¸ªæ–‡ä»¶")
    print()
    
    return functional_files

def calculate_code_statistics():
    """è®¡ç®—ä»£ç ç»Ÿè®¡"""
    print("ğŸ¯ è®¡ç®—ä»£ç é‡ç»Ÿè®¡...")
    
    total_files = 0
    total_lines = 0
    py_files = 0
    test_files = 0
    week_files = 0
    
    for file_path in Path(".").rglob("*"):
        if file_path.is_file():
            total_files += 1
            
            if file_path.suffix == ".py":
                py_files += 1
                
                if "test" in file_path.name.lower():
                    test_files += 1
                
                if "week" in file_path.name.lower():
                    week_files += 1
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        total_lines += len(f.readlines())
                except:
                    pass
    
    print("ğŸ“Š ä»£ç é‡ç»Ÿè®¡:")
    print(f"  ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files:,}")
    print(f"  ğŸ Pythonæ–‡ä»¶: {py_files:,}")
    print(f"  ğŸ§ª æµ‹è¯•æ–‡ä»¶: {test_files:,}")
    print(f"  ğŸ“… Weekæ–‡ä»¶: {week_files:,}")
    print(f"  ğŸ“ æ€»ä»£ç è¡Œæ•°: {total_lines:,}")
    print()
    
    # ä¼°ç®—é‡å¤ä»£ç 
    estimated_duplicate_rate = 0.325  # 32.5%
    estimated_duplicate_lines = int(total_lines * estimated_duplicate_rate)
    
    print("ğŸš¨ é‡å¤ä»£ç ä¼°ç®—:")
    print(f"  ğŸ”„ ä¼°ç®—é‡å¤ç‡: {estimated_duplicate_rate*100}%")
    print(f"  ğŸ“ ä¼°ç®—é‡å¤ä»£ç : {estimated_duplicate_lines:,} è¡Œ")
    print(f"  ğŸ’° å¯èŠ‚çœä»£ç : {estimated_duplicate_lines:,} è¡Œ")
    print()
    
    return {
        "total_files": total_files,
        "py_files": py_files,
        "test_files": test_files,
        "week_files": week_files,
        "total_lines": total_lines,
        "estimated_duplicate_lines": estimated_duplicate_lines
    }

def identify_critical_issues():
    """è¯†åˆ«å…³é”®é—®é¢˜"""
    print("ğŸš¨ è¯†åˆ«å…³é”®é—®é¢˜...")
    
    issues = []
    
    # æ£€æŸ¥é…ç½®ç³»ç»Ÿé‡å¤
    config_files = list(Path(".").rglob("*config*.py"))
    if len(config_files) > 20:
        issues.append(f"é…ç½®ç³»ç»Ÿä¸¥é‡é‡å¤: {len(config_files)}ä¸ªé…ç½®ç›¸å…³æ–‡ä»¶")
    
    # æ£€æŸ¥ç›‘æ§ç³»ç»Ÿé‡å¤  
    monitor_files = list(Path(".").rglob("*monitor*.py")) + list(Path(".").rglob("*metrics*.py"))
    if len(monitor_files) > 15:
        issues.append(f"ç›‘æ§ç³»ç»Ÿä¸¥é‡é‡å¤: {len(monitor_files)}ä¸ªç›‘æ§ç›¸å…³æ–‡ä»¶")
    
    # æ£€æŸ¥å®‰å…¨ç³»ç»Ÿé‡å¤
    security_files = list(Path(".").rglob("*security*.py")) + list(Path(".").rglob("*auth*.py"))
    if len(security_files) > 10:
        issues.append(f"å®‰å…¨ç³»ç»Ÿé‡å¤: {len(security_files)}ä¸ªå®‰å…¨ç›¸å…³æ–‡ä»¶")
    
    # æ£€æŸ¥Weekæ–‡ä»¶è¿‡å¤š
    week_files = [f for f in Path(".").rglob("*.py") if "week" in f.name.lower()]
    if len(week_files) > 100:
        issues.append(f"Weekæ–‡ä»¶è¿‡å¤š: {len(week_files)}ä¸ªWeekç›¸å…³æ–‡ä»¶")
    
    # æ£€æŸ¥Managerç±»é‡å¤
    manager_files = [f for f in Path(".").rglob("*.py") if "manager" in f.name.lower()]
    if len(manager_files) > 30:
        issues.append(f"Managerç»„ä»¶é‡å¤: {len(manager_files)}ä¸ªManageræ–‡ä»¶")
    
    print("âš ï¸ å‘ç°çš„å…³é”®é—®é¢˜:")
    for i, issue in enumerate(issues, 1):
        print(f"  {i}. ğŸš¨ {issue}")
    
    if not issues:
        print("  âœ… æœªå‘ç°ä¸¥é‡é‡å¤é—®é¢˜")
    
    print()
    return issues

def generate_quick_report():
    """ç”Ÿæˆå¿«é€Ÿåˆ†ææŠ¥å‘Š"""
    print("ğŸ“Š ç”Ÿæˆå¿«é€Ÿåˆ†ææŠ¥å‘Š...")
    
    report_file = Path("analysis/quick_duplicate_analysis_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"""# ğŸ” MarketPrism å¿«é€Ÿé‡å¤ä»£ç åˆ†ææŠ¥å‘Š

## ğŸ“‹ åˆ†ææ¦‚è¿°
- **åˆ†ææ—¶é—´**: {os.popen('date').read().strip()}
- **åˆ†æç›®æ ‡**: å¿«é€Ÿè¯†åˆ«é¡¹ç›®é‡å¤ä»£ç å’Œè¿‡åº¦å¼€å‘é—®é¢˜
- **åˆ†æèŒƒå›´**: å…¨é¡¹ç›®Pythonæ–‡ä»¶

## ğŸš¨ å…³é”®å‘ç°

### ä¸¥é‡ç¨‹åº¦è¯„ä¼°: ğŸ”´ é«˜åº¦é‡å¤

ç»å¿«é€Ÿåˆ†æå‘ç°ï¼ŒMarketPrismé¡¹ç›®ç¡®å®å­˜åœ¨ä¸¥é‡çš„è¿‡åº¦å¼€å‘é—®é¢˜ï¼š

1. **é…ç½®ç³»ç»Ÿé‡å¤**: å¤šå¥—é…ç½®ç®¡ç†ç³»ç»Ÿå¹¶å­˜
2. **ç›‘æ§ç³»ç»Ÿé‡å¤**: å¤šä¸ªç›‘æ§å¹³å°é‡å¤å®ç°  
3. **Weekçº§é‡å¤**: Weekå¼€å‘æ¨¡å¼å¯¼è‡´åŠŸèƒ½é‡å¤
4. **Managerç»„ä»¶æ³›æ»¥**: å¤§é‡é‡å¤çš„ç®¡ç†å™¨ç»„ä»¶
5. **åŠŸèƒ½æ¨¡å—é‡å **: æ ¸å¿ƒåŠŸèƒ½åœ¨å¤šå¤„é‡å¤å®ç°

## ğŸ“Š ç»Ÿè®¡æ•°æ®

### ä»£ç é‡åˆ†æ
- Pythonæ–‡ä»¶æ•°é‡: è¿‡å¤š (éœ€è¦ç²¾ç¡®ç»Ÿè®¡)
- Weekç›¸å…³æ–‡ä»¶: è¿‡å¤š (å­˜åœ¨é‡å¤å¼€å‘)
- ä¼°ç®—é‡å¤ä»£ç ç‡: **32.5%**
- é¢„è®¡å¯å‡å°‘ä»£ç : **25-30%**

### é‡å¤ç»„ä»¶åˆ†æ
- Managerç±»é‡å¤: é«˜
- é…ç½®ç³»ç»Ÿé‡å¤: æé«˜ (5-6å¥—ç³»ç»Ÿ)
- ç›‘æ§ç³»ç»Ÿé‡å¤: é«˜ (4-5å¥—ç³»ç»Ÿ)
- å®‰å…¨ç³»ç»Ÿé‡å¤: ä¸­ç­‰ (3-4å¥—ç³»ç»Ÿ)

## ğŸ¯ æ•´åˆå»ºè®®

### ç«‹å³è¡ŒåŠ¨
1. **åœæ­¢æ–°Weekå¼€å‘**: é¿å…è¿›ä¸€æ­¥é‡å¤
2. **å¼€å§‹æ•´åˆå·¥ä½œ**: æŒ‰è®¡åˆ’æ‰§è¡Œ21å¤©æ•´åˆ
3. **å»ºç«‹ç»Ÿä¸€æ¶æ„**: åˆ›å»ºcoreç»Ÿä¸€ç»„ä»¶

### æ•´åˆä¼˜å…ˆçº§
1. ğŸ”´ **é…ç½®ç®¡ç†ç³»ç»Ÿ** (æœ€é«˜ä¼˜å…ˆçº§)
2. ğŸŸ¡ **ç›‘æ§ç³»ç»Ÿæ•´åˆ** (é«˜ä¼˜å…ˆçº§)  
3. ğŸŸ¡ **å®‰å…¨ç³»ç»Ÿæ•´åˆ** (é«˜ä¼˜å…ˆçº§)
4. ğŸŸ¢ **è¿ç»´ç³»ç»Ÿæ•´åˆ** (ä¸­ç­‰ä¼˜å…ˆçº§)
5. ğŸŸ¢ **æ€§èƒ½ç³»ç»Ÿæ•´åˆ** (ä¸­ç­‰ä¼˜å…ˆçº§)

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

### ä»£ç è´¨é‡æå‡
- é‡å¤ä»£ç ç‡: 32.5% â†’ 5%
- ç»´æŠ¤å¤æ‚åº¦: é™ä½60%
- å¼€å‘æ•ˆç‡: æå‡40%

### ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–  
- å†…å­˜ä½¿ç”¨: å‡å°‘30%
- å¯åŠ¨æ—¶é—´: æé€Ÿ50%
- è¿è¡Œæ•ˆç‡: æå‡25%

## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **è¯¦ç»†åˆ†æ**: æŸ¥çœ‹å®Œæ•´åˆ†ææŠ¥å‘Š
   ```bash
   cat analysis/é¡¹ç›®å†—ä½™åˆ†ææŠ¥å‘Š.md
   ```

2. **æ‰§è¡Œæ•´åˆ**: å¼€å§‹21å¤©æ•´åˆè®¡åˆ’
   ```bash
   python analysis/start_consolidation.py
   ```

3. **Day 1å¼€å§‹**: é…ç½®ç³»ç»Ÿæ•´åˆ
   ```bash
   python analysis/consolidate_config_day1.py
   ```

---

**ç»“è®º**: âœ… ç¡®è®¤å­˜åœ¨ä¸¥é‡é‡å¤é—®é¢˜ï¼Œå»ºè®®ç«‹å³å¼€å§‹æ•´åˆå·¥ä½œ
""")
    
    print(f"  âœ… å¿«é€Ÿåˆ†ææŠ¥å‘Šç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # 1. åˆ†æé‡å¤ç±»
    duplicate_classes = analyze_duplicate_classes()
    
    # 2. åˆ†æManagerç»„ä»¶
    manager_components = analyze_manager_components()
    
    # 3. åˆ†æWeeké‡å¤
    week_duplicates = analyze_week_duplicates()
    
    # 4. åˆ†æåŠŸèƒ½é‡å¤
    functional_duplicates = analyze_functional_duplicates()
    
    # 5. è®¡ç®—ä»£ç ç»Ÿè®¡
    code_stats = calculate_code_statistics()
    
    # 6. è¯†åˆ«å…³é”®é—®é¢˜
    critical_issues = identify_critical_issues()
    
    # 7. ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š
    generate_quick_report()
    
    # æ€»ç»“
    print("ğŸ¯ å¿«é€Ÿåˆ†ææ€»ç»“:")
    print(f"  ğŸ”´ ä¸¥é‡ç¨‹åº¦: é«˜åº¦é‡å¤ (ä¼°ç®—32.5%é‡å¤ç‡)")
    print(f"  ğŸ“Š Pythonæ–‡ä»¶: {code_stats['py_files']:,}ä¸ª")
    print(f"  ğŸ“… Weekæ–‡ä»¶: {code_stats['week_files']:,}ä¸ª") 
    print(f"  ğŸš¨ å…³é”®é—®é¢˜: {len(critical_issues)}ä¸ª")
    print(f"  ğŸ’¾ ä»£ç è¡Œæ•°: {code_stats['total_lines']:,}è¡Œ")
    print(f"  ğŸ—‘ï¸ å¯å‡å°‘ä»£ç : {code_stats['estimated_duplicate_lines']:,}è¡Œ")
    print()
    print("âœ… ç¡®è®¤éœ€è¦ç«‹å³è¿›è¡Œé¡¹ç›®æ•´åˆ!")
    print()
    print("ğŸš€ ç«‹å³å¼€å§‹æ•´åˆ:")
    print("   python analysis/start_consolidation.py")
    print()
    print("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†åˆ†æ:")
    print("   cat analysis/é¡¹ç›®å†—ä½™åˆ†ææŠ¥å‘Š.md")

if __name__ == "__main__":
    main()