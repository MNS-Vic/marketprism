"""
ğŸ“¡ IOOptimizer - IOä¼˜åŒ–å™¨

ç½‘ç»œå’Œç£ç›˜IOä¼˜åŒ–
æä¾›ç½‘ç»œä¼˜åŒ–ã€ç£ç›˜ä¼˜åŒ–ã€å‹ç¼©ä¼ è¾“ã€åè®®ä¼˜åŒ–ç­‰åŠŸèƒ½
"""

import asyncio
import aiofiles
import aiohttp
import time
import gzip
import brotli
import zlib
import hashlib
import os
from typing import Dict, Any, Optional, List, Callable, Union, BinaryIO, TextIO
from dataclasses import dataclass, field
from enum import Enum
import logging
from collections import defaultdict, deque
import socket
import ssl

logger = logging.getLogger(__name__)


class CompressionType(Enum):
    """å‹ç¼©ç±»å‹æšä¸¾"""
    NONE = "none"
    GZIP = "gzip"
    DEFLATE = "deflate"
    BROTLI = "brotli"
    LZ4 = "lz4"
    AUTO = "auto"


class IOMode(Enum):
    """IOæ¨¡å¼æšä¸¾"""
    BLOCKING = "blocking"
    NON_BLOCKING = "non_blocking"
    ASYNC = "async"
    MEMORY_MAPPED = "memory_mapped"


class NetworkProtocol(Enum):
    """ç½‘ç»œåè®®æšä¸¾"""
    HTTP_1_1 = "http/1.1"
    HTTP_2 = "http/2"
    HTTP_3 = "http/3"
    WEBSOCKET = "websocket"
    TCP = "tcp"
    UDP = "udp"


@dataclass
class IOConfig:
    """IOé…ç½®"""
    # ç½‘ç»œé…ç½®
    connection_timeout: float = 10.0
    read_timeout: float = 30.0
    write_timeout: float = 30.0
    max_connections: int = 100
    max_connections_per_host: int = 30
    keepalive_timeout: float = 30.0
    
    # ç¼“å†²åŒºé…ç½®
    read_buffer_size: int = 64 * 1024      # 64KB
    write_buffer_size: int = 64 * 1024     # 64KB
    tcp_nodelay: bool = True
    tcp_keepalive: bool = True
    
    # å‹ç¼©é…ç½®
    compression_enabled: bool = True
    compression_type: CompressionType = CompressionType.AUTO
    compression_level: int = 6
    min_compression_size: int = 1024
    
    # ç£ç›˜IOé…ç½®
    disk_io_mode: IOMode = IOMode.ASYNC
    max_file_cache_size: int = 100
    file_buffer_size: int = 8192
    enable_sendfile: bool = True
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_interval: float = 60.0


@dataclass
class IOStats:
    """IOç»Ÿè®¡"""
    # ç½‘ç»œç»Ÿè®¡
    bytes_sent: int = 0
    bytes_received: int = 0
    requests_sent: int = 0
    responses_received: int = 0
    connection_errors: int = 0
    timeout_errors: int = 0
    
    # ç£ç›˜ç»Ÿè®¡
    files_read: int = 0
    files_written: int = 0
    disk_bytes_read: int = 0
    disk_bytes_written: int = 0
    disk_operations: int = 0
    
    # å‹ç¼©ç»Ÿè®¡
    compression_ratio: float = 0.0
    compression_time: float = 0.0
    decompression_time: float = 0.0
    
    # æ€§èƒ½ç»Ÿè®¡
    avg_response_time: float = 0.0
    avg_throughput: float = 0.0
    response_times: deque = field(default_factory=lambda: deque(maxlen=1000))


class CompressionManager:
    """å‹ç¼©ç®¡ç†å™¨"""
    
    def __init__(self, config: IOConfig):
        self.config = config
        self.stats = {
            "compressed_bytes": 0,
            "uncompressed_bytes": 0,
            "compression_time": 0.0,
            "decompression_time": 0.0
        }
    
    def compress(self, data: Union[str, bytes], 
                compression_type: Optional[CompressionType] = None) -> bytes:
        """å‹ç¼©æ•°æ®"""
        if isinstance(data, str):
            data = data.encode('utf-8')
        
        if len(data) < self.config.min_compression_size:
            return data
        
        compression_type = compression_type or self.config.compression_type
        start_time = time.time()
        
        try:
            if compression_type == CompressionType.GZIP:
                compressed = gzip.compress(data, compresslevel=self.config.compression_level)
            elif compression_type == CompressionType.DEFLATE:
                compressed = zlib.compress(data, self.config.compression_level)
            elif compression_type == CompressionType.BROTLI:
                compressed = brotli.compress(data, quality=self.config.compression_level)
            elif compression_type == CompressionType.AUTO:
                # è‡ªåŠ¨é€‰æ‹©æœ€ä½³å‹ç¼©ç®—æ³•
                compressed = self._auto_compress(data)
            else:
                compressed = data
            
            # æ›´æ–°ç»Ÿè®¡
            compression_time = time.time() - start_time
            self.stats["compressed_bytes"] += len(compressed)
            self.stats["uncompressed_bytes"] += len(data)
            self.stats["compression_time"] += compression_time
            
            return compressed
        
        except Exception as e:
            logger.error(f"å‹ç¼©å¤±è´¥: {e}")
            return data
    
    def decompress(self, data: bytes, 
                  compression_type: CompressionType) -> bytes:
        """è§£å‹æ•°æ®"""
        start_time = time.time()
        
        try:
            if compression_type == CompressionType.GZIP:
                decompressed = gzip.decompress(data)
            elif compression_type == CompressionType.DEFLATE:
                decompressed = zlib.decompress(data)
            elif compression_type == CompressionType.BROTLI:
                decompressed = brotli.decompress(data)
            else:
                decompressed = data
            
            # æ›´æ–°ç»Ÿè®¡
            decompression_time = time.time() - start_time
            self.stats["decompression_time"] += decompression_time
            
            return decompressed
        
        except Exception as e:
            logger.error(f"è§£å‹å¤±è´¥: {e}")
            return data
    
    def _auto_compress(self, data: bytes) -> bytes:
        """è‡ªåŠ¨é€‰æ‹©å‹ç¼©ç®—æ³•"""
        # å°è¯•ä¸åŒå‹ç¼©ç®—æ³•ï¼Œé€‰æ‹©å‹ç¼©ç‡æœ€é«˜çš„
        compression_results = {}
        
        try:
            compression_results[CompressionType.GZIP] = gzip.compress(data, compresslevel=6)
        except:
            pass
        
        try:
            compression_results[CompressionType.DEFLATE] = zlib.compress(data, 6)
        except:
            pass
        
        try:
            compression_results[CompressionType.BROTLI] = brotli.compress(data, quality=6)
        except:
            pass
        
        if not compression_results:
            return data
        
        # é€‰æ‹©å‹ç¼©åå¤§å°æœ€å°çš„
        best_type, best_result = min(compression_results.items(), key=lambda x: len(x[1]))
        return best_result
    
    def get_compression_ratio(self) -> float:
        """è·å–å‹ç¼©æ¯”"""
        if self.stats["uncompressed_bytes"] == 0:
            return 0.0
        return self.stats["compressed_bytes"] / self.stats["uncompressed_bytes"]


class NetworkOptimizer:
    """ç½‘ç»œä¼˜åŒ–å™¨"""
    
    def __init__(self, config: IOConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self.connection_pool: Dict[str, List[Any]] = defaultdict(list)
        self.stats = IOStats()
        
    async def start(self):
        """å¯åŠ¨ç½‘ç»œä¼˜åŒ–å™¨"""
        if self.session:
            return
        
        # åˆ›å»ºä¼˜åŒ–çš„è¿æ¥å™¨
        connector = aiohttp.TCPConnector(
            limit=self.config.max_connections,
            limit_per_host=self.config.max_connections_per_host,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=self.config.keepalive_timeout,
            enable_cleanup_closed=True
        )
        
        # åˆ›å»ºä¼šè¯
        timeout = aiohttp.ClientTimeout(
            total=self.config.read_timeout,
            connect=self.config.connection_timeout
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': 'MarketPrism-IOOptimizer/1.0'}
        )
        
        logger.info("ç½‘ç»œä¼˜åŒ–å™¨å·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢ç½‘ç»œä¼˜åŒ–å™¨"""
        if self.session:
            await self.session.close()
            self.session = None
        
        logger.info("ç½‘ç»œä¼˜åŒ–å™¨å·²åœæ­¢")
    
    async def request(self, method: str, url: str, **kwargs) -> aiohttp.ClientResponse:
        """ä¼˜åŒ–çš„HTTPè¯·æ±‚"""
        if not self.session:
            await self.start()
        
        start_time = time.time()
        
        try:
            # æ·»åŠ å‹ç¼©æ”¯æŒ
            headers = kwargs.get('headers', {})
            if self.config.compression_enabled:
                headers['Accept-Encoding'] = 'gzip, deflate, br'
            kwargs['headers'] = headers
            
            response = await self.session.request(method, url, **kwargs)
            
            # æ›´æ–°ç»Ÿè®¡
            response_time = time.time() - start_time
            self.stats.requests_sent += 1
            self.stats.responses_received += 1
            self.stats.response_times.append(response_time)
            
            return response
        
        except asyncio.TimeoutError:
            self.stats.timeout_errors += 1
            raise
        except Exception as e:
            self.stats.connection_errors += 1
            raise
    
    async def download_file(self, url: str, file_path: str, chunk_size: int = None) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ–‡ä»¶ä¸‹è½½"""
        chunk_size = chunk_size or self.config.read_buffer_size
        start_time = time.time()
        bytes_downloaded = 0
        
        async with self.request('GET', url) as response:
            response.raise_for_status()
            
            async with aiofiles.open(file_path, 'wb') as file:
                async for chunk in response.content.iter_chunked(chunk_size):
                    await file.write(chunk)
                    bytes_downloaded += len(chunk)
        
        download_time = time.time() - start_time
        throughput = bytes_downloaded / download_time if download_time > 0 else 0
        
        self.stats.bytes_received += bytes_downloaded
        
        return {
            "bytes_downloaded": bytes_downloaded,
            "download_time": download_time,
            "throughput": throughput
        }
    
    def configure_socket(self, sock: socket.socket):
        """é…ç½®socketé€‰é¡¹"""
        # å¯ç”¨TCP_NODELAY
        if self.config.tcp_nodelay:
            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        
        # å¯ç”¨SO_KEEPALIVE
        if self.config.tcp_keepalive:
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)
        
        # è®¾ç½®ç¼“å†²åŒºå¤§å°
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, self.config.read_buffer_size)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, self.config.write_buffer_size)


class DiskOptimizer:
    """ç£ç›˜ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: IOConfig):
        self.config = config
        self.file_cache: Dict[str, Any] = {}
        self.stats = IOStats()
        
    async def read_file(self, file_path: str, mode: str = 'r', **kwargs) -> Union[str, bytes]:
        """ä¼˜åŒ–çš„æ–‡ä»¶è¯»å–"""
        start_time = time.time()
        
        try:
            if self.config.disk_io_mode == IOMode.ASYNC:
                async with aiofiles.open(file_path, mode, **kwargs) as file:
                    content = await file.read()
            else:
                with open(file_path, mode, **kwargs) as file:
                    content = file.read()
            
            # æ›´æ–°ç»Ÿè®¡
            read_time = time.time() - start_time
            self.stats.files_read += 1
            self.stats.disk_bytes_read += len(content) if isinstance(content, (str, bytes)) else 0
            self.stats.disk_operations += 1
            
            return content
        
        except Exception as e:
            logger.error(f"æ–‡ä»¶è¯»å–å¤±è´¥: {file_path}, error={e}")
            raise
    
    async def write_file(self, file_path: str, content: Union[str, bytes], 
                        mode: str = 'w', **kwargs) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ–‡ä»¶å†™å…¥"""
        start_time = time.time()
        
        try:
            if self.config.disk_io_mode == IOMode.ASYNC:
                async with aiofiles.open(file_path, mode, **kwargs) as file:
                    await file.write(content)
            else:
                with open(file_path, mode, **kwargs) as file:
                    file.write(content)
            
            # æ›´æ–°ç»Ÿè®¡
            write_time = time.time() - start_time
            self.stats.files_written += 1
            self.stats.disk_bytes_written += len(content) if isinstance(content, (str, bytes)) else 0
            self.stats.disk_operations += 1
            
            return {
                "bytes_written": len(content) if isinstance(content, (str, bytes)) else 0,
                "write_time": write_time
            }
        
        except Exception as e:
            logger.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {file_path}, error={e}")
            raise
    
    async def copy_file(self, src_path: str, dst_path: str, 
                       chunk_size: int = None) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤åˆ¶"""
        chunk_size = chunk_size or self.config.file_buffer_size
        start_time = time.time()
        bytes_copied = 0
        
        try:
            if self.config.disk_io_mode == IOMode.ASYNC:
                async with aiofiles.open(src_path, 'rb') as src:
                    async with aiofiles.open(dst_path, 'wb') as dst:
                        while True:
                            chunk = await src.read(chunk_size)
                            if not chunk:
                                break
                            await dst.write(chunk)
                            bytes_copied += len(chunk)
            else:
                with open(src_path, 'rb') as src:
                    with open(dst_path, 'wb') as dst:
                        while True:
                            chunk = src.read(chunk_size)
                            if not chunk:
                                break
                            dst.write(chunk)
                            bytes_copied += len(chunk)
            
            copy_time = time.time() - start_time
            throughput = bytes_copied / copy_time if copy_time > 0 else 0
            
            return {
                "bytes_copied": bytes_copied,
                "copy_time": copy_time,
                "throughput": throughput
            }
        
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥: {src_path} -> {dst_path}, error={e}")
            raise
    
    def cache_file(self, file_path: str, content: Any):
        """ç¼“å­˜æ–‡ä»¶å†…å®¹"""
        if len(self.file_cache) >= self.config.max_file_cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self.file_cache))
            del self.file_cache[oldest_key]
        
        self.file_cache[file_path] = {
            "content": content,
            "cached_at": time.time(),
            "access_count": 0
        }
    
    def get_cached_file(self, file_path: str) -> Optional[Any]:
        """è·å–ç¼“å­˜çš„æ–‡ä»¶å†…å®¹"""
        if file_path in self.file_cache:
            cache_entry = self.file_cache[file_path]
            cache_entry["access_count"] += 1
            return cache_entry["content"]
        return None


class IOOptimizer:
    """
    ğŸ“¡ IOä¼˜åŒ–å™¨
    
    æä¾›ç½‘ç»œä¼˜åŒ–ã€ç£ç›˜ä¼˜åŒ–ã€å‹ç¼©ä¼ è¾“ã€åè®®ä¼˜åŒ–ç­‰åŠŸèƒ½
    """
    
    def __init__(self, config: Optional[IOConfig] = None):
        self.config = config or IOConfig()
        self.compression_manager = CompressionManager(self.config)
        self.network_optimizer = NetworkOptimizer(self.config)
        self.disk_optimizer = DiskOptimizer(self.config)
        self.stats = IOStats()
        self.metrics_task: Optional[asyncio.Task] = None
        self.is_running = False
        
        logger.info("IOOptimizeråˆå§‹åŒ–å®Œæˆ")
    
    async def start(self):
        """å¯åŠ¨IOä¼˜åŒ–å™¨"""
        if self.is_running:
            return
        
        self.is_running = True
        
        # å¯åŠ¨ç½‘ç»œä¼˜åŒ–å™¨
        await self.network_optimizer.start()
        
        # å¯åŠ¨æŒ‡æ ‡æ”¶é›†
        if self.config.enable_metrics:
            self.metrics_task = asyncio.create_task(self._metrics_loop())
        
        logger.info("IOOptimizerå·²å¯åŠ¨")
    
    async def stop(self):
        """åœæ­¢IOä¼˜åŒ–å™¨"""
        if not self.is_running:
            return
        
        self.is_running = False
        
        # å–æ¶ˆæŒ‡æ ‡ä»»åŠ¡
        if self.metrics_task:
            self.metrics_task.cancel()
        
        # åœæ­¢ç½‘ç»œä¼˜åŒ–å™¨
        await self.network_optimizer.stop()
        
        logger.info("IOOptimizerå·²åœæ­¢")
    
    # ç½‘ç»œç›¸å…³æ–¹æ³•
    async def http_request(self, method: str, url: str, **kwargs) -> aiohttp.ClientResponse:
        """HTTPè¯·æ±‚"""
        return await self.network_optimizer.request(method, url, **kwargs)
    
    async def download_file(self, url: str, file_path: str, **kwargs) -> Dict[str, Any]:
        """ä¸‹è½½æ–‡ä»¶"""
        return await self.network_optimizer.download_file(url, file_path, **kwargs)
    
    # ç£ç›˜ç›¸å…³æ–¹æ³•
    async def read_file(self, file_path: str, use_cache: bool = True, **kwargs) -> Union[str, bytes]:
        """è¯»å–æ–‡ä»¶"""
        if use_cache:
            cached_content = self.disk_optimizer.get_cached_file(file_path)
            if cached_content is not None:
                return cached_content
        
        content = await self.disk_optimizer.read_file(file_path, **kwargs)
        
        if use_cache:
            self.disk_optimizer.cache_file(file_path, content)
        
        return content
    
    async def write_file(self, file_path: str, content: Union[str, bytes], 
                        compress: bool = False, **kwargs) -> Dict[str, Any]:
        """å†™å…¥æ–‡ä»¶"""
        if compress and self.config.compression_enabled:
            if isinstance(content, str):
                content = content.encode('utf-8')
            content = self.compression_manager.compress(content)
            file_path += '.gz'  # æ·»åŠ å‹ç¼©æ–‡ä»¶æ‰©å±•å
            kwargs['mode'] = 'wb'
        
        return await self.disk_optimizer.write_file(file_path, content, **kwargs)
    
    async def copy_file(self, src_path: str, dst_path: str, **kwargs) -> Dict[str, Any]:
        """å¤åˆ¶æ–‡ä»¶"""
        return await self.disk_optimizer.copy_file(src_path, dst_path, **kwargs)
    
    # å‹ç¼©ç›¸å…³æ–¹æ³•
    def compress_data(self, data: Union[str, bytes], 
                     compression_type: Optional[CompressionType] = None) -> bytes:
        """å‹ç¼©æ•°æ®"""
        return self.compression_manager.compress(data, compression_type)
    
    def decompress_data(self, data: bytes, 
                       compression_type: CompressionType) -> bytes:
        """è§£å‹æ•°æ®"""
        return self.compression_manager.decompress(data, compression_type)
    
    # ç»Ÿè®¡å’Œåˆ†ææ–¹æ³•
    def get_stats(self) -> Dict[str, Any]:
        """è·å–IOç»Ÿè®¡"""
        # åˆå¹¶å„ç»„ä»¶ç»Ÿè®¡
        network_stats = self.network_optimizer.stats
        disk_stats = self.disk_optimizer.stats
        compression_stats = self.compression_manager.stats
        
        return {
            "network": {
                "bytes_sent": network_stats.bytes_sent,
                "bytes_received": network_stats.bytes_received,
                "requests_sent": network_stats.requests_sent,
                "responses_received": network_stats.responses_received,
                "connection_errors": network_stats.connection_errors,
                "timeout_errors": network_stats.timeout_errors,
                "avg_response_time": (
                    sum(network_stats.response_times) / len(network_stats.response_times)
                    if network_stats.response_times else 0.0
                )
            },
            "disk": {
                "files_read": disk_stats.files_read,
                "files_written": disk_stats.files_written,
                "bytes_read": disk_stats.disk_bytes_read,
                "bytes_written": disk_stats.disk_bytes_written,
                "operations": disk_stats.disk_operations,
                "cache_size": len(self.disk_optimizer.file_cache)
            },
            "compression": {
                "compression_ratio": self.compression_manager.get_compression_ratio(),
                "compression_time": compression_stats["compression_time"],
                "decompression_time": compression_stats["decompression_time"],
                "compressed_bytes": compression_stats["compressed_bytes"],
                "uncompressed_bytes": compression_stats["uncompressed_bytes"]
            }
        }
    
    def get_performance_analysis(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½åˆ†æ"""
        stats = self.get_stats()
        analysis = {
            "stats": stats,
            "recommendations": []
        }
        
        # ç½‘ç»œæ€§èƒ½åˆ†æ
        if stats["network"]["connection_errors"] > 0:
            error_rate = stats["network"]["connection_errors"] / max(stats["network"]["requests_sent"], 1)
            if error_rate > 0.1:
                analysis["recommendations"].append("ç½‘ç»œè¿æ¥é”™è¯¯ç‡è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œé…ç½®")
        
        if stats["network"]["avg_response_time"] > 5.0:
            analysis["recommendations"].append("å¹³å‡å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–ç½‘ç»œé…ç½®æˆ–å¢åŠ è¶…æ—¶æ—¶é—´")
        
        # ç£ç›˜æ€§èƒ½åˆ†æ
        cache_hit_potential = stats["disk"]["files_read"] > stats["disk"]["cache_size"]
        if cache_hit_potential:
            analysis["recommendations"].append("å¯ä»¥å¢åŠ æ–‡ä»¶ç¼“å­˜å¤§å°ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡")
        
        # å‹ç¼©åˆ†æ
        compression_ratio = stats["compression"]["compression_ratio"]
        if compression_ratio > 0.8:
            analysis["recommendations"].append("å‹ç¼©æ¯”è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘è°ƒæ•´å‹ç¼©ç®—æ³•æˆ–å‹ç¼©çº§åˆ«")
        
        return analysis
    
    def optimize_config(self) -> Dict[str, Any]:
        """ä¼˜åŒ–é…ç½®"""
        stats = self.get_stats()
        optimizations = {
            "current_config": {
                "read_buffer_size": self.config.read_buffer_size,
                "write_buffer_size": self.config.write_buffer_size,
                "max_connections": self.config.max_connections,
                "compression_level": self.config.compression_level
            },
            "recommended_config": {},
            "optimizations": []
        }
        
        # æ ¹æ®ç»Ÿè®¡æ•°æ®ä¼˜åŒ–é…ç½®
        avg_response_time = stats["network"]["avg_response_time"]
        if avg_response_time > 2.0:
            optimizations["recommended_config"]["read_buffer_size"] = self.config.read_buffer_size * 2
            optimizations["optimizations"].append("å¢åŠ è¯»ç¼“å†²åŒºå¤§å°ä»¥å‡å°‘å“åº”æ—¶é—´")
        
        connection_errors = stats["network"]["connection_errors"]
        if connection_errors > 10:
            optimizations["recommended_config"]["max_connections"] = min(
                self.config.max_connections * 2, 500
            )
            optimizations["optimizations"].append("å¢åŠ æœ€å¤§è¿æ¥æ•°ä»¥å‡å°‘è¿æ¥é”™è¯¯")
        
        compression_ratio = stats["compression"]["compression_ratio"]
        if compression_ratio > 0.9:
            optimizations["recommended_config"]["compression_level"] = max(
                self.config.compression_level - 1, 1
            )
            optimizations["optimizations"].append("é™ä½å‹ç¼©çº§åˆ«ä»¥æé«˜å‹ç¼©æ¯”")
        
        return optimizations
    
    async def _metrics_loop(self):
        """æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        while self.is_running:
            try:
                await asyncio.sleep(self.config.metrics_interval)
                await self._update_metrics()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æŒ‡æ ‡æ›´æ–°å¤±è´¥: {e}")
    
    async def _update_metrics(self):
        """æ›´æ–°æŒ‡æ ‡"""
        stats = self.get_stats()
        
        # è®¡ç®—ååé‡
        network_stats = stats["network"]
        if network_stats["requests_sent"] > 0:
            total_bytes = network_stats["bytes_sent"] + network_stats["bytes_received"]
            self.stats.avg_throughput = total_bytes / self.config.metrics_interval
        
        logger.debug(f"IOæŒ‡æ ‡æ›´æ–°: {stats}")


# å·¥å…·å‡½æ•°

async def bulk_download(optimizer: IOOptimizer, urls: List[str], 
                       destination_dir: str, max_concurrent: int = 10) -> List[Dict[str, Any]]:
    """æ‰¹é‡ä¸‹è½½æ–‡ä»¶"""
    semaphore = asyncio.Semaphore(max_concurrent)
    results = []
    
    async def download_single(url: str, index: int):
        async with semaphore:
            file_name = f"file_{index}_{hashlib.md5(url.encode()).hexdigest()[:8]}"
            file_path = os.path.join(destination_dir, file_name)
            try:
                result = await optimizer.download_file(url, file_path)
                result["url"] = url
                result["success"] = True
                return result
            except Exception as e:
                return {"url": url, "success": False, "error": str(e)}
    
    tasks = [download_single(url, i) for i, url in enumerate(urls)]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return results


def io_performance_monitor(func):
    """IOæ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = 0  # å¯ä»¥æ·»åŠ å†…å­˜ç›‘æ§
        
        try:
            result = await func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
            raise
        finally:
            duration = time.time() - start_time
            logger.debug(f"IOæ“ä½œå®Œæˆ: {func.__name__}, duration={duration:.3f}s, success={success}")
            
            # å¯ä»¥å°†æ€§èƒ½æ•°æ®å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
            if hasattr(args[0], 'performance_callback'):
                await args[0].performance_callback({
                    "function": func.__name__,
                    "duration": duration,
                    "success": success,
                    "error": error
                })
        
        return result
    
    return wrapper