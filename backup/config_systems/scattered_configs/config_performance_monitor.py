#!/usr/bin/env python3
"""
MarketPrism é…ç½®æ€§èƒ½ç›‘æ§å™¨
é…ç½®ç®¡ç†ç³»ç»Ÿ 2.0 - Week 5 Day 5

é«˜æ€§èƒ½é…ç½®ç›‘æ§ç»„ä»¶ï¼Œæä¾›å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†ã€ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–å»ºè®®ã€‚
æ”¯æŒå¤šç»´åº¦æ€§èƒ½åˆ†æã€å®æ—¶è­¦æŠ¥å’Œè‡ªåŠ¨è°ƒä¼˜æœºåˆ¶ã€‚

Author: MarketPrismå›¢é˜Ÿ
Created: 2025-01-29
Version: 1.0.0
"""

import time
import threading
import statistics
import json
from typing import Dict, Any, List, Optional, Callable, Union, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum, auto
from collections import defaultdict, deque
import asyncio
import concurrent.futures
from contextlib import contextmanager
import psutil
import uuid
import gc


class MetricType(Enum):
    """æ€§èƒ½æŒ‡æ ‡ç±»å‹"""
    LATENCY = auto()          # å»¶è¿Ÿ
    THROUGHPUT = auto()       # ååé‡
    ERROR_RATE = auto()       # é”™è¯¯ç‡
    MEMORY_USAGE = auto()     # å†…å­˜ä½¿ç”¨
    CPU_USAGE = auto()        # CPUä½¿ç”¨
    CACHE_HIT_RATE = auto()   # ç¼“å­˜å‘½ä¸­ç‡
    QUEUE_LENGTH = auto()     # é˜Ÿåˆ—é•¿åº¦
    CONNECTION_COUNT = auto()  # è¿æ¥æ•°


class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class PerformanceThreshold(Enum):
    """æ€§èƒ½é˜ˆå€¼çº§åˆ«"""
    EXCELLENT = auto()    # ä¼˜ç§€ (<10ms)
    GOOD = auto()        # è‰¯å¥½ (10-50ms)
    ACCEPTABLE = auto()   # å¯æ¥å— (50-200ms)
    POOR = auto()        # è¾ƒå·® (200-1000ms)
    CRITICAL = auto()     # ä¸¥é‡ (>1000ms)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    metric_id: str
    metric_type: MetricType
    value: float
    unit: str
    timestamp: datetime
    component: str
    operation: str
    tags: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['metric_type'] = self.metric_type.name
        return data


@dataclass
class PerformanceAlert:
    """æ€§èƒ½å‘Šè­¦æ•°æ®ç»“æ„"""
    alert_id: str
    severity: AlertSeverity
    metric_type: MetricType
    current_value: float
    threshold: float
    component: str
    operation: str
    message: str
    timestamp: datetime
    resolved: bool = False
    resolution_time: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        data['timestamp'] = self.timestamp.isoformat()
        data['severity'] = self.severity.value
        data['metric_type'] = self.metric_type.name
        if self.resolution_time:
            data['resolution_time'] = self.resolution_time.isoformat()
        return data


@dataclass
class PerformanceStats:
    """æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    count: int = 0
    min_value: float = float('inf')
    max_value: float = float('-inf')
    avg_value: float = 0.0
    p50: float = 0.0
    p95: float = 0.0
    p99: float = 0.0
    std_dev: float = 0.0
    first_seen: Optional[datetime] = None
    last_seen: Optional[datetime] = None
    
    def update(self, value: float, timestamp: datetime):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.count += 1
        self.min_value = min(self.min_value, value)
        self.max_value = max(self.max_value, value)
        
        if self.first_seen is None:
            self.first_seen = timestamp
        self.last_seen = timestamp


class ConfigPerformanceMonitor:
    """é…ç½®æ€§èƒ½ç›‘æ§å™¨
    
    æä¾›å…¨æ–¹ä½çš„é…ç½®ç³»ç»Ÿæ€§èƒ½ç›‘æ§ï¼ŒåŒ…æ‹¬ï¼š
    - å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†
    - è‡ªåŠ¨é˜ˆå€¼æ£€æµ‹å’Œå‘Šè­¦
    - æ€§èƒ½è¶‹åŠ¿åˆ†æ
    - ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–å»ºè®®
    - å¤šç»´åº¦ç»Ÿè®¡åˆ†æ
    """
    
    def __init__(self, 
                 monitoring_interval: float = 1.0,
                 history_size: int = 10000,
                 enable_auto_gc: bool = True):
        """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        
        Args:
            monitoring_interval: ç›‘æ§é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
            history_size: å†å²æ•°æ®ä¿ç•™æ•°é‡
            enable_auto_gc: æ˜¯å¦å¯ç”¨è‡ªåŠ¨åƒåœ¾å›æ”¶
        """
        self.monitoring_interval = monitoring_interval
        self.history_size = history_size
        self.enable_auto_gc = enable_auto_gc
        
        # æ€§èƒ½æŒ‡æ ‡å­˜å‚¨
        self.metrics: deque = deque(maxlen=history_size)
        self.alerts: deque = deque(maxlen=history_size)
        self.stats: Dict[str, PerformanceStats] = defaultdict(PerformanceStats)
        
        # é˜ˆå€¼é…ç½®
        self.thresholds: Dict[MetricType, Dict[str, float]] = {
            MetricType.LATENCY: {
                'excellent': 10.0,    # ms
                'good': 50.0,
                'acceptable': 200.0,
                'poor': 1000.0
            },
            MetricType.THROUGHPUT: {
                'excellent': 10000.0,  # ops/sec
                'good': 5000.0,
                'acceptable': 1000.0,
                'poor': 100.0
            },
            MetricType.ERROR_RATE: {
                'excellent': 0.001,   # 0.1%
                'good': 0.01,         # 1%
                'acceptable': 0.05,   # 5%
                'poor': 0.1           # 10%
            },
            MetricType.MEMORY_USAGE: {
                'excellent': 100.0,   # MB
                'good': 500.0,
                'acceptable': 1000.0,
                'poor': 2000.0
            },
            MetricType.CPU_USAGE: {
                'excellent': 20.0,    # %
                'good': 50.0,
                'acceptable': 80.0,
                'poor': 95.0
            },
            MetricType.CACHE_HIT_RATE: {
                'excellent': 0.95,    # 95%
                'good': 0.90,         # 90%
                'acceptable': 0.80,   # 80%
                'poor': 0.50          # 50%
            }
        }
        
        # å‘Šè­¦å›è°ƒå‡½æ•°
        self.alert_callbacks: List[Callable[[PerformanceAlert], None]] = []
        self.metric_callbacks: List[Callable[[PerformanceMetric], None]] = []
        
        # æ§åˆ¶å˜é‡
        self._monitoring = False
        self._monitor_thread: Optional[threading.Thread] = None
        self._lock = threading.RLock()
        
        # æ€§èƒ½è®¡æ•°å™¨
        self._operation_counters: Dict[str, int] = defaultdict(int)
        self._timing_data: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # ç³»ç»Ÿèµ„æºç›‘æ§
        self._process = psutil.Process()
        
        # æŒ‡æ ‡èšåˆå™¨
        self._metric_aggregators: Dict[str, List[float]] = defaultdict(list)
        self._aggregation_window = timedelta(minutes=1)
        
    def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self._monitoring:
            return
            
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="ConfigPerformanceMonitor"
        )
        self._monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=5.0)
            
    def record_metric(self, 
                     metric_type: MetricType,
                     value: float,
                     component: str,
                     operation: str,
                     unit: str = "",
                     tags: Optional[Dict[str, Any]] = None) -> str:
        """è®°å½•æ€§èƒ½æŒ‡æ ‡
        
        Args:
            metric_type: æŒ‡æ ‡ç±»å‹
            value: æŒ‡æ ‡å€¼
            component: ç»„ä»¶åç§°
            operation: æ“ä½œåç§°
            unit: å•ä½
            tags: é™„åŠ æ ‡ç­¾
            
        Returns:
            æŒ‡æ ‡ID
        """
        metric_id = str(uuid.uuid4())
        timestamp = datetime.now()
        
        metric = PerformanceMetric(
            metric_id=metric_id,
            metric_type=metric_type,
            value=value,
            unit=unit,
            timestamp=timestamp,
            component=component,
            operation=operation,
            tags=tags or {}
        )
        
        with self._lock:
            # å­˜å‚¨æŒ‡æ ‡
            self.metrics.append(metric)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            stats_key = f"{component}.{operation}.{metric_type.name}"
            self.stats[stats_key].update(value, timestamp)
            
            # èšåˆæ•°æ®
            self._metric_aggregators[stats_key].append(value)
            
            # æ£€æŸ¥é˜ˆå€¼
            self._check_thresholds(metric)
            
            # è§¦å‘å›è°ƒ
            for callback in self.metric_callbacks:
                try:
                    callback(metric)
                except Exception as e:
                    print(f"Error in metric callback: {e}")
                    
        return metric_id
        
    @contextmanager
    def measure_operation(self, 
                         component: str, 
                         operation: str,
                         tags: Optional[Dict[str, Any]] = None):
        """æµ‹é‡æ“ä½œè€—æ—¶çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            component: ç»„ä»¶åç§°
            operation: æ“ä½œåç§°
            tags: é™„åŠ æ ‡ç­¾
        """
        start_time = time.perf_counter()
        operation_key = f"{component}.{operation}"
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            duration_ms = (end_time - start_time) * 1000
            
            # è®°å½•å»¶è¿ŸæŒ‡æ ‡
            self.record_metric(
                MetricType.LATENCY,
                duration_ms,
                component,
                operation,
                "ms",
                tags
            )
            
            # æ›´æ–°æ“ä½œè®¡æ•°
            with self._lock:
                self._operation_counters[operation_key] += 1
                self._timing_data[operation_key].append(duration_ms)
                
    def add_alert_callback(self, callback: Callable[[PerformanceAlert], None]):
        """æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°"""
        self.alert_callbacks.append(callback)
        
    def add_metric_callback(self, callback: Callable[[PerformanceMetric], None]):
        """æ·»åŠ æŒ‡æ ‡å›è°ƒå‡½æ•°"""
        self.metric_callbacks.append(callback)
        
    def get_metrics(self, 
                   component: Optional[str] = None,
                   metric_type: Optional[MetricType] = None,
                   since: Optional[datetime] = None,
                   limit: Optional[int] = None) -> List[PerformanceMetric]:
        """è·å–æ€§èƒ½æŒ‡æ ‡
        
        Args:
            component: ç»„ä»¶è¿‡æ»¤
            metric_type: æŒ‡æ ‡ç±»å‹è¿‡æ»¤
            since: æ—¶é—´è¿‡æ»¤
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            æŒ‡æ ‡åˆ—è¡¨
        """
        with self._lock:
            filtered_metrics = []
            
            for metric in reversed(self.metrics):
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if component and metric.component != component:
                    continue
                if metric_type and metric.metric_type != metric_type:
                    continue
                if since and metric.timestamp < since:
                    continue
                    
                filtered_metrics.append(metric)
                
                # åº”ç”¨æ•°é‡é™åˆ¶
                if limit and len(filtered_metrics) >= limit:
                    break
                    
            return filtered_metrics
            
    def get_alerts(self, 
                  severity: Optional[AlertSeverity] = None,
                  resolved: Optional[bool] = None,
                  since: Optional[datetime] = None) -> List[PerformanceAlert]:
        """è·å–å‘Šè­¦ä¿¡æ¯
        
        Args:
            severity: ä¸¥é‡ç¨‹åº¦è¿‡æ»¤
            resolved: è§£å†³çŠ¶æ€è¿‡æ»¤
            since: æ—¶é—´è¿‡æ»¤
            
        Returns:
            å‘Šè­¦åˆ—è¡¨
        """
        with self._lock:
            filtered_alerts = []
            
            for alert in reversed(self.alerts):
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if severity and alert.severity != severity:
                    continue
                if resolved is not None and alert.resolved != resolved:
                    continue
                if since and alert.timestamp < since:
                    continue
                    
                filtered_alerts.append(alert)
                
            return filtered_alerts
            
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        with self._lock:
            now = datetime.now()
            
            # ç³»ç»Ÿèµ„æºä½¿ç”¨
            memory_info = self._process.memory_info()
            cpu_percent = self._process.cpu_percent()
            
            # æŒ‡æ ‡ç»Ÿè®¡
            total_metrics = len(self.metrics)
            total_alerts = len(self.alerts)
            active_alerts = sum(1 for alert in self.alerts if not alert.resolved)
            
            # æœ€è¿‘æ€§èƒ½
            recent_metrics = [m for m in self.metrics 
                            if (now - m.timestamp) < timedelta(minutes=5)]
            
            # è®¡ç®—å¹³å‡å»¶è¿Ÿ
            latency_metrics = [m for m in recent_metrics 
                             if m.metric_type == MetricType.LATENCY]
            avg_latency = statistics.mean([m.value for m in latency_metrics]) if latency_metrics else 0
            
            # è®¡ç®—ååé‡
            throughput_metrics = [m for m in recent_metrics 
                                if m.metric_type == MetricType.THROUGHPUT]
            avg_throughput = statistics.mean([m.value for m in throughput_metrics]) if throughput_metrics else 0
            
            # è®¡ç®—é”™è¯¯ç‡
            error_metrics = [m for m in recent_metrics 
                           if m.metric_type == MetricType.ERROR_RATE]
            avg_error_rate = statistics.mean([m.value for m in error_metrics]) if error_metrics else 0
            
            return {
                "timestamp": now.isoformat(),
                "monitoring_status": "active" if self._monitoring else "inactive",
                "system_resources": {
                    "memory_usage_mb": memory_info.rss / 1024 / 1024,
                    "memory_usage_percent": self._process.memory_percent(),
                    "cpu_usage_percent": cpu_percent,
                    "threads_count": self._process.num_threads()
                },
                "metrics_summary": {
                    "total_metrics": total_metrics,
                    "recent_metrics_5min": len(recent_metrics),
                    "metrics_per_second": len(recent_metrics) / 300 if recent_metrics else 0
                },
                "alerts_summary": {
                    "total_alerts": total_alerts,
                    "active_alerts": active_alerts,
                    "resolved_alerts": total_alerts - active_alerts
                },
                "performance_indicators": {
                    "avg_latency_ms": round(avg_latency, 2),
                    "avg_throughput_ops": round(avg_throughput, 2),
                    "avg_error_rate_percent": round(avg_error_rate * 100, 4),
                    "performance_level": self._assess_performance_level(avg_latency)
                },
                "operation_stats": dict(self._operation_counters),
                "top_operations": self._get_top_operations()
            }
            
    def get_component_stats(self, component: str) -> Dict[str, Any]:
        """è·å–ç»„ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            component_metrics = [m for m in self.metrics if m.component == component]
            
            if not component_metrics:
                return {"component": component, "status": "no_data"}
                
            # æŒ‰æ“ä½œåˆ†ç»„
            operations = defaultdict(list)
            for metric in component_metrics:
                operations[metric.operation].append(metric)
                
            operation_stats = {}
            for operation, metrics in operations.items():
                latency_metrics = [m.value for m in metrics if m.metric_type == MetricType.LATENCY]
                
                if latency_metrics:
                    operation_stats[operation] = {
                        "count": len(latency_metrics),
                        "avg_latency_ms": statistics.mean(latency_metrics),
                        "min_latency_ms": min(latency_metrics),
                        "max_latency_ms": max(latency_metrics),
                        "p95_latency_ms": self._calculate_percentile(latency_metrics, 0.95),
                        "p99_latency_ms": self._calculate_percentile(latency_metrics, 0.99)
                    }
                    
            return {
                "component": component,
                "total_metrics": len(component_metrics),
                "operation_stats": operation_stats,
                "first_seen": min(m.timestamp for m in component_metrics).isoformat(),
                "last_seen": max(m.timestamp for m in component_metrics).isoformat()
            }
            
    def export_metrics(self, format_type: str = "json") -> str:
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®
        
        Args:
            format_type: å¯¼å‡ºæ ¼å¼ (json, csv, prometheus)
            
        Returns:
            å¯¼å‡ºçš„æ•°æ®å­—ç¬¦ä¸²
        """
        if format_type.lower() == "json":
            return self._export_json()
        elif format_type.lower() == "csv":
            return self._export_csv()
        elif format_type.lower() == "prometheus":
            return self._export_prometheus()
        else:
            raise ValueError(f"Unsupported format: {format_type}")
            
    def clear_metrics(self, before: Optional[datetime] = None):
        """æ¸…ç†æŒ‡æ ‡æ•°æ®
        
        Args:
            before: æ¸…ç†æ­¤æ—¶é—´ä¹‹å‰çš„æ•°æ®ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        with self._lock:
            if before is None:
                self.metrics.clear()
                self.alerts.clear()
                self.stats.clear()
                self._operation_counters.clear()
                self._timing_data.clear()
                self._metric_aggregators.clear()
            else:
                # æ¸…ç†æ—§æŒ‡æ ‡
                self.metrics = deque(
                    [m for m in self.metrics if m.timestamp >= before],
                    maxlen=self.history_size
                )
                
                # æ¸…ç†æ—§å‘Šè­¦
                self.alerts = deque(
                    [a for a in self.alerts if a.timestamp >= before],
                    maxlen=self.history_size
                )
                
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                self._collect_system_metrics()
                
                # èšåˆæŒ‡æ ‡
                self._aggregate_metrics()
                
                # åƒåœ¾å›æ”¶
                if self.enable_auto_gc:
                    gc.collect()
                    
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                print(f"Error in monitoring loop: {e}")
                time.sleep(self.monitoring_interval)
                
    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # å†…å­˜ä½¿ç”¨
            memory_info = self._process.memory_info()
            self.record_metric(
                MetricType.MEMORY_USAGE,
                memory_info.rss / 1024 / 1024,  # MB
                "system",
                "memory_monitor",
                "MB"
            )
            
            # CPUä½¿ç”¨
            cpu_percent = self._process.cpu_percent()
            self.record_metric(
                MetricType.CPU_USAGE,
                cpu_percent,
                "system",
                "cpu_monitor",
                "%"
            )
            
        except Exception as e:
            print(f"Error collecting system metrics: {e}")
            
    def _aggregate_metrics(self):
        """èšåˆæŒ‡æ ‡æ•°æ®"""
        try:
            with self._lock:
                now = datetime.now()
                
                for key, values in self._metric_aggregators.items():
                    if values:
                        # è®¡ç®—ååé‡ (æ“ä½œæ•°/ç§’)
                        component, operation, metric_type = key.split('.')
                        
                        if metric_type == MetricType.LATENCY.name:
                            ops_per_sec = len(values) / 60  # æ¯åˆ†é’Ÿçš„æ“ä½œæ•° / 60ç§’
                            self.record_metric(
                                MetricType.THROUGHPUT,
                                ops_per_sec,
                                component,
                                operation,
                                "ops/sec"
                            )
                            
                        # æ¸…ç†æ—§æ•°æ®
                        self._metric_aggregators[key] = []
                        
        except Exception as e:
            print(f"Error aggregating metrics: {e}")
            
    def _check_thresholds(self, metric: PerformanceMetric):
        """æ£€æŸ¥é˜ˆå€¼å¹¶ç”Ÿæˆå‘Šè­¦"""
        if metric.metric_type not in self.thresholds:
            return
            
        thresholds = self.thresholds[metric.metric_type]
        
        # ç¡®å®šå‘Šè­¦çº§åˆ«
        severity = None
        threshold_value = None
        
        if metric.metric_type in [MetricType.LATENCY, MetricType.ERROR_RATE, 
                                MetricType.MEMORY_USAGE, MetricType.CPU_USAGE]:
            # å€¼è¶Šé«˜è¶Šå·®
            if metric.value > thresholds['poor']:
                severity = AlertSeverity.CRITICAL
                threshold_value = thresholds['poor']
            elif metric.value > thresholds['acceptable']:
                severity = AlertSeverity.HIGH
                threshold_value = thresholds['acceptable']
            elif metric.value > thresholds['good']:
                severity = AlertSeverity.MEDIUM
                threshold_value = thresholds['good']
                
        elif metric.metric_type in [MetricType.THROUGHPUT, MetricType.CACHE_HIT_RATE]:
            # å€¼è¶Šé«˜è¶Šå¥½
            if metric.value < thresholds['poor']:
                severity = AlertSeverity.CRITICAL
                threshold_value = thresholds['poor']
            elif metric.value < thresholds['acceptable']:
                severity = AlertSeverity.HIGH
                threshold_value = thresholds['acceptable']
            elif metric.value < thresholds['good']:
                severity = AlertSeverity.MEDIUM
                threshold_value = thresholds['good']
                
        if severity:
            alert = PerformanceAlert(
                alert_id=str(uuid.uuid4()),
                severity=severity,
                metric_type=metric.metric_type,
                current_value=metric.value,
                threshold=threshold_value,
                component=metric.component,
                operation=metric.operation,
                message=self._generate_alert_message(metric, severity, threshold_value),
                timestamp=metric.timestamp
            )
            
            with self._lock:
                self.alerts.append(alert)
                
            # è§¦å‘å‘Šè­¦å›è°ƒ
            for callback in self.alert_callbacks:
                try:
                    callback(alert)
                except Exception as e:
                    print(f"Error in alert callback: {e}")
                    
    def _generate_alert_message(self, 
                              metric: PerformanceMetric, 
                              severity: AlertSeverity,
                              threshold: float) -> str:
        """ç”Ÿæˆå‘Šè­¦æ¶ˆæ¯"""
        return (f"{severity.value.upper()} alert: {metric.component}.{metric.operation} "
                f"{metric.metric_type.name.lower()} is {metric.value:.2f}{metric.unit}, "
                f"exceeding {threshold:.2f}{metric.unit} threshold")
                
    def _assess_performance_level(self, avg_latency: float) -> str:
        """è¯„ä¼°æ€§èƒ½çº§åˆ«"""
        if avg_latency < 10:
            return "excellent"
        elif avg_latency < 50:
            return "good"
        elif avg_latency < 200:
            return "acceptable"
        elif avg_latency < 1000:
            return "poor"
        else:
            return "critical"
            
    def _get_top_operations(self, limit: int = 5) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨æ“ä½œ"""
        with self._lock:
            sorted_ops = sorted(
                self._operation_counters.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            top_ops = []
            for operation_key, count in sorted_ops[:limit]:
                timing_data = self._timing_data.get(operation_key, [])
                avg_latency = statistics.mean(timing_data) if timing_data else 0
                
                top_ops.append({
                    "operation": operation_key,
                    "count": count,
                    "avg_latency_ms": round(avg_latency, 2)
                })
                
            return top_ops
            
    def _calculate_percentile(self, values: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not values:
            return 0.0
            
        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile)
        return sorted_values[min(index, len(sorted_values) - 1)]
        
    def _export_json(self) -> str:
        """å¯¼å‡ºJSONæ ¼å¼"""
        with self._lock:
            data = {
                "timestamp": datetime.now().isoformat(),
                "metrics": [metric.to_dict() for metric in self.metrics],
                "alerts": [alert.to_dict() for alert in self.alerts],
                "summary": self.get_performance_summary()
            }
            return json.dumps(data, indent=2, ensure_ascii=False)
            
    def _export_csv(self) -> str:
        """å¯¼å‡ºCSVæ ¼å¼"""
        import csv
        import io
        
        output = io.StringIO()
        writer = csv.writer(output)
        
        # å†™å…¥å¤´éƒ¨
        writer.writerow([
            'timestamp', 'metric_type', 'value', 'unit', 
            'component', 'operation', 'tags'
        ])
        
        # å†™å…¥æ•°æ®
        with self._lock:
            for metric in self.metrics:
                writer.writerow([
                    metric.timestamp.isoformat(),
                    metric.metric_type.name,
                    metric.value,
                    metric.unit,
                    metric.component,
                    metric.operation,
                    json.dumps(metric.tags)
                ])
                
        return output.getvalue()
        
    def _export_prometheus(self) -> str:
        """å¯¼å‡ºPrometheusæ ¼å¼"""
        lines = []
        
        with self._lock:
            # æŒ‰æŒ‡æ ‡ç±»å‹åˆ†ç»„
            metrics_by_type = defaultdict(list)
            for metric in self.metrics:
                metrics_by_type[metric.metric_type].append(metric)
                
            for metric_type, metrics in metrics_by_type.items():
                metric_name = f"config_{metric_type.name.lower()}"
                
                # æ·»åŠ HELPå’ŒTYPEæ³¨é‡Š
                lines.append(f"# HELP {metric_name} {metric_type.name} metric")
                lines.append(f"# TYPE {metric_name} gauge")
                
                # æ·»åŠ æŒ‡æ ‡æ•°æ®
                for metric in metrics[-100:]:  # åªå¯¼å‡ºæœ€è¿‘100ä¸ª
                    labels = f'component="{metric.component}",operation="{metric.operation}"'
                    if metric.tags:
                        tag_labels = ','.join(f'{k}="{v}"' for k, v in metric.tags.items())
                        labels += f",{tag_labels}"
                        
                    timestamp_ms = int(metric.timestamp.timestamp() * 1000)
                    lines.append(f"{metric_name}{{{labels}}} {metric.value} {timestamp_ms}")
                    
        return '\n'.join(lines)


# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_global_monitor: Optional[ConfigPerformanceMonitor] = None


def get_performance_monitor() -> ConfigPerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = ConfigPerformanceMonitor()
        _global_monitor.start_monitoring()
    return _global_monitor


def monitor_performance(component: str, operation: str, tags: Optional[Dict[str, Any]] = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨
    
    Args:
        component: ç»„ä»¶åç§°
        operation: æ“ä½œåç§°
        tags: é™„åŠ æ ‡ç­¾
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            monitor = get_performance_monitor()
            with monitor.measure_operation(component, operation, tags):
                return func(*args, **kwargs)
        return wrapper
    return decorator


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºç›‘æ§å™¨
    monitor = ConfigPerformanceMonitor(monitoring_interval=0.5)
    
    # æ·»åŠ å‘Šè­¦å›è°ƒ
    def alert_handler(alert: PerformanceAlert):
        print(f"ğŸš¨ {alert.severity.value.upper()}: {alert.message}")
        
    monitor.add_alert_callback(alert_handler)
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    import random
    
    for i in range(20):
        # æ¨¡æ‹Ÿé…ç½®åŠ è½½æ“ä½œ
        latency = random.uniform(5, 150)
        monitor.record_metric(
            MetricType.LATENCY,
            latency,
            "config_loader",
            "load_config",
            "ms"
        )
        
        # æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
        hit_rate = random.uniform(0.7, 0.99)
        monitor.record_metric(
            MetricType.CACHE_HIT_RATE,
            hit_rate,
            "config_cache",
            "get_config",
            "ratio"
        )
        
        time.sleep(0.1)
        
    # æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    @monitor_performance("test_component", "test_operation")
    def test_function():
        time.sleep(random.uniform(0.01, 0.1))
        
    for _ in range(10):
        test_function()
        
    time.sleep(2)
    
    # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
    summary = monitor.get_performance_summary()
    print("\n=== æ€§èƒ½æ‘˜è¦ ===")
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    
    # æ˜¾ç¤ºå‘Šè­¦
    alerts = monitor.get_alerts()
    if alerts:
        print(f"\n=== å‘Šè­¦ä¿¡æ¯ ({len(alerts)}æ¡) ===")
        for alert in alerts:
            print(f"- {alert.severity.value}: {alert.message}")
            
    # åœæ­¢ç›‘æ§
    monitor.stop_monitoring()
    
    print("\nâœ… é…ç½®æ€§èƒ½ç›‘æ§å™¨æ¼”ç¤ºå®Œæˆ")