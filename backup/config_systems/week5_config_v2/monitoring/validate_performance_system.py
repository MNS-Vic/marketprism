#!/usr/bin/env python3
"""
MarketPrism é…ç½®æ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯è„šæœ¬
é…ç½®ç®¡ç†ç³»ç»Ÿ 2.0 - Week 5 Day 5

å®Œæ•´éªŒè¯é…ç½®æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿçš„æ‰€æœ‰åŠŸèƒ½ç»„ä»¶ï¼Œ
ç¡®ä¿ç³»ç»Ÿé›†æˆæ­£å¸¸å’Œæ€§èƒ½æŒ‡æ ‡è¾¾æ ‡ã€‚

Author: MarketPrismå›¢é˜Ÿ
Created: 2025-01-29
Version: 1.0.0
"""

import sys
import time
import json
import random
import logging
import traceback
from datetime import datetime, timedelta
from typing import Dict, Any, List

# å¯¼å…¥æ€§èƒ½ä¼˜åŒ–ç³»ç»Ÿ
from . import (
    initialize_performance_system,
    ConfigPerformanceManager,
    MetricType,
    CachePriority,
    OptimizationType,
    LoadBalancingStrategy,
    ConfigNode
)


class PerformanceSystemValidator:
    """æ€§èƒ½ç³»ç»ŸéªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.test_results: List[Dict[str, Any]] = []
        self.performance_manager: ConfigPerformanceManager = None
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(self.__class__.__name__)
        
    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹é…ç½®æ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯")
        
        overall_result = {
            "validation_id": f"perf_validation_{int(datetime.now().timestamp())}",
            "start_time": datetime.now().isoformat(),
            "tests": [],
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "success_rate": 0.0
            }
        }
        
        # å®šä¹‰æµ‹è¯•åºåˆ—
        test_sequence = [
            ("ç³»ç»Ÿåˆå§‹åŒ–æµ‹è¯•", self.test_system_initialization),
            ("æ€§èƒ½ç›‘æ§å™¨æµ‹è¯•", self.test_performance_monitor),
            ("æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿæµ‹è¯•", self.test_cache_system),
            ("é…ç½®ä¼˜åŒ–å¼•æ“æµ‹è¯•", self.test_optimizer),
            ("æ€§èƒ½åˆ†æå™¨æµ‹è¯•", self.test_analyzer),
            ("è´Ÿè½½å‡è¡¡å™¨æµ‹è¯•", self.test_load_balancer),
            ("ç³»ç»Ÿé›†æˆæµ‹è¯•", self.test_system_integration),
            ("æ€§èƒ½åŸºå‡†æµ‹è¯•", self.test_performance_benchmarks),
            ("ç»¼åˆæŠ¥å‘Šç”Ÿæˆæµ‹è¯•", self.test_comprehensive_reporting)
        ]
        
        # æ‰§è¡Œæµ‹è¯•
        for test_name, test_func in test_sequence:
            try:
                self.logger.info(f"ğŸ“‹ æ‰§è¡Œæµ‹è¯•: {test_name}")
                result = test_func()
                result["test_name"] = test_name
                result["timestamp"] = datetime.now().isoformat()
                overall_result["tests"].append(result)
                
                if result["passed"]:
                    self.logger.info(f"âœ… {test_name} - é€šè¿‡")
                else:
                    self.logger.error(f"âŒ {test_name} - å¤±è´¥: {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                error_result = {
                    "test_name": test_name,
                    "passed": False,
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "timestamp": datetime.now().isoformat()
                }
                overall_result["tests"].append(error_result)
                self.logger.error(f"ğŸ’¥ {test_name} - å¼‚å¸¸: {e}")
                
        # è®¡ç®—æ€»ä½“ç»“æœ
        total_tests = len(overall_result["tests"])
        passed_tests = sum(1 for test in overall_result["tests"] if test["passed"])
        failed_tests = total_tests - passed_tests
        
        overall_result["summary"].update({
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
        })
        
        overall_result["end_time"] = datetime.now().isoformat()
        
        # æ¸…ç†èµ„æº
        self._cleanup()
        
        # è¾“å‡ºç»“æœ
        self._print_summary(overall_result)
        
        return overall_result
        
    def test_system_initialization(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–"""
        try:
            # åˆå§‹åŒ–æ€§èƒ½ç³»ç»Ÿ
            self.performance_manager = initialize_performance_system(
                auto_start=True,
                enable_monitoring=True,
                enable_caching=True,
                enable_optimization=True,
                enable_analysis=True,
                enable_load_balancing=True
            )
            
            # éªŒè¯ç»„ä»¶åˆå§‹åŒ–
            checks = {
                "monitor_initialized": self.performance_manager.monitor is not None,
                "cache_initialized": self.performance_manager.cache is not None,
                "optimizer_initialized": self.performance_manager.optimizer is not None,
                "analyzer_initialized": self.performance_manager.analyzer is not None,
                "load_balancer_initialized": self.performance_manager.load_balancer is not None
            }
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            time.sleep(2)
            
            # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
            status = self.performance_manager.get_system_status()
            
            all_initialized = all(checks.values())
            has_status = "component_status" in status
            
            return {
                "passed": all_initialized and has_status,
                "details": {
                    "initialization_checks": checks,
                    "system_status_available": has_status,
                    "enabled_components": status.get("enabled_components", {})
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_performance_monitor(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
        try:
            monitor = self.performance_manager.monitor
            
            # æµ‹è¯•æŒ‡æ ‡è®°å½•
            test_metrics = [
                (MetricType.LATENCY, 45.5, "test_component", "test_operation"),
                (MetricType.THROUGHPUT, 1200.0, "test_component", "throughput_test"),
                (MetricType.MEMORY_USAGE, 256.0, "system", "memory_check"),
                (MetricType.CACHE_HIT_RATE, 0.85, "cache", "hit_rate_check")
            ]
            
            recorded_metrics = []
            for metric_type, value, component, operation in test_metrics:
                metric_id = monitor.record_metric(metric_type, value, component, operation)
                recorded_metrics.append(metric_id)
                
            # æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            with monitor.measure_operation("test_component", "context_operation"):
                time.sleep(0.05)  # æ¨¡æ‹Ÿæ“ä½œ
                
            # è·å–æ€§èƒ½æ‘˜è¦
            summary = monitor.get_performance_summary()
            
            # éªŒè¯ç»“æœ
            checks = {
                "metrics_recorded": len(recorded_metrics) == len(test_metrics),
                "summary_available": "performance_indicators" in summary,
                "system_resources_tracked": "system_resources" in summary,
                "monitoring_active": summary.get("monitoring_status") == "active"
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "recorded_metrics_count": len(recorded_metrics),
                    "summary_keys": list(summary.keys()),
                    "performance_level": summary.get("performance_indicators", {}).get("performance_level")
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_cache_system(self) -> Dict[str, Any]:
        """æµ‹è¯•æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
        try:
            cache = self.performance_manager.cache
            
            # æµ‹è¯•ç¼“å­˜æ“ä½œ
            test_data = {
                "config.database.host": "localhost",
                "config.database.port": 5432,
                "config.api.timeout": 30,
                "config.cache.ttl": 3600
            }
            
            # å­˜å‚¨é…ç½®
            for key, value in test_data.items():
                success = cache.put(key, value, ttl=300, priority=CachePriority.HIGH)
                if not success:
                    raise Exception(f"Failed to cache {key}")
                    
            # è¯»å–é…ç½®
            cached_values = {}
            for key in test_data.keys():
                cached_values[key] = cache.get(key)
                
            # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
            stats = cache.get_global_stats()
            
            # éªŒè¯ç»“æœ
            checks = {
                "all_values_cached": all(v is not None for v in cached_values.values()),
                "values_match": all(cached_values[k] == test_data[k] for k in test_data.keys()),
                "stats_available": "total_requests" in stats,
                "hit_rate_reasonable": stats.get("global_hit_rate", 0) >= 0
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "cached_items": len(cached_values),
                    "cache_stats": stats,
                    "hit_rate": stats.get("global_hit_rate", 0)
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_optimizer(self) -> Dict[str, Any]:
        """æµ‹è¯•é…ç½®ä¼˜åŒ–å¼•æ“"""
        try:
            optimizer = self.performance_manager.optimizer
            
            # æ¨¡æ‹Ÿä¸€äº›æ€§èƒ½é—®é¢˜ä»¥è§¦å‘ä¼˜åŒ–å»ºè®®
            monitor = self.performance_manager.monitor
            
            # æ¨¡æ‹Ÿé«˜å»¶è¿Ÿ
            for _ in range(5):
                monitor.record_metric(MetricType.LATENCY, random.uniform(120, 200), "slow_component", "slow_operation")
                
            # æ¨¡æ‹Ÿä½ç¼“å­˜å‘½ä¸­ç‡
            for _ in range(5):
                monitor.record_metric(MetricType.CACHE_HIT_RATE, random.uniform(0.3, 0.6), "cache_component", "cache_operation")
                
            # ç­‰å¾…ä¸€ä¸‹è®©æ•°æ®è¢«å¤„ç†
            time.sleep(1)
            
            # è§¦å‘ä¼˜åŒ–åˆ†æ
            recommendations = optimizer.analyze_performance()
            
            # è·å–ä¼˜åŒ–æ‘˜è¦
            summary = optimizer.get_optimization_summary()
            
            # éªŒè¯ç»“æœ
            checks = {
                "analysis_completed": isinstance(recommendations, list),
                "has_recommendations": len(recommendations) > 0,
                "summary_available": "summary" in summary,
                "optimization_active": summary.get("optimization_status") == "active"
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "recommendations_count": len(recommendations),
                    "summary_keys": list(summary.keys()),
                    "recent_recommendations": [rec.description for rec in recommendations[:3]]
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_analyzer(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½åˆ†æå™¨"""
        try:
            analyzer = self.performance_manager.analyzer
            
            # ç”Ÿæˆä¸€äº›åˆ†ææ•°æ®
            monitor = self.performance_manager.monitor
            
            # æ¨¡æ‹Ÿè¶‹åŠ¿æ•°æ®
            base_latency = 50
            for i in range(15):
                latency = base_latency + i * 2 + random.uniform(-5, 5)
                monitor.record_metric(MetricType.LATENCY, latency, "trend_component", "trend_operation")
                time.sleep(0.1)
                
            # æ‰§è¡Œåˆ†æ
            trends = analyzer.analyze_trends()
            anomalies = analyzer.detect_anomalies()
            bottlenecks = analyzer.analyze_bottlenecks()
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            report = analyzer.generate_performance_report(include_charts=False)
            
            # è·å–åˆ†ææ‘˜è¦
            summary = analyzer.get_analysis_summary()
            
            # éªŒè¯ç»“æœ
            checks = {
                "trends_analyzed": isinstance(trends, list),
                "anomalies_detected": isinstance(anomalies, list),
                "bottlenecks_analyzed": isinstance(bottlenecks, list),
                "report_generated": report is not None and hasattr(report, 'report_id'),
                "summary_available": "stats" in summary
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "trends_count": len(trends),
                    "anomalies_count": len(anomalies),
                    "bottlenecks_count": len(bottlenecks),
                    "report_id": getattr(report, 'report_id', None),
                    "analysis_summary": summary
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_load_balancer(self) -> Dict[str, Any]:
        """æµ‹è¯•è´Ÿè½½å‡è¡¡å™¨"""
        try:
            load_balancer = self.performance_manager.load_balancer
            
            # æ·»åŠ æµ‹è¯•èŠ‚ç‚¹
            test_nodes = [
                ConfigNode(
                    node_id="test_node_1",
                    name="æµ‹è¯•èŠ‚ç‚¹1",
                    host="127.0.0.1",
                    port=8001,
                    weight=100
                ),
                ConfigNode(
                    node_id="test_node_2", 
                    name="æµ‹è¯•èŠ‚ç‚¹2",
                    host="127.0.0.1",
                    port=8002,
                    weight=150
                )
            ]
            
            # æ·»åŠ èŠ‚ç‚¹
            for node in test_nodes:
                success = load_balancer.add_node(node)
                if not success:
                    raise Exception(f"Failed to add node {node.name}")
                    
            # æµ‹è¯•ä¸åŒçš„è´Ÿè½½å‡è¡¡ç­–ç•¥
            strategies_tested = []
            for strategy in [LoadBalancingStrategy.ROUND_ROBIN, LoadBalancingStrategy.WEIGHTED_ROUND_ROBIN]:
                load_balancer.set_strategy(strategy)
                strategies_tested.append(strategy.name)
                
            # è·å–è´Ÿè½½å‡è¡¡å™¨ç»Ÿè®¡
            stats = load_balancer.get_load_balancer_stats()
            
            # è·å–èŠ‚ç‚¹çŠ¶æ€
            node_statuses = load_balancer.get_node_status()
            
            # éªŒè¯ç»“æœ
            checks = {
                "nodes_added": len(test_nodes) == 2,
                "strategies_tested": len(strategies_tested) == 2,
                "stats_available": "nodes" in stats,
                "node_statuses_available": isinstance(node_statuses, list) and len(node_statuses) > 0
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "nodes_count": len(test_nodes),
                    "strategies_tested": strategies_tested,
                    "load_balancer_stats": stats,
                    "healthy_nodes": stats.get("nodes", {}).get("healthy", 0)
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_system_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
        try:
            # æµ‹è¯•ç»„ä»¶é—´åä½œ
            monitor = self.performance_manager.monitor
            cache = self.performance_manager.cache
            optimizer = self.performance_manager.optimizer
            
            # æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµ
            
            # 1. è®°å½•æ€§èƒ½æŒ‡æ ‡
            for i in range(10):
                latency = random.uniform(20, 80)
                monitor.record_metric(MetricType.LATENCY, latency, "integration_test", "workflow")
                
            # 2. ç¼“å­˜ä¸€äº›æ•°æ®
            cache.put("integration.test.config", {"value": "test_data"}, priority=CachePriority.HIGH)
            cached_value = cache.get("integration.test.config")
            
            # 3. è§¦å‘ä¼˜åŒ–åˆ†æ
            recommendations = optimizer.analyze_performance()
            
            # 4. è·å–ç³»ç»ŸçŠ¶æ€
            system_status = self.performance_manager.get_system_status()
            
            # éªŒè¯é›†æˆ
            checks = {
                "monitor_cache_integration": cached_value is not None,
                "monitor_optimizer_integration": len(recommendations) >= 0,
                "system_status_complete": len(system_status.get("component_status", {})) >= 3,
                "all_components_active": all(
                    comp.get("active", False) 
                    for comp in system_status.get("component_status", {}).values()
                )
            }
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "cached_value_retrieved": cached_value is not None,
                    "recommendations_generated": len(recommendations),
                    "active_components": len(system_status.get("component_status", {})),
                    "system_status_keys": list(system_status.keys())
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_performance_benchmarks(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        try:
            monitor = self.performance_manager.monitor
            cache = self.performance_manager.cache
            
            # æ€§èƒ½åŸºå‡†æµ‹è¯•
            benchmarks = {}
            
            # 1. æŒ‡æ ‡è®°å½•æ€§èƒ½
            start_time = time.perf_counter()
            for i in range(100):
                monitor.record_metric(MetricType.LATENCY, random.uniform(10, 50), "benchmark", "metric_test")
            end_time = time.perf_counter()
            benchmarks["metric_recording_100ops"] = (end_time - start_time) * 1000  # ms
            
            # 2. ç¼“å­˜è¯»å†™æ€§èƒ½
            start_time = time.perf_counter()
            for i in range(100):
                cache.put(f"benchmark.key.{i}", f"value_{i}")
            end_time = time.perf_counter()
            benchmarks["cache_write_100ops"] = (end_time - start_time) * 1000  # ms
            
            start_time = time.perf_counter()
            for i in range(100):
                cache.get(f"benchmark.key.{i}")
            end_time = time.perf_counter()
            benchmarks["cache_read_100ops"] = (end_time - start_time) * 1000  # ms
            
            # 3. ç³»ç»ŸçŠ¶æ€è·å–æ€§èƒ½
            start_time = time.perf_counter()
            for _ in range(10):
                self.performance_manager.get_system_status()
            end_time = time.perf_counter()
            benchmarks["system_status_10ops"] = (end_time - start_time) * 1000  # ms
            
            # æ€§èƒ½è¦æ±‚æ£€æŸ¥
            performance_checks = {
                "metric_recording_fast": benchmarks["metric_recording_100ops"] < 1000,  # <1s for 100 ops
                "cache_write_fast": benchmarks["cache_write_100ops"] < 500,  # <500ms for 100 ops
                "cache_read_fast": benchmarks["cache_read_100ops"] < 100,   # <100ms for 100 ops
                "status_query_fast": benchmarks["system_status_10ops"] < 1000  # <1s for 10 ops
            }
            
            return {
                "passed": all(performance_checks.values()),
                "details": {
                    "benchmarks_ms": benchmarks,
                    "performance_checks": performance_checks,
                    "overall_performance": "excellent" if all(performance_checks.values()) else "needs_optimization"
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def test_comprehensive_reporting(self) -> Dict[str, Any]:
        """æµ‹è¯•ç»¼åˆæŠ¥å‘Šç”Ÿæˆ"""
        try:
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self.performance_manager.generate_comprehensive_report()
            
            # éªŒè¯æŠ¥å‘Šç»“æ„
            required_sections = ["report_id", "generated_at", "title", "summary", "components"]
            
            checks = {
                "report_generated": report is not None,
                "has_required_sections": all(section in report for section in required_sections),
                "has_summary": "overall_health" in report.get("summary", {}),
                "has_components": len(report.get("components", {})) > 0,
                "report_is_serializable": self._test_json_serializable(report)
            }
            
            # æ£€æŸ¥ç»„ä»¶æŠ¥å‘Š
            components = report.get("components", {})
            component_checks = {
                f"{comp}_reported": comp in components 
                for comp in ["monitoring", "caching", "optimization", "analysis"]
            }
            
            checks.update(component_checks)
            
            return {
                "passed": all(checks.values()),
                "details": {
                    "checks": checks,
                    "report_id": report.get("report_id"),
                    "overall_health": report.get("summary", {}).get("overall_health"),
                    "performance_score": report.get("summary", {}).get("performance_score"),
                    "components_count": len(components),
                    "report_size_kb": len(json.dumps(report, default=str)) / 1024
                }
            }
            
        except Exception as e:
            return {
                "passed": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
            
    def _test_json_serializable(self, obj: Any) -> bool:
        """æµ‹è¯•å¯¹è±¡æ˜¯å¦å¯JSONåºåˆ—åŒ–"""
        try:
            json.dumps(obj, default=str)
            return True
        except Exception:
            return False
            
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.performance_manager:
                self.performance_manager.stop_all_services()
                self.logger.info("âœ“ ç³»ç»Ÿèµ„æºå·²æ¸…ç†")
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
            
    def _print_summary(self, result: Dict[str, Any]):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ MarketPrism é…ç½®æ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯æŠ¥å‘Š")
        print("="*80)
        
        summary = result["summary"]
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {result['start_time']} - {result['end_time']}")
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for i, test in enumerate(result["tests"], 1):
            status = "âœ… é€šè¿‡" if test["passed"] else "âŒ å¤±è´¥"
            print(f"   {i:2d}. {test['test_name']:<30} {status}")
            if not test["passed"] and "error" in test:
                print(f"       é”™è¯¯: {test['error']}")
                
        # æ€»ä½“è¯„ä¼°
        print(f"\nğŸ† æ€»ä½“è¯„ä¼°:")
        if summary['success_rate'] >= 90:
            print("   ğŸŒŸ ä¼˜ç§€ - ç³»ç»ŸåŠŸèƒ½å®Œæ•´ï¼Œæ€§èƒ½ä¼˜å¼‚")
        elif summary['success_rate'] >= 75:
            print("   ğŸ‘ è‰¯å¥½ - ç³»ç»ŸåŠŸèƒ½åŸºæœ¬å®Œæ•´ï¼Œæ€§èƒ½è‰¯å¥½")
        elif summary['success_rate'] >= 60:
            print("   âš ï¸  å¯æ¥å— - ç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print("   ğŸš¨ éœ€è¦æ”¹è¿› - ç³»ç»Ÿå­˜åœ¨é‡è¦é—®é¢˜ï¼Œéœ€è¦ä¿®å¤")
            
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MarketPrism é…ç½®æ€§èƒ½ä¼˜åŒ–ç³»ç»ŸéªŒè¯å¼€å§‹")
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–éªŒè¯ç¯å¢ƒ...")
    
    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = PerformanceSystemValidator()
        
        # è¿è¡ŒéªŒè¯
        result = validator.run_all_tests()
        
        # ä¿å­˜ç»“æœ
        result_file = f"performance_validation_result_{int(datetime.now().timestamp())}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            
        print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        
        # è¿”å›é€€å‡ºç 
        success_rate = result["summary"]["success_rate"]
        if success_rate >= 90:
            sys.exit(0)  # ä¼˜ç§€
        elif success_rate >= 75:
            sys.exit(0)  # è‰¯å¥½
        else:
            sys.exit(1)  # éœ€è¦æ”¹è¿›
            
    except Exception as e:
        print(f"ğŸ’¥ éªŒè¯è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        traceback.print_exc()
        sys.exit(2)


if __name__ == "__main__":
    main()