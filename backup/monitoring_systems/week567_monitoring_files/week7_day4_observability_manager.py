#!/usr/bin/env python3
"""
Week 7 Day 4: å¯è§‚æµ‹æ€§å’Œç›‘æ§ç³»ç»Ÿ
MarketPrism Observability and Monitoring System

ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°ï¼Œæä¾›å®Œæ•´çš„ç›‘æ§ã€å‘Šè­¦ã€è¿½è¸ªå’Œåˆ†æèƒ½åŠ›
"""

import asyncio
import json
import time
import hashlib
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from abc import ABC, abstractmethod
import logging
from collections import defaultdict, deque
import statistics
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('observability_system.log')
    ]
)
logger = logging.getLogger(__name__)

# ===================== æ•°æ®æ¨¡å‹å®šä¹‰ =====================

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    COUNTER = "counter"          # è®¡æ•°å™¨ - ç´¯ç§¯å‹æŒ‡æ ‡
    GAUGE = "gauge"              # ä»ªè¡¨ - ç¬æ—¶å€¼æŒ‡æ ‡
    HISTOGRAM = "histogram"      # ç›´æ–¹å›¾ - åˆ†å¸ƒç»Ÿè®¡
    SUMMARY = "summary"          # æ‘˜è¦ - åˆ†ä½æ•°ç»Ÿè®¡
    CUSTOM = "custom"            # è‡ªå®šä¹‰ - ä¸šåŠ¡æŒ‡æ ‡

class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«"""
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ComponentStatus(Enum):
    """ç»„ä»¶çŠ¶æ€"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    DOWN = "down"

@dataclass
class MetricPoint:
    """æŒ‡æ ‡æ•°æ®ç‚¹"""
    name: str
    value: Union[int, float]
    labels: Dict[str, str]
    timestamp: datetime
    metric_type: MetricType

@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: datetime
    level: LogLevel
    message: str
    source: str
    labels: Dict[str, str]
    trace_id: Optional[str] = None
    span_id: Optional[str] = None

@dataclass
class TraceSpan:
    """è¿½è¸ªè·¨åº¦"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: datetime
    end_time: Optional[datetime]
    duration_ms: Optional[float]
    tags: Dict[str, str]
    logs: List[Dict[str, Any]]
    status: str = "ok"

@dataclass
class Alert:
    """å‘Šè­¦"""
    id: str
    name: str
    description: str
    severity: AlertSeverity
    source: str
    labels: Dict[str, str]
    timestamp: datetime
    resolved: bool = False
    resolved_at: Optional[datetime] = None

@dataclass
class SLOConfig:
    """SLOé…ç½®"""
    name: str
    description: str
    sli_query: str
    target: float  # ç›®æ ‡å€¼ (ä¾‹å¦‚ 99.9%)
    error_budget_burn_rate: float
    time_window: str  # ä¾‹å¦‚ "30d", "7d"
    alert_threshold: float

# ===================== æ ¸å¿ƒç»„ä»¶å®ç° =====================

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨ - å¤šç»´åº¦æŒ‡æ ‡æ”¶é›†å’Œå­˜å‚¨"""
    
    def __init__(self):
        self.metrics_store: Dict[str, List[MetricPoint]] = defaultdict(list)
        self.aggregations: Dict[str, Dict[str, Any]] = defaultdict(dict)
        self.retention_policy = {"default": timedelta(days=30)}
        self.compression_ratio = 0.8
        
    async def collect_metric(self, metric: MetricPoint) -> bool:
        """æ”¶é›†æŒ‡æ ‡"""
        try:
            # éªŒè¯æŒ‡æ ‡æ•°æ®
            if not metric.name or metric.value is None:
                return False
                
            # å­˜å‚¨æŒ‡æ ‡
            self.metrics_store[metric.name].append(metric)
            
            # å®æ—¶èšåˆ
            await self._update_aggregations(metric)
            
            # åº”ç”¨ä¿ç•™ç­–ç•¥
            await self._apply_retention_policy(metric.name)
            
            logger.debug(f"æ”¶é›†æŒ‡æ ‡: {metric.name}={metric.value}")
            return True
            
        except Exception as e:
            logger.error(f"æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            return False
    
    async def _update_aggregations(self, metric: MetricPoint):
        """æ›´æ–°èšåˆæ•°æ®"""
        key = metric.name
        
        if key not in self.aggregations:
            self.aggregations[key] = {
                "count": 0,
                "sum": 0,
                "min": float('inf'),
                "max": float('-inf'),
                "avg": 0,
                "last_value": 0,
                "rate": 0,
                "percentiles": {}
            }
        
        agg = self.aggregations[key]
        agg["count"] += 1
        agg["sum"] += metric.value
        agg["min"] = min(agg["min"], metric.value)
        agg["max"] = max(agg["max"], metric.value)
        agg["avg"] = agg["sum"] / agg["count"]
        agg["last_value"] = metric.value
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        values = [m.value for m in self.metrics_store[key]]
        if len(values) >= 10:  # è¶³å¤Ÿçš„æ•°æ®ç‚¹
            agg["percentiles"] = {
                "p50": statistics.median(values),
                "p95": statistics.quantiles(values, n=20)[18] if len(values) > 20 else max(values),
                "p99": statistics.quantiles(values, n=100)[98] if len(values) > 100 else max(values)
            }
    
    async def _apply_retention_policy(self, metric_name: str):
        """åº”ç”¨æ•°æ®ä¿ç•™ç­–ç•¥"""
        retention = self.retention_policy.get(metric_name, self.retention_policy["default"])
        cutoff_time = datetime.now() - retention
        
        # ç§»é™¤è¿‡æœŸæ•°æ®
        self.metrics_store[metric_name] = [
            m for m in self.metrics_store[metric_name] 
            if m.timestamp > cutoff_time
        ]
    
    async def query_metrics(self, metric_name: str, 
                          start_time: Optional[datetime] = None,
                          end_time: Optional[datetime] = None,
                          labels: Optional[Dict[str, str]] = None) -> List[MetricPoint]:
        """æŸ¥è¯¢æŒ‡æ ‡"""
        try:
            metrics = self.metrics_store.get(metric_name, [])
            
            # æ—¶é—´è¿‡æ»¤
            if start_time:
                metrics = [m for m in metrics if m.timestamp >= start_time]
            if end_time:
                metrics = [m for m in metrics if m.timestamp <= end_time]
            
            # æ ‡ç­¾è¿‡æ»¤
            if labels:
                metrics = [
                    m for m in metrics 
                    if all(m.labels.get(k) == v for k, v in labels.items())
                ]
            
            return metrics
            
        except Exception as e:
            logger.error(f"æŒ‡æ ‡æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    async def get_aggregations(self, metric_name: str) -> Dict[str, Any]:
        """è·å–èšåˆæ•°æ®"""
        return self.aggregations.get(metric_name, {})
    
    async def compress_data(self) -> Dict[str, Any]:
        """æ•°æ®å‹ç¼©"""
        compressed_size = 0
        original_size = 0
        
        for metric_name, points in self.metrics_store.items():
            original_size += len(points)
            # æ¨¡æ‹Ÿå‹ç¼© - ä¿ç•™å…³é”®æ•°æ®ç‚¹
            if len(points) > 1000:
                # ä¿ç•™æœ€æ–°çš„500ä¸ªç‚¹ + é‡‡æ ·çš„500ä¸ªå†å²ç‚¹
                recent_points = points[-500:]
                sampled_points = points[:-500:max(1, len(points[:-500]) // 500)]
                self.metrics_store[metric_name] = recent_points + sampled_points
                compressed_size += len(self.metrics_store[metric_name])
            else:
                compressed_size += len(points)
        
        compression_ratio = 1 - (compressed_size / max(original_size, 1))
        return {
            "original_size": original_size,
            "compressed_size": compressed_size,
            "compression_ratio": compression_ratio
        }

class LogAggregator:
    """æ—¥å¿—èšåˆå™¨ - å¤šæºæ—¥å¿—æ”¶é›†å’Œåˆ†æ"""
    
    def __init__(self):
        self.log_store: List[LogEntry] = []
        self.index: Dict[str, List[int]] = defaultdict(list)  # å€’æ’ç´¢å¼•
        self.retention_days = 30
        self.max_logs = 100000
        
    async def ingest_log(self, log_entry: LogEntry) -> bool:
        """æ¥æ”¶æ—¥å¿—"""
        try:
            # æ·»åŠ åˆ°å­˜å‚¨
            self.log_store.append(log_entry)
            
            # æ›´æ–°ç´¢å¼•
            log_index = len(self.log_store) - 1
            await self._update_index(log_entry, log_index)
            
            # åº”ç”¨ä¿ç•™ç­–ç•¥
            await self._apply_log_retention()
            
            logger.debug(f"æ¥æ”¶æ—¥å¿—: {log_entry.level} - {log_entry.message[:100]}")
            return True
            
        except Exception as e:
            logger.error(f"æ—¥å¿—æ¥æ”¶å¤±è´¥: {e}")
            return False
    
    async def _update_index(self, log_entry: LogEntry, index: int):
        """æ›´æ–°æœç´¢ç´¢å¼•"""
        # ç´¢å¼•å…³é”®å­—æ®µ
        keywords = [
            log_entry.level.value,
            log_entry.source,
            *log_entry.message.split(),
            *log_entry.labels.keys(),
            *log_entry.labels.values()
        ]
        
        for keyword in keywords:
            if isinstance(keyword, str) and len(keyword) > 1:  # å¿½ç•¥å¤ªçŸ­çš„è¯
                self.index[keyword.lower()].append(index)
    
    async def _apply_log_retention(self):
        """åº”ç”¨æ—¥å¿—ä¿ç•™ç­–ç•¥"""
        # æŒ‰æ—¶é—´ä¿ç•™
        cutoff_time = datetime.now() - timedelta(days=self.retention_days)
        valid_logs = []
        new_index = defaultdict(list)
        
        for i, log in enumerate(self.log_store):
            if log.timestamp > cutoff_time:
                new_index_pos = len(valid_logs)
                valid_logs.append(log)
                
                # é‡å»ºç´¢å¼•
                keywords = [
                    log.level.value,
                    log.source,
                    *log.message.split(),
                    *log.labels.keys(),
                    *log.labels.values()
                ]
                
                for keyword in keywords:
                    if isinstance(keyword, str) and len(keyword) > 1:
                        new_index[keyword.lower()].append(new_index_pos)
        
        # æŒ‰æ•°é‡ä¿ç•™ - å¦‚æœè¿˜æ˜¯å¤ªå¤šï¼Œä¿ç•™æœ€æ–°çš„
        if len(valid_logs) > self.max_logs:
            valid_logs = valid_logs[-self.max_logs:]
            # é‡å»ºç´¢å¼•
            new_index = defaultdict(list)
            for i, log in enumerate(valid_logs):
                keywords = [
                    log.level.value,
                    log.source,
                    *log.message.split(),
                    *log.labels.keys(),
                    *log.labels.values()
                ]
                
                for keyword in keywords:
                    if isinstance(keyword, str) and len(keyword) > 1:
                        new_index[keyword.lower()].append(i)
        
        self.log_store = valid_logs
        self.index = new_index
    
    async def search_logs(self, query: str, 
                         level: Optional[LogLevel] = None,
                         source: Optional[str] = None,
                         start_time: Optional[datetime] = None,
                         end_time: Optional[datetime] = None,
                         limit: int = 100) -> List[LogEntry]:
        """æœç´¢æ—¥å¿—"""
        try:
            # å…¨æ–‡æœç´¢
            matching_indices = set()
            query_terms = query.lower().split()
            
            if query_terms:
                # è·å–ç¬¬ä¸€ä¸ªè¯çš„åŒ¹é…
                first_term = query_terms[0]
                if first_term in self.index:
                    matching_indices = set(self.index[first_term])
                    
                    # ä¸å…¶ä»–è¯æ±‚äº¤é›†
                    for term in query_terms[1:]:
                        if term in self.index:
                            matching_indices &= set(self.index[term])
                        else:
                            matching_indices = set()  # æ²¡æœ‰åŒ¹é…
                            break
            else:
                # æ²¡æœ‰æŸ¥è¯¢è¯ï¼Œè¿”å›æ‰€æœ‰
                matching_indices = set(range(len(self.log_store)))
            
            # åº”ç”¨è¿‡æ»¤å™¨
            results = []
            for i in matching_indices:
                if i < len(self.log_store):
                    log = self.log_store[i]
                    
                    # çº§åˆ«è¿‡æ»¤
                    if level and log.level != level:
                        continue
                    
                    # æ¥æºè¿‡æ»¤
                    if source and log.source != source:
                        continue
                    
                    # æ—¶é—´è¿‡æ»¤
                    if start_time and log.timestamp < start_time:
                        continue
                    if end_time and log.timestamp > end_time:
                        continue
                    
                    results.append(log)
            
            # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            results.sort(key=lambda x: x.timestamp, reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"æ—¥å¿—æœç´¢å¤±è´¥: {e}")
            return []
    
    async def get_log_statistics(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡"""
        try:
            total_logs = len(self.log_store)
            if total_logs == 0:
                return {"total_logs": 0}
            
            # æŒ‰çº§åˆ«ç»Ÿè®¡
            level_counts = defaultdict(int)
            source_counts = defaultdict(int)
            hourly_counts = defaultdict(int)
            
            for log in self.log_store:
                level_counts[log.level.value] += 1
                source_counts[log.source] += 1
                hour_key = log.timestamp.strftime("%Y-%m-%d %H:00")
                hourly_counts[hour_key] += 1
            
            return {
                "total_logs": total_logs,
                "level_distribution": dict(level_counts),
                "source_distribution": dict(source_counts),
                "hourly_distribution": dict(hourly_counts),
                "index_size": len(self.index),
                "retention_days": self.retention_days
            }
            
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}

class TracingSystem:
    """åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ - ç«¯åˆ°ç«¯è¯·æ±‚è¿½è¸ª"""
    
    def __init__(self):
        self.traces: Dict[str, List[TraceSpan]] = defaultdict(list)
        self.span_index: Dict[str, TraceSpan] = {}
        self.sampling_rate = 0.1  # 10%é‡‡æ ·ç‡
        self.retention_hours = 24
        
    async def start_span(self, operation_name: str, 
                        parent_span_id: Optional[str] = None,
                        trace_id: Optional[str] = None) -> TraceSpan:
        """å¼€å§‹æ–°çš„è·¨åº¦"""
        try:
            if trace_id is None:
                trace_id = str(uuid.uuid4())
            
            span_id = str(uuid.uuid4())
            
            span = TraceSpan(
                trace_id=trace_id,
                span_id=span_id,
                parent_span_id=parent_span_id,
                operation_name=operation_name,
                start_time=datetime.now(),
                end_time=None,
                duration_ms=None,
                tags={},
                logs=[]
            )
            
            # é‡‡æ ·å†³ç­–
            if random.random() < self.sampling_rate:
                self.traces[trace_id].append(span)
                self.span_index[span_id] = span
                
            logger.debug(f"å¼€å§‹è·¨åº¦: {operation_name} (trace_id: {trace_id})")
            return span
            
        except Exception as e:
            logger.error(f"å¼€å§‹è·¨åº¦å¤±è´¥: {e}")
            raise
    
    async def finish_span(self, span: TraceSpan, 
                         tags: Optional[Dict[str, str]] = None,
                         status: str = "ok") -> bool:
        """ç»“æŸè·¨åº¦"""
        try:
            span.end_time = datetime.now()
            span.duration_ms = (span.end_time - span.start_time).total_seconds() * 1000
            span.status = status
            
            if tags:
                span.tags.update(tags)
            
            logger.debug(f"ç»“æŸè·¨åº¦: {span.operation_name} (è€—æ—¶: {span.duration_ms:.2f}ms)")
            return True
            
        except Exception as e:
            logger.error(f"ç»“æŸè·¨åº¦å¤±è´¥: {e}")
            return False
    
    async def add_span_log(self, span: TraceSpan, log_data: Dict[str, Any]) -> bool:
        """æ·»åŠ è·¨åº¦æ—¥å¿—"""
        try:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                **log_data
            }
            span.logs.append(log_entry)
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ è·¨åº¦æ—¥å¿—å¤±è´¥: {e}")
            return False
    
    async def get_trace(self, trace_id: str) -> List[TraceSpan]:
        """è·å–å®Œæ•´è¿½è¸ª"""
        return self.traces.get(trace_id, [])
    
    async def find_traces(self, operation_name: Optional[str] = None,
                         min_duration_ms: Optional[float] = None,
                         max_duration_ms: Optional[float] = None,
                         tags: Optional[Dict[str, str]] = None,
                         limit: int = 100) -> List[str]:
        """æŸ¥æ‰¾è¿½è¸ª"""
        try:
            matching_trace_ids = []
            
            for trace_id, spans in self.traces.items():
                if len(matching_trace_ids) >= limit:
                    break
                    
                match = False
                for span in spans:
                    # æ“ä½œåè¿‡æ»¤
                    if operation_name and span.operation_name != operation_name:
                        continue
                    
                    # æŒç»­æ—¶é—´è¿‡æ»¤
                    if span.duration_ms is not None:
                        if min_duration_ms and span.duration_ms < min_duration_ms:
                            continue
                        if max_duration_ms and span.duration_ms > max_duration_ms:
                            continue
                    
                    # æ ‡ç­¾è¿‡æ»¤
                    if tags:
                        if not all(span.tags.get(k) == v for k, v in tags.items()):
                            continue
                    
                    match = True
                    break
                
                if match:
                    matching_trace_ids.append(trace_id)
            
            return matching_trace_ids
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾è¿½è¸ªå¤±è´¥: {e}")
            return []
    
    async def get_service_map(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡ä¾èµ–å›¾"""
        try:
            services = set()
            dependencies = []
            
            for spans in self.traces.values():
                # æŒ‰æ—¶é—´æ’åº
                sorted_spans = sorted(spans, key=lambda x: x.start_time)
                
                for span in sorted_spans:
                    service = span.tags.get("service.name", "unknown")
                    services.add(service)
                    
                    # æ‰¾åˆ°çˆ¶å­å…³ç³»
                    if span.parent_span_id:
                        parent_span = self.span_index.get(span.parent_span_id)
                        if parent_span:
                            parent_service = parent_span.tags.get("service.name", "unknown")
                            if parent_service != service:
                                dependency = {
                                    "from": parent_service,
                                    "to": service,
                                    "operation": span.operation_name
                                }
                                if dependency not in dependencies:
                                    dependencies.append(dependency)
            
            return {
                "services": list(services),
                "dependencies": dependencies,
                "total_traces": len(self.traces),
                "total_spans": sum(len(spans) for spans in self.traces.values())
            }
            
        except Exception as e:
            logger.error(f"è·å–æœåŠ¡ä¾èµ–å›¾å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def cleanup_old_traces(self):
        """æ¸…ç†æ—§çš„è¿½è¸ªæ•°æ®"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=self.retention_hours)
            traces_to_remove = []
            
            for trace_id, spans in self.traces.items():
                if all(span.start_time < cutoff_time for span in spans):
                    traces_to_remove.append(trace_id)
            
            for trace_id in traces_to_remove:
                spans = self.traces.pop(trace_id)
                for span in spans:
                    self.span_index.pop(span.span_id, None)
            
            logger.info(f"æ¸…ç†äº† {len(traces_to_remove)} ä¸ªè¿‡æœŸè¿½è¸ª")
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿½è¸ªæ•°æ®å¤±è´¥: {e}")

class ObservabilityManager:
    """å¯è§‚æµ‹æ€§ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†å’Œåè°ƒæ‰€æœ‰å¯è§‚æµ‹æ€§ç»„ä»¶"""
    
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.log_aggregator = LogAggregator()
        self.tracing_system = TracingSystem()
        
        self.status = ComponentStatus.DOWN
        self.start_time: Optional[datetime] = None
        self.components = {}
        self.health_checks = {}
        
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å¯è§‚æµ‹æ€§ç³»ç»Ÿ"""
        try:
            logger.info("åˆå§‹åŒ–å¯è§‚æµ‹æ€§ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–ç»„ä»¶
            self.components = {
                "metrics_collector": self.metrics_collector,
                "log_aggregator": self.log_aggregator,
                "tracing_system": self.tracing_system
            }
            
            # è®¾ç½®å¥åº·æ£€æŸ¥
            self.health_checks = {
                "metrics_collector": self._check_metrics_health,
                "log_aggregator": self._check_logs_health,
                "tracing_system": self._check_tracing_health
            }
            
            self.status = ComponentStatus.HEALTHY
            self.start_time = datetime.now()
            
            logger.info("å¯è§‚æµ‹æ€§ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            self.status = ComponentStatus.DOWN
            return False
    
    async def _check_metrics_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥æŒ‡æ ‡æ”¶é›†å™¨å¥åº·çŠ¶æ€"""
        try:
            metrics_count = sum(len(points) for points in self.metrics_collector.metrics_store.values())
            compression_stats = await self.metrics_collector.compress_data()
            
            return {
                "status": "healthy",
                "metrics_count": metrics_count,
                "unique_metrics": len(self.metrics_collector.metrics_store),
                "compression_ratio": compression_stats.get("compression_ratio", 0)
            }
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def _check_logs_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ—¥å¿—èšåˆå™¨å¥åº·çŠ¶æ€"""
        try:
            stats = await self.log_aggregator.get_log_statistics()
            return {
                "status": "healthy",
                **stats
            }
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def _check_tracing_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¿½è¸ªç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            service_map = await self.tracing_system.get_service_map()
            return {
                "status": "healthy",
                **service_map,
                "sampling_rate": self.tracing_system.sampling_rate
            }
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            uptime = datetime.now() - self.start_time if self.start_time else timedelta(0)
            
            # æ£€æŸ¥å„ç»„ä»¶å¥åº·çŠ¶æ€
            component_health = {}
            overall_health = "healthy"
            
            for component_name, health_check in self.health_checks.items():
                health = await health_check()
                component_health[component_name] = health
                
                if health.get("status") != "healthy":
                    overall_health = "degraded"
            
            return {
                "overall_status": overall_health,
                "uptime_seconds": uptime.total_seconds(),
                "start_time": self.start_time.isoformat() if self.start_time else None,
                "components": component_health,
                "system_info": {
                    "version": "1.0.0",
                    "environment": "production"
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return {
                "overall_status": "unhealthy",
                "error": str(e)
            }
    
    async def get_metrics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # æ”¶é›†å„ç»„ä»¶æŒ‡æ ‡
            metrics_stats = {}
            for metric_name, aggregations in self.metrics_collector.aggregations.items():
                metrics_stats[metric_name] = aggregations
            
            log_stats = await self.log_aggregator.get_log_statistics()
            service_map = await self.tracing_system.get_service_map()
            
            return {
                "metrics": {
                    "total_metrics": len(self.metrics_collector.metrics_store),
                    "aggregations": metrics_stats
                },
                "logs": log_stats,
                "tracing": service_map,
                "system": {
                    "memory_usage_mb": 0,  # æ¨¡æ‹Ÿ
                    "cpu_usage_percent": 0,  # æ¨¡æ‹Ÿ
                    "disk_usage_mb": 0  # æ¨¡æ‹Ÿ
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def shutdown(self) -> bool:
        """å…³é—­å¯è§‚æµ‹æ€§ç³»ç»Ÿ"""
        try:
            logger.info("å…³é—­å¯è§‚æµ‹æ€§ç³»ç»Ÿ...")
            
            # æ¸…ç†èµ„æº
            await self.tracing_system.cleanup_old_traces()
            
            self.status = ComponentStatus.DOWN
            logger.info("å¯è§‚æµ‹æ€§ç³»ç»Ÿå·²å…³é—­")
            return True
            
        except Exception as e:
            logger.error(f"å…³é—­å¤±è´¥: {e}")
            return False

# ===================== æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•° =====================

async def demonstrate_observability_system():
    """æ¼”ç¤ºå¯è§‚æµ‹æ€§ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸ” å¯è§‚æµ‹æ€§å’Œç›‘æ§ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    obs_manager = ObservabilityManager()
    await obs_manager.initialize()
    
    # 1. æŒ‡æ ‡æ”¶é›†æ¼”ç¤º
    print("\nğŸ“Š æŒ‡æ ‡æ”¶é›†æ¼”ç¤º:")
    metrics = [
        MetricPoint("api_requests_total", 100, {"method": "GET", "status": "200"}, datetime.now(), MetricType.COUNTER),
        MetricPoint("api_latency_ms", 150.5, {"endpoint": "/api/v1/trades"}, datetime.now(), MetricType.GAUGE),
        MetricPoint("cpu_usage_percent", 75.2, {"node": "worker-1"}, datetime.now(), MetricType.GAUGE),
        MetricPoint("memory_usage_mb", 2048, {"service": "collector"}, datetime.now(), MetricType.GAUGE)
    ]
    
    for metric in metrics:
        await obs_manager.metrics_collector.collect_metric(metric)
        print(f"  âœ… æ”¶é›†æŒ‡æ ‡: {metric.name} = {metric.value}")
    
    # æŸ¥è¯¢èšåˆæ•°æ®
    aggregations = await obs_manager.metrics_collector.get_aggregations("api_latency_ms")
    print(f"  ğŸ“ˆ å»¶è¿Ÿèšåˆ: å¹³å‡={aggregations.get('avg', 0):.2f}ms, æœ€å¤§={aggregations.get('max', 0):.2f}ms")
    
    # 2. æ—¥å¿—æ”¶é›†æ¼”ç¤º
    print("\nğŸ“ æ—¥å¿—æ”¶é›†æ¼”ç¤º:")
    logs = [
        LogEntry(datetime.now(), LogLevel.INFO, "APIè¯·æ±‚æˆåŠŸ", "api-gateway", {"request_id": "req-001"}),
        LogEntry(datetime.now(), LogLevel.WARNING, "æ•°æ®åº“è¿æ¥è¶…æ—¶ï¼Œé‡è¯•ä¸­", "collector", {"db": "clickhouse"}),
        LogEntry(datetime.now(), LogLevel.ERROR, "äº¤æ˜“æ•°æ®è§£æå¤±è´¥", "parser", {"exchange": "binance"}),
        LogEntry(datetime.now(), LogLevel.INFO, "ç›‘æ§æ£€æŸ¥å®Œæˆ", "monitor", {"status": "healthy"})
    ]
    
    for log in logs:
        await obs_manager.log_aggregator.ingest_log(log)
        print(f"  âœ… æ”¶é›†æ—¥å¿—: [{log.level.value.upper()}] {log.message[:50]}...")
    
    # æ—¥å¿—æœç´¢
    search_results = await obs_manager.log_aggregator.search_logs("è¿æ¥", limit=5)
    print(f"  ğŸ” æœç´¢'è¿æ¥': æ‰¾åˆ° {len(search_results)} æ¡æ—¥å¿—")
    
    # 3. åˆ†å¸ƒå¼è¿½è¸ªæ¼”ç¤º
    print("\nğŸ” åˆ†å¸ƒå¼è¿½è¸ªæ¼”ç¤º:")
    
    # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„è¿½è¸ª
    root_span = await obs_manager.tracing_system.start_span("http_request")
    root_span.tags.update({"service.name": "api-gateway", "http.method": "POST"})
    
    # å­è·¨åº¦ - æ•°æ®åº“æŸ¥è¯¢
    db_span = await obs_manager.tracing_system.start_span(
        "database_query", 
        parent_span_id=root_span.span_id,
        trace_id=root_span.trace_id
    )
    db_span.tags.update({"service.name": "database", "db.statement": "SELECT * FROM trades"})
    
    await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    await obs_manager.tracing_system.finish_span(db_span, {"db.rows": "1000"})
    
    # å­è·¨åº¦ - å¤–éƒ¨APIè°ƒç”¨
    api_span = await obs_manager.tracing_system.start_span(
        "external_api_call",
        parent_span_id=root_span.span_id,
        trace_id=root_span.trace_id
    )
    api_span.tags.update({"service.name": "external-api", "http.url": "https://api.binance.com"})
    
    await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    await obs_manager.tracing_system.finish_span(api_span, {"http.status_code": "200"})
    
    await obs_manager.tracing_system.finish_span(root_span, {"http.status_code": "200"})
    
    print(f"  âœ… åˆ›å»ºè¿½è¸ª: {root_span.trace_id[:8]}... (åŒ…å« 3 ä¸ªè·¨åº¦)")
    
    # è·å–è¿½è¸ªæ•°æ®
    trace_spans = await obs_manager.tracing_system.get_trace(root_span.trace_id)
    print(f"  ğŸ“Š è¿½è¸ªè¯¦æƒ…: æ€»è·¨åº¦æ•°={len(trace_spans)}, æ€»è€—æ—¶={root_span.duration_ms:.2f}ms")
    
    # 4. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
    print("\nğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥:")
    status = await obs_manager.get_system_status()
    print(f"  ğŸ“Š æ€»ä½“çŠ¶æ€: {status['overall_status'].upper()}")
    print(f"  â±ï¸ è¿è¡Œæ—¶é—´: {status['uptime_seconds']:.1f}ç§’")
    
    for component, health in status['components'].items():
        print(f"  ğŸ”§ {component}: {health.get('status', 'unknown').upper()}")
    
    # 5. æ€§èƒ½æŒ‡æ ‡å±•ç¤º
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    metrics_data = await obs_manager.get_metrics()
    
    print(f"  ğŸ“Š æŒ‡æ ‡ç»Ÿè®¡: {metrics_data['metrics']['total_metrics']} ä¸ªæŒ‡æ ‡ç±»å‹")
    print(f"  ğŸ“ æ—¥å¿—ç»Ÿè®¡: {metrics_data['logs'].get('total_logs', 0)} æ¡æ—¥å¿—")
    print(f"  ğŸ” è¿½è¸ªç»Ÿè®¡: {metrics_data['tracing'].get('total_traces', 0)} ä¸ªè¿½è¸ª")
    
    # æ•°æ®å‹ç¼©æ¼”ç¤º
    compression_stats = await obs_manager.metrics_collector.compress_data()
    print(f"  ğŸ—œï¸ æ•°æ®å‹ç¼©: {compression_stats.get('compression_ratio', 0)*100:.1f}% å‹ç¼©ç‡")
    
    print("\nâœ¨ å¯è§‚æµ‹æ€§ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    
    await obs_manager.shutdown()
    return obs_manager

if __name__ == "__main__":
    asyncio.run(demonstrate_observability_system())