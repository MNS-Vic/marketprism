# çƒ­å†·å­˜å‚¨åè°ƒæ¶æ„è¯¦è§£

## ğŸ¯ æ¶æ„æ¾„æ¸…ï¼šç»Ÿä¸€æœåŠ¡ vs åˆ†ç¦»éƒ¨ç½²

### âŒ è¯¯è§£çš„æ–¹æ¡ˆï¼ˆä¸æ¨èï¼‰
```yaml
# é”™è¯¯ç†è§£ï¼šéƒ¨ç½²ä¸¤ä¸ªç‹¬ç«‹çš„å­˜å‚¨æœåŠ¡
hot-data-storage-service:     # çƒ­æ•°æ®æœåŠ¡å®ä¾‹
  config: hot_storage_config.yaml
  port: 8001

cold-data-storage-service:    # å†·æ•°æ®æœåŠ¡å®ä¾‹  
  config: cold_storage_config.yaml
  port: 8002

# é—®é¢˜ï¼šè°æ¥åè°ƒæ•°æ®è¿ç§»ï¼Ÿå¦‚ä½•ä¿è¯ä¸€è‡´æ€§ï¼Ÿ
```

### âœ… æ¨èçš„æ–¹æ¡ˆï¼ˆç»Ÿä¸€æœåŠ¡ï¼‰
```yaml
# æ­£ç¡®æ–¹æ¡ˆï¼šå•ä¸€æœåŠ¡ï¼Œå†…éƒ¨ç®¡ç†çƒ­å†·å­˜å‚¨
data-storage-service:
  config: unified_storage_config.yaml
  port: 8002
  
  # å†…éƒ¨ç»„ä»¶
  components:
    - hot_storage_manager    # çƒ­å­˜å‚¨ç®¡ç†å™¨
    - cold_storage_manager   # å†·å­˜å‚¨ç®¡ç†å™¨
    - lifecycle_manager      # æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
    - migration_coordinator  # è¿ç§»åè°ƒå™¨
```

## ğŸ”„ æ•°æ®è¿ç§»åè°ƒæœºåˆ¶

### 1. æ¶æ„æ€»è§ˆ
```mermaid
graph TD
    A[scheduler-service] -->|å®šæ—¶è§¦å‘| B[data-storage-service]
    B --> C[lifecycle_manager]
    C --> D[hot_storage_manager]
    C --> E[cold_storage_manager]
    C --> F[migration_coordinator]
    
    D -->|7å¤©å‰æ•°æ®| F
    F -->|è¿ç§»å¹¶éªŒè¯| E
    F -->|åˆ é™¤å·²è¿ç§»| D
```

### 2. åè°ƒè€…è§’è‰²åˆ†å·¥
```python
# scheduler-service: æ—¶é—´è§¦å‘å™¨
@scheduler.cron('0 2 * * *')  # æ¯å¤©å‡Œæ™¨2ç‚¹
async def trigger_daily_archive():
    """è§¦å‘æ¯æ—¥æ•°æ®å½’æ¡£ä»»åŠ¡"""
    response = await http_client.post(
        "http://data-storage-service:8002/api/internal/archive",
        json={"trigger_time": datetime.utcnow()}
    )
    return response

# data-storage-service: å®é™…æ‰§è¡Œè€…
class DataStorageService:
    async def handle_archive_trigger(self, request):
        """å¤„ç†å½’æ¡£è§¦å‘è¯·æ±‚"""
        return await self.lifecycle_manager.execute_daily_archive()
```

### 3. è¯¦ç»†è¿ç§»æµç¨‹
```python
class DataLifecycleManager:
    """æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨ - è´Ÿè´£åè°ƒè¿ç§»"""
    
    def __init__(self):
        self.hot_storage = HotStorageManager()
        self.cold_storage = ColdStorageManager()
        self.migration_coordinator = MigrationCoordinator()
        
    async def execute_daily_archive(self):
        """æ‰§è¡Œæ¯æ—¥å½’æ¡£ä»»åŠ¡"""
        try:
            # 1. ç¡®å®šè¿ç§»è¾¹ç•Œ
            cutoff_date = datetime.now() - timedelta(days=7)
            
            # 2. æŸ¥æ‰¾å¾…è¿ç§»æ•°æ®
            data_to_migrate = await self.hot_storage.find_data_before_date(cutoff_date)
            
            if not data_to_migrate:
                return {"status": "success", "message": "No data to migrate"}
            
            # 3. æ‰§è¡Œè¿ç§»
            migration_result = await self.migration_coordinator.migrate_data(
                data_to_migrate, cutoff_date
            )
            
            # 4. è¿”å›ç»“æœ
            return {
                "status": "success", 
                "migrated_records": migration_result.record_count,
                "freed_space": migration_result.freed_space,
                "duration": migration_result.duration
            }
            
        except Exception as e:
            await self._handle_migration_failure(e)
            return {"status": "failed", "error": str(e)}
```

### 4. è¿ç§»åè°ƒå™¨å®ç°
```python
class MigrationCoordinator:
    """è¿ç§»åè°ƒå™¨ - ç¡®ä¿æ•°æ®è¿ç§»çš„åŸå­æ€§"""
    
    async def migrate_data(self, data_batch, cutoff_date):
        """åŸå­æ€§æ•°æ®è¿ç§»"""
        migration_id = self._generate_migration_id()
        
        try:
            # é˜¶æ®µ1: æ•°æ®å‹ç¼©å’Œé¢„å¤„ç†
            compressed_data = await self._compress_and_validate(data_batch)
            
            # é˜¶æ®µ2: å†™å…¥å†·å­˜å‚¨
            cold_write_result = await self.cold_storage.write_compressed_data(
                compressed_data, migration_id
            )
            
            # é˜¶æ®µ3: éªŒè¯è¿ç§»å®Œæ•´æ€§
            verification_result = await self._verify_migration_integrity(
                data_batch, cold_write_result
            )
            
            if not verification_result.success:
                # å›æ»šå†·å­˜å‚¨å†™å…¥
                await self.cold_storage.rollback_migration(migration_id)
                raise MigrationError("Data integrity verification failed")
            
            # é˜¶æ®µ4: ä»çƒ­å­˜å‚¨åˆ é™¤å·²è¿ç§»æ•°æ®
            hot_delete_result = await self.hot_storage.delete_migrated_data(
                data_batch, migration_id
            )
            
            # é˜¶æ®µ5: è®°å½•è¿ç§»æ—¥å¿—
            await self._log_migration_success(migration_id, {
                "cutoff_date": cutoff_date,
                "record_count": len(data_batch),
                "compressed_size": len(compressed_data),
                "duration": verification_result.duration
            })
            
            return MigrationResult(
                success=True,
                migration_id=migration_id,
                record_count=len(data_batch),
                freed_space=hot_delete_result.freed_space,
                duration=verification_result.duration
            )
            
        except Exception as e:
            # å…¨é¢å›æ»š
            await self._rollback_migration(migration_id)
            raise MigrationError(f"Migration failed: {e}")
```

## âš™ï¸ é…ç½®ç®¡ç†æ–¹æ¡ˆ

### 1. ç»Ÿä¸€é…ç½®æ–‡ä»¶
```yaml
# config/storage_service.yaml
service:
  name: "data-storage-service"
  port: 8002
  
# çƒ­å­˜å‚¨é…ç½®
hot_storage:
  type: "clickhouse_cluster"
  cluster_config:
    nodes:
      - host: "clickhouse-hot-1"
        port: 9000
      - host: "clickhouse-hot-2" 
        port: 9000
      - host: "clickhouse-hot-3"
        port: 9000
  retention_policy:
    days: 7                    # çƒ­æ•°æ®ä¿ç•™7å¤©
    cleanup_hour: 2            # å‡Œæ™¨2ç‚¹æ¸…ç†
  performance:
    compression: "lz4"         # è½»é‡å‹ç¼©ï¼Œå¿«é€Ÿè®¿é—®
    replica_count: 3
    shard_count: 4

# å†·å­˜å‚¨é…ç½®  
cold_storage:
  type: "clickhouse_cold_cluster"
  cluster_config:
    nodes:
      - host: "clickhouse-cold-1"
        port: 9000
      - host: "clickhouse-cold-2"
        port: 9000
  retention_policy:
    years: 5                   # å†·æ•°æ®ä¿ç•™5å¹´
    cleanup_hour: 3            # å‡Œæ™¨3ç‚¹æ¸…ç†è¿‡æœŸæ•°æ®
  performance:
    compression: "zstd_ultra"  # è¶…é«˜å‹ç¼©æ¯”
    replica_count: 2           # å†·æ•°æ®å‰¯æœ¬æ•°è¾ƒå°‘
    
# è¿ç§»ç­–ç•¥é…ç½®
migration:
  schedule:
    cron: "0 2 * * *"         # æ¯å¤©å‡Œæ™¨2ç‚¹è§¦å‘
    timeout: "2h"             # è¿ç§»è¶…æ—¶æ—¶é—´
  batch_size: 100000          # æ¯æ‰¹æ¬¡è¿ç§»è®°å½•æ•°
  verification_enabled: true   # å¯ç”¨å®Œæ•´æ€§éªŒè¯
  rollback_on_failure: true   # å¤±è´¥æ—¶è‡ªåŠ¨å›æ»š
  
# ç¼“å­˜å±‚é…ç½®
cache:
  type: "redis_cluster"
  cluster_config:
    nodes:
      - "redis-1:6379"
      - "redis-2:6379" 
      - "redis-3:6379"
  settings:
    memory_limit: "16GB"
    ttl_hours: 24
```

### 2. ç¯å¢ƒç‰¹å®šé…ç½®
```yaml
# config/environments/production.yaml
hot_storage:
  cluster_config:
    nodes:
      - host: "prod-clickhouse-hot-1.internal"
      - host: "prod-clickhouse-hot-2.internal"
      - host: "prod-clickhouse-hot-3.internal"

# config/environments/development.yaml  
hot_storage:
  cluster_config:
    nodes:
      - host: "localhost"
        port: 9001            # å¼€å‘ç¯å¢ƒå•èŠ‚ç‚¹
cold_storage:
  cluster_config:
    nodes:
      - host: "localhost" 
        port: 9002            # å¼€å‘ç¯å¢ƒå•èŠ‚ç‚¹
```

### 3. é…ç½®åŠ è½½é€»è¾‘
```python
class StorageServiceConfig:
    def __init__(self, environment="production"):
        self.environment = environment
        self.config = self._load_config()
        
    def _load_config(self):
        # 1. åŠ è½½åŸºç¡€é…ç½®
        base_config = yaml.load(open("config/storage_service.yaml"))
        
        # 2. åŠ è½½ç¯å¢ƒç‰¹å®šé…ç½®
        env_config = yaml.load(open(f"config/environments/{self.environment}.yaml"))
        
        # 3. åˆå¹¶é…ç½®
        return self._deep_merge(base_config, env_config)
    
    @property  
    def hot_storage_config(self):
        return self.config['hot_storage']
        
    @property
    def cold_storage_config(self):
        return self.config['cold_storage']
        
    @property
    def migration_config(self):
        return self.config['migration']
```

## ğŸ”§ æœåŠ¡é—´åè°ƒæ¥å£

### 1. scheduler-service â†’ data-storage-service
```python
# scheduler-serviceå‘èµ·çš„è°ƒç”¨
async def trigger_archive_task():
    """è°ƒåº¦æœåŠ¡è§¦å‘å½’æ¡£ä»»åŠ¡"""
    response = await http_client.post(
        "http://data-storage-service:8002/api/internal/lifecycle/archive",
        json={
            "task_id": generate_task_id(),
            "trigger_time": datetime.utcnow().isoformat(),
            "parameters": {
                "cutoff_days": 7,
                "batch_size": 100000
            }
        }
    )
    return response.json()

# data-storage-serviceçš„å†…éƒ¨æ¥å£
@app.post("/api/internal/lifecycle/archive")
async def handle_archive_request(request: ArchiveRequest):
    """å¤„ç†å½’æ¡£è¯·æ±‚"""
    result = await storage_service.lifecycle_manager.execute_archive(
        cutoff_days=request.parameters.cutoff_days,
        batch_size=request.parameters.batch_size
    )
    return result
```

### 2. ç›‘æ§å’Œå‘Šè­¦é›†æˆ
```python
# data-storage-serviceå‘monitoring-serviceæŠ¥å‘ŠçŠ¶æ€
async def report_migration_metrics(self, migration_result):
    """å‘ç›‘æ§æœåŠ¡æŠ¥å‘Šè¿ç§»æŒ‡æ ‡"""
    metrics = {
        "migration_duration": migration_result.duration,
        "migrated_records": migration_result.record_count,
        "freed_space_gb": migration_result.freed_space / (1024**3),
        "success_rate": 1.0 if migration_result.success else 0.0
    }
    
    await self.monitoring_client.send_metrics("data_migration", metrics)
    
    if not migration_result.success:
        await self.monitoring_client.send_alert({
            "level": "error",
            "message": f"Data migration failed: {migration_result.error}",
            "service": "data-storage-service"
        })
```

## ğŸš€ éƒ¨ç½²æ¶æ„

### 1. å•ä¸€æœåŠ¡éƒ¨ç½²
```yaml
# docker-compose.yml
services:
  data-storage-service:
    image: marketprism/data-storage:v1.0.0
    ports:
      - "8002:8002"
    environment:
      - ENVIRONMENT=production
      - CONFIG_PATH=/app/config
    volumes:
      - ./config:/app/config
      - ./data:/app/data
    depends_on:
      - clickhouse-hot-cluster
      - clickhouse-cold-cluster
      - redis-cluster
      
  # çƒ­å­˜å‚¨é›†ç¾¤
  clickhouse-hot-1:
    image: clickhouse/clickhouse-server:latest
    environment:
      - CLICKHOUSE_DB=marketprism_hot
    volumes:
      - hot_data_1:/var/lib/clickhouse
      
  # å†·å­˜å‚¨é›†ç¾¤  
  clickhouse-cold-1:
    image: clickhouse/clickhouse-server:latest
    environment:
      - CLICKHOUSE_DB=marketprism_cold
    volumes:
      - cold_data_1:/var/lib/clickhouse
```

### 2. æœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥
```python
# å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health")
async def health_check():
    """ç»¼åˆå¥åº·æ£€æŸ¥"""
    health_status = {
        "service": "data-storage-service",
        "timestamp": datetime.utcnow(),
        "status": "healthy",
        "components": {}
    }
    
    # æ£€æŸ¥çƒ­å­˜å‚¨
    hot_health = await self.hot_storage.health_check()
    health_status["components"]["hot_storage"] = hot_health
    
    # æ£€æŸ¥å†·å­˜å‚¨
    cold_health = await self.cold_storage.health_check()
    health_status["components"]["cold_storage"] = cold_health
    
    # æ£€æŸ¥è¿ç§»çŠ¶æ€
    migration_health = await self.lifecycle_manager.health_check()
    health_status["components"]["migration"] = migration_health
    
    # æ•´ä½“çŠ¶æ€è¯„ä¼°
    if all(comp["status"] == "healthy" for comp in health_status["components"].values()):
        health_status["status"] = "healthy"
    else:
        health_status["status"] = "degraded"
        
    return health_status
```

## æ€»ç»“

### âœ… æ¨èæ¶æ„çš„ä¼˜åŠ¿

1. **ç»Ÿä¸€åè°ƒ**ï¼šå•ä¸€æœåŠ¡å†…éƒ¨ç®¡ç†ï¼Œé¿å…åˆ†å¸ƒå¼ä¸€è‡´æ€§é—®é¢˜
2. **schedulerè§¦å‘**ï¼šå®šæ—¶ä»»åŠ¡ç”±ä¸“é—¨çš„è°ƒåº¦æœåŠ¡ç®¡ç†
3. **åŸå­æ€§ä¿è¯**ï¼šè¿ç§»è¿‡ç¨‹å…·å¤‡å®Œæ•´çš„äº‹åŠ¡è¯­ä¹‰
4. **é…ç½®é›†ä¸­**ï¼šç»Ÿä¸€é…ç½®ç®¡ç†ï¼Œç¯å¢ƒéš”ç¦»
5. **ç›‘æ§é›†æˆ**ï¼šä¸ç›‘æ§æœåŠ¡æ— ç¼é›†æˆï¼Œå…¨é“¾è·¯å¯è§‚æµ‹

### ğŸ¯ å…³é”®åè°ƒæµç¨‹

```
scheduler-service â†’ å®šæ—¶è§¦å‘ â†’ data-storage-service
                                      â†“
                                lifecycle_manager 
                                      â†“
                  hot_storage â†â†’ migration_coordinator â†â†’ cold_storage
                                      â†“
                              monitoring-service (æŒ‡æ ‡ä¸ŠæŠ¥)
```

è¿™æ ·çš„æ¶æ„ç¡®ä¿äº†çƒ­å†·æ•°æ®è¿ç§»çš„å¯é æ€§å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„ç®€æ´æ€§ï¼