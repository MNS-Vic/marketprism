# MarketPrism 独立部署架构说明

## 🎯 独立部署核心特征

每个服务都是**完全独立的部署单元**，具备以下特性：

### 1. 独立的生命周期
```yaml
独立开发: 每个团队可以独立开发自己负责的服务
独立测试: 各服务有独立的测试环境和测试流程  
独立发布: 服务可以按需发布，不影响其他服务
独立扩展: 根据负载情况独立扩缩容
独立故障: 一个服务故障不会导致整个系统崩溃
```

### 2. 独立的运行环境
```yaml
独立进程: 每个服务运行在独立的进程/容器中
独立端口: 每个服务占用不同的端口
独立资源: CPU、内存、存储资源独立分配
独立配置: 每个服务有独立的配置文件
```

## 🐳 Docker容器化部署

### 服务容器化方案
```yaml
market-data-collector:
  container_name: marketprism-collector
  image: marketprism/collector:v1.0.0
  ports: ["8001:8000"]
  environment:
    - SERVICE_NAME=market-data-collector
    - NATS_URL=nats://message-broker:4222
    - CLICKHOUSE_URL=clickhouse://data-storage:9000

data-storage-service:
  container_name: marketprism-storage
  image: marketprism/storage:v1.0.0
  ports: ["8002:8000"]
  environment:
    - SERVICE_NAME=data-storage-service
    - CLICKHOUSE_HOST=clickhouse
    - CLICKHOUSE_PORT=9000

api-gateway-service:
  container_name: marketprism-gateway
  image: marketprism/gateway:v1.0.0
  ports: ["8000:8000"]  # 外部访问入口
  environment:
    - SERVICE_NAME=api-gateway
    - COLLECTOR_URL=http://market-data-collector:8000
    - STORAGE_URL=http://data-storage-service:8000

monitoring-service:
  container_name: marketprism-monitoring
  image: marketprism/monitoring:v1.0.0
  ports: ["3000:3000", "9090:9090"]
  environment:
    - SERVICE_NAME=monitoring-service
    - GRAFANA_PORT=3000
    - PROMETHEUS_PORT=9090

message-broker-service:
  container_name: marketprism-nats
  image: nats:2.9-alpine
  ports: ["4222:4222", "8222:8222"]
  command: ["--jetstream", "--store_dir", "/data"]

scheduler-service:
  container_name: marketprism-scheduler
  image: marketprism/scheduler:v1.0.0
  ports: ["8003:8000"]
  environment:
    - SERVICE_NAME=scheduler-service
    - NATS_URL=nats://message-broker:4222
```

## 🚀 独立部署示例

### 1. 单服务部署
```bash
# 只部署数据收集服务
docker run -d \
  --name marketprism-collector \
  -p 8001:8000 \
  -e NATS_URL=nats://192.168.1.100:4222 \
  -e CLICKHOUSE_URL=clickhouse://192.168.1.101:9000 \
  marketprism/collector:v1.0.0

# 只部署API网关
docker run -d \
  --name marketprism-gateway \
  -p 8000:8000 \
  -e COLLECTOR_URL=http://192.168.1.102:8001 \
  -e STORAGE_URL=http://192.168.1.103:8002 \
  marketprism/gateway:v1.0.0
```

### 2. 分机房部署
```yaml
# 机房A - 核心业务服务
services:
  - market-data-collector
  - data-storage-service
  - api-gateway-service

# 机房B - 基础设施服务  
services:
  - monitoring-service
  - message-broker-service
  - scheduler-service

# 跨机房通信
network:
  - VPN隧道或专线连接
  - 服务发现注册中心
```

### 3. 多环境部署
```yaml
# 开发环境 - 单机部署
docker-compose -f docker-compose.dev.yml up

# 测试环境 - 模拟生产拓扑
docker-compose -f docker-compose.test.yml up

# 生产环境 - K8s集群部署
kubectl apply -f k8s/production/
```

## 🔧 独立部署的技术实现

### 1. 服务发现
```python
# 每个服务启动时自动注册
from core.networking import ServiceRegistry

class MarketDataCollector:
    def __init__(self):
        self.registry = ServiceRegistry()
        
    async def start(self):
        # 注册自己
        await self.registry.register(
            service_name="market-data-collector",
            host=self.host,
            port=self.port,
            health_check_url=f"http://{self.host}:{self.port}/health"
        )
        
    async def discover_storage_service(self):
        # 发现存储服务
        storage_nodes = await self.registry.discover("data-storage-service")
        return storage_nodes[0]  # 负载均衡选择
```

### 2. 配置管理
```yaml
# 每个服务有独立的配置文件
market-data-collector/
├── config/
│   ├── base.yaml          # 基础配置
│   ├── development.yaml   # 开发环境
│   ├── production.yaml    # 生产环境
│   └── local.yaml         # 本地覆盖
└── Dockerfile

data-storage-service/
├── config/
│   ├── base.yaml
│   ├── development.yaml
│   └── production.yaml
└── Dockerfile
```

### 3. 健康检查
```python
# 每个服务提供标准健康检查接口
@app.get("/health")
async def health_check():
    return {
        "service": "market-data-collector",
        "status": "healthy",
        "version": "1.0.0",
        "timestamp": datetime.utcnow(),
        "dependencies": {
            "clickhouse": await check_clickhouse_health(),
            "nats": await check_nats_health()
        }
    }
```

## 📊 独立部署的优势

### 1. 技术栈灵活性
```yaml
market-data-collector:
  language: Python
  framework: FastAPI
  database: Redis (缓存)

data-storage-service:
  language: Python
  framework: FastAPI  
  database: ClickHouse

api-gateway-service:
  language: Go (高性能需求)
  framework: Gin
  middleware: Nginx

monitoring-service:
  technology: Prometheus + Grafana
  language: Python (自定义指标)
```

### 2. 独立扩展
```bash
# 只扩展数据收集服务（高并发场景）
docker service scale marketprism-collector=5

# 只扩展存储服务（大数据写入场景）
docker service scale marketprism-storage=3

# API网关保持单实例（负载均衡器后面）
docker service scale marketprism-gateway=1
```

### 3. 独立版本管理
```yaml
services:
  market-data-collector: v2.1.0  # 最新版本，新功能
  data-storage-service: v1.5.2   # 稳定版本，bug修复
  api-gateway-service: v1.0.0    # 稳定运行，无需更新
  monitoring-service: v3.0.0     # 大版本升级，新监控功能
```

## ⚠️ 需要注意的问题

### 1. 服务间依赖
```python
# 启动顺序依赖
startup_order = [
    "message-broker-service",    # 消息中间件先启动
    "data-storage-service",      # 存储服务
    "market-data-collector",     # 数据收集依赖存储
    "api-gateway-service",       # 网关最后启动
    "monitoring-service",        # 监控服务独立
    "scheduler-service"          # 调度服务独立
]
```

### 2. 数据一致性
```python
# 分布式事务处理
class DataConsistencyManager:
    async def process_market_data(self, data):
        # 使用Saga模式保证最终一致性
        saga = Saga()
        saga.add_step(
            action=self.collector.normalize_data,
            compensation=self.collector.rollback_normalization
        )
        saga.add_step(
            action=self.storage.save_data,
            compensation=self.storage.delete_data
        )
        await saga.execute()
```

### 3. 网络延迟
```python
# 服务间通信优化
class ServiceCommunication:
    def __init__(self):
        # 连接池复用
        self.http_pool = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
        
        # 熔断器
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60
        )
```

## 总结

是的，**每个服务都完全独立部署**：

✅ **独立进程**: 每个服务运行在独立容器中
✅ **独立端口**: 避免端口冲突  
✅ **独立配置**: 各自的配置文件和环境变量
✅ **独立扩展**: 根据需要独立扩缩容
✅ **独立发布**: 服务更新不影响其他服务
✅ **独立故障**: 故障隔离，提高系统可用性

这样的架构设计让你可以：
- 按需部署特定服务
- 在不同机房部署不同服务  
- 使用不同技术栈开发不同服务
- 根据业务需要独立扩展服务
- 实现真正的DevOps和CI/CD