#!/usr/bin/env python3
"""
MarketPrismé…ç½®æ–‡ä»¶å®¡è®¡åˆ†æå·¥å…·
åˆ†æconfigç›®å½•ä¸‹æ‰€æœ‰é…ç½®æ–‡ä»¶ï¼Œè¯†åˆ«é‡å¤ã€è¿‡æ—¶ã€æœªä½¿ç”¨çš„é…ç½®
"""

import os
import json
import yaml
import xml.etree.ElementTree as ET
from pathlib import Path
from collections import defaultdict
import hashlib
import re

class ConfigAuditor:
    def __init__(self, config_root="config"):
        self.config_root = Path(config_root)
        self.files_by_type = defaultdict(list)
        self.files_by_category = defaultdict(list)
        self.duplicate_files = []
        self.unused_files = []
        self.file_dependencies = defaultdict(set)
        
    def scan_config_files(self):
        """æ‰«ææ‰€æœ‰é…ç½®æ–‡ä»¶"""
        print("ğŸ” æ‰«æé…ç½®æ–‡ä»¶...")
        
        for file_path in self.config_root.rglob("*"):
            if file_path.is_file() and not file_path.name.startswith('.'):
                # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç±»
                suffix = file_path.suffix.lower()
                if suffix in ['.yaml', '.yml', '.json', '.xml', '.conf', '.py', '.sql', '.md']:
                    self.files_by_type[suffix].append(file_path)
                    
                # æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç±»
                category = self._categorize_file(file_path)
                self.files_by_category[category].append(file_path)
        
        print(f"âœ… æ‰«æå®Œæˆï¼Œå…±å‘ç° {sum(len(files) for files in self.files_by_type.values())} ä¸ªé…ç½®æ–‡ä»¶")
        
    def _categorize_file(self, file_path):
        """æ ¹æ®è·¯å¾„å’Œæ–‡ä»¶åå¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç±»"""
        path_str = str(file_path).lower()
        
        # ç›‘æ§ç›¸å…³
        if any(keyword in path_str for keyword in ['monitoring', 'prometheus', 'grafana', 'alert']):
            return 'monitoring'
        
        # äº¤æ˜“æ‰€ç›¸å…³
        if any(keyword in path_str for keyword in ['exchange', 'binance', 'okx', 'deribit']):
            return 'exchanges'
        
        # æ•°æ®åº“ç›¸å…³
        if any(keyword in path_str for keyword in ['clickhouse', 'storage', 'database']):
            return 'database'
        
        # æ¶ˆæ¯é˜Ÿåˆ—ç›¸å…³
        if any(keyword in path_str for keyword in ['nats', 'message', 'queue']):
            return 'messaging'
        
        # æœåŠ¡ç›¸å…³
        if any(keyword in path_str for keyword in ['service', 'gateway', 'collector']):
            return 'services'
        
        # æ ¸å¿ƒé…ç½®
        if any(keyword in path_str for keyword in ['core', 'factory', 'unified']):
            return 'core'
        
        # æµ‹è¯•é…ç½®
        if any(keyword in path_str for keyword in ['test', 'dev']):
            return 'testing'
        
        # åŸºç¡€è®¾æ–½
        if any(keyword in path_str for keyword in ['infrastructure', 'systemd', 'proxy']):
            return 'infrastructure'
        
        return 'misc'
    
    def find_duplicate_files(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰"""
        print("ğŸ” æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
        
        file_hashes = defaultdict(list)
        
        for file_list in self.files_by_type.values():
            for file_path in file_list:
                try:
                    with open(file_path, 'rb') as f:
                        content_hash = hashlib.md5(f.read()).hexdigest()
                        file_hashes[content_hash].append(file_path)
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
        
        # æ‰¾å‡ºé‡å¤æ–‡ä»¶
        for content_hash, files in file_hashes.items():
            if len(files) > 1:
                self.duplicate_files.append(files)
        
        print(f"âœ… å‘ç° {len(self.duplicate_files)} ç»„é‡å¤æ–‡ä»¶")
    
    def analyze_file_dependencies(self):
        """åˆ†ææ–‡ä»¶ä¾èµ–å…³ç³»"""
        print("ğŸ” åˆ†ææ–‡ä»¶ä¾èµ–å…³ç³»...")
        
        # æ‰«ææ‰€æœ‰æ–‡ä»¶ä¸­çš„è·¯å¾„å¼•ç”¨
        for file_list in self.files_by_type.values():
            for file_path in file_list:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶å¼•ç”¨
                    config_refs = re.findall(r'config/[a-zA-Z0-9_/.-]+\.(yaml|yml|json|xml|conf)', content)
                    for ref in config_refs:
                        ref_path = Path(ref)
                        if ref_path.exists():
                            self.file_dependencies[file_path].add(ref_path)
                            
                except Exception as e:
                    continue
        
        print(f"âœ… åˆ†æå®Œæˆï¼Œå‘ç° {len(self.file_dependencies)} ä¸ªæ–‡ä»¶æœ‰ä¾èµ–å…³ç³»")
    
    def identify_unused_files(self):
        """è¯†åˆ«æœªä½¿ç”¨çš„é…ç½®æ–‡ä»¶"""
        print("ğŸ” è¯†åˆ«æœªä½¿ç”¨çš„é…ç½®æ–‡ä»¶...")
        
        # è·å–æ‰€æœ‰è¢«å¼•ç”¨çš„æ–‡ä»¶
        referenced_files = set()
        for deps in self.file_dependencies.values():
            referenced_files.update(deps)
        
        # è·å–æ‰€æœ‰é…ç½®æ–‡ä»¶
        all_config_files = set()
        for file_list in self.files_by_type.values():
            all_config_files.update(file_list)
        
        # æ‰¾å‡ºå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶ï¼ˆæ’é™¤ä¸€äº›é‡è¦æ–‡ä»¶ï¼‰
        important_patterns = [
            'docker-compose',
            'prometheus.yml',
            'alert_manager.yml',
            'exchanges.yaml',
            'services.yaml'
        ]
        
        for file_path in all_config_files:
            if file_path not in referenced_files:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡è¦æ–‡ä»¶
                is_important = any(pattern in str(file_path) for pattern in important_patterns)
                if not is_important:
                    self.unused_files.append(file_path)
        
        print(f"âœ… å‘ç° {len(self.unused_files)} ä¸ªå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶")
    
    def generate_report(self):
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆå®¡è®¡æŠ¥å‘Š...")
        
        report = {
            "audit_summary": {
                "total_files": sum(len(files) for files in self.files_by_type.values()),
                "files_by_type": {k: len(v) for k, v in self.files_by_type.items()},
                "files_by_category": {k: len(v) for k, v in self.files_by_category.items()},
                "duplicate_groups": len(self.duplicate_files),
                "potentially_unused": len(self.unused_files),
                "files_with_dependencies": len(self.file_dependencies)
            },
            "file_categories": {},
            "duplicate_files": [],
            "potentially_unused_files": [],
            "recommendations": []
        }
        
        # è¯¦ç»†åˆ†ç±»ä¿¡æ¯
        for category, files in self.files_by_category.items():
            report["file_categories"][category] = [str(f) for f in files]
        
        # é‡å¤æ–‡ä»¶ä¿¡æ¯
        for duplicate_group in self.duplicate_files:
            report["duplicate_files"].append([str(f) for f in duplicate_group])
        
        # å¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶
        report["potentially_unused_files"] = [str(f) for f in self.unused_files]
        
        # ç”Ÿæˆå»ºè®®
        report["recommendations"] = self._generate_recommendations()
        
        return report
    
    def _generate_recommendations(self):
        """ç”Ÿæˆæ•´ç†å»ºè®®"""
        recommendations = []
        
        # é‡å¤æ–‡ä»¶å»ºè®®
        if self.duplicate_files:
            recommendations.append({
                "type": "duplicate_cleanup",
                "priority": "high",
                "description": f"å‘ç° {len(self.duplicate_files)} ç»„é‡å¤æ–‡ä»¶ï¼Œå»ºè®®ä¿ç•™æœ€æ–°ç‰ˆæœ¬å¹¶åˆ é™¤é‡å¤æ–‡ä»¶"
            })
        
        # æœªä½¿ç”¨æ–‡ä»¶å»ºè®®
        if self.unused_files:
            recommendations.append({
                "type": "unused_cleanup",
                "priority": "medium",
                "description": f"å‘ç° {len(self.unused_files)} ä¸ªå¯èƒ½æœªä½¿ç”¨çš„æ–‡ä»¶ï¼Œå»ºè®®ç§»åŠ¨åˆ°archiveç›®å½•"
            })
        
        # åˆ†ç±»æ•´ç†å»ºè®®
        monitoring_files = len(self.files_by_category.get('monitoring', []))
        if monitoring_files > 10:
            recommendations.append({
                "type": "monitoring_reorganization",
                "priority": "high",
                "description": f"ç›‘æ§é…ç½®æ–‡ä»¶è¿‡å¤š({monitoring_files}ä¸ª)ï¼Œå»ºè®®è¿›ä¸€æ­¥ç»†åˆ†ç»„ç»‡"
            })
        
        # å‘½åè§„èŒƒå»ºè®®
        inconsistent_naming = []
        for category, files in self.files_by_category.items():
            for file_path in files:
                if '_' in file_path.name and '-' in file_path.name:
                    inconsistent_naming.append(file_path)
        
        if inconsistent_naming:
            recommendations.append({
                "type": "naming_standardization",
                "priority": "medium",
                "description": f"å‘ç° {len(inconsistent_naming)} ä¸ªæ–‡ä»¶å‘½åä¸ä¸€è‡´ï¼Œå»ºè®®ç»Ÿä¸€ä½¿ç”¨è¿å­—ç¬¦æˆ–ä¸‹åˆ’çº¿"
            })
        
        return recommendations

def main():
    auditor = ConfigAuditor()
    
    # æ‰§è¡Œå®¡è®¡
    auditor.scan_config_files()
    auditor.find_duplicate_files()
    auditor.analyze_file_dependencies()
    auditor.identify_unused_files()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = auditor.generate_report()
    
    # ä¿å­˜æŠ¥å‘Š
    with open('config_audit_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š é…ç½®æ–‡ä»¶å®¡è®¡æ‘˜è¦:")
    print(f"æ€»æ–‡ä»¶æ•°: {report['audit_summary']['total_files']}")
    print(f"æŒ‰ç±»å‹åˆ†å¸ƒ: {report['audit_summary']['files_by_type']}")
    print(f"æŒ‰åŠŸèƒ½åˆ†å¸ƒ: {report['audit_summary']['files_by_category']}")
    print(f"é‡å¤æ–‡ä»¶ç»„: {report['audit_summary']['duplicate_groups']}")
    print(f"å¯èƒ½æœªä½¿ç”¨: {report['audit_summary']['potentially_unused']}")
    
    print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: config_audit_report.json")

if __name__ == "__main__":
    main()
