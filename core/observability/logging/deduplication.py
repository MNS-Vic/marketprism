"""
MarketPrismæ—¥å¿—å»é‡ç³»ç»Ÿ

æä¾›æ™ºèƒ½çš„æ—¥å¿—å»é‡ã€èšåˆå’Œæ‰¹é‡å¤„ç†åŠŸèƒ½ï¼Œæœ‰æ•ˆå‡å°‘æ—¥å¿—æ´ªæ°´é—®é¢˜ã€‚
"""

import time
import hashlib
from typing import Dict, Any, Optional, List, Tuple, Set
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import Enum
import threading


class DeduplicationStrategy(Enum):
    """å»é‡ç­–ç•¥"""
    EXACT_MATCH = "exact_match"           # ç²¾ç¡®åŒ¹é…
    CONTENT_HASH = "content_hash"         # å†…å®¹å“ˆå¸Œ
    PATTERN_MATCH = "pattern_match"       # æ¨¡å¼åŒ¹é…
    TIME_WINDOW = "time_window"           # æ—¶é—´çª—å£
    FREQUENCY_LIMIT = "frequency_limit"   # é¢‘ç‡é™åˆ¶


@dataclass
class LogEntry:
    """æ—¥å¿—æ¡ç›®"""
    timestamp: float
    level: str
    component: str
    message: str
    context: Dict[str, Any] = field(default_factory=dict)
    hash_key: Optional[str] = None
    
    def __post_init__(self):
        if self.hash_key is None:
            self.hash_key = self._generate_hash()
    
    def _generate_hash(self) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œ"""
        # æ’é™¤æ—¶é—´æˆ³å’ŒåŠ¨æ€å€¼ï¼Œåªå¯¹æ ¸å¿ƒå†…å®¹ç”Ÿæˆå“ˆå¸Œ
        content = f"{self.level}:{self.component}:{self._normalize_message()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def _normalize_message(self) -> str:
        """æ ‡å‡†åŒ–æ¶ˆæ¯å†…å®¹ï¼Œç§»é™¤åŠ¨æ€éƒ¨åˆ†"""
        import re
        
        # ç§»é™¤æ—¶é—´æˆ³
        message = re.sub(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}', '[TIMESTAMP]', self.message)
        
        # ç§»é™¤æ•°å­—ID
        message = re.sub(r'\b\d{10,}\b', '[ID]', message)
        
        # ç§»é™¤ä»·æ ¼å’Œæ•°é‡
        message = re.sub(r'\b\d+\.\d+\b', '[NUMBER]', message)
        
        # ç§»é™¤ç¬¦å·åç§°ï¼ˆä¿ç•™æ¨¡å¼ï¼‰
        message = re.sub(r'\b[A-Z]{3,10}USDT?\b', '[SYMBOL]', message)
        
        # ç§»é™¤emoji
        message = re.sub(r'[ğŸ”ğŸš€âœ…âŒâš ï¸ğŸ’“ğŸ”§ğŸ“ŠğŸ¯ğŸ­ğŸ“‹ğŸ”„ğŸ›‘]', '', message)
        
        return message.strip()


@dataclass
class DeduplicationRule:
    """å»é‡è§„åˆ™"""
    pattern: str
    strategy: DeduplicationStrategy
    time_window: int = 60  # ç§’
    max_occurrences: int = 5
    batch_size: int = 10
    enabled: bool = True


class LogDeduplicator:
    """æ—¥å¿—å»é‡å™¨"""
    
    def __init__(self, max_cache_size: int = 10000):
        self.max_cache_size = max_cache_size
        self._cache: Dict[str, List[LogEntry]] = defaultdict(list)
        self._suppressed_counts: Dict[str, int] = defaultdict(int)
        self._last_logged: Dict[str, float] = {}
        self._lock = threading.Lock()
        
        # é¢„å®šä¹‰å»é‡è§„åˆ™
        self.rules = self._initialize_default_rules()
    
    def _initialize_default_rules(self) -> Dict[str, DeduplicationRule]:
        """åˆå§‹åŒ–é»˜è®¤å»é‡è§„åˆ™"""
        return {
            "websocket_connection": DeduplicationRule(
                pattern=r"WebSocket.*è¿æ¥.*æˆåŠŸ",
                strategy=DeduplicationStrategy.PATTERN_MATCH,
                time_window=300,  # 5åˆ†é’Ÿ
                max_occurrences=1
            ),
            
            "data_processing_success": DeduplicationRule(
                pattern=r".*æ›´æ–°åº”ç”¨æˆåŠŸ",
                strategy=DeduplicationStrategy.FREQUENCY_LIMIT,
                time_window=60,   # 1åˆ†é’Ÿ
                max_occurrences=10,
                batch_size=100
            ),
            
            "message_queue": DeduplicationRule(
                pattern=r"æ¶ˆæ¯å…¥é˜Ÿ.*é˜Ÿåˆ—å¤§å°",
                strategy=DeduplicationStrategy.FREQUENCY_LIMIT,
                time_window=30,   # 30ç§’
                max_occurrences=5,
                batch_size=50
            ),
            
            "heartbeat": DeduplicationRule(
                pattern=r"å¿ƒè·³.*æ£€æŸ¥",
                strategy=DeduplicationStrategy.TIME_WINDOW,
                time_window=120,  # 2åˆ†é’Ÿ
                max_occurrences=1
            ),
            
            "performance_stats": DeduplicationRule(
                pattern=r"æ€§èƒ½.*ç»Ÿè®¡|Performance.*metric",
                strategy=DeduplicationStrategy.TIME_WINDOW,
                time_window=60,   # 1åˆ†é’Ÿ
                max_occurrences=3
            )
        }
    
    def should_log(self, entry: LogEntry) -> Tuple[bool, Optional[str]]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è®°å½•æ—¥å¿—
        
        Returns:
            (should_log, aggregation_message)
        """
        with self._lock:
            # æŸ¥æ‰¾åŒ¹é…çš„è§„åˆ™
            matching_rule = self._find_matching_rule(entry)
            
            if not matching_rule or not matching_rule.enabled:
                return True, None
            
            # åº”ç”¨å»é‡ç­–ç•¥
            return self._apply_deduplication_strategy(entry, matching_rule)
    
    def _find_matching_rule(self, entry: LogEntry) -> Optional[DeduplicationRule]:
        """æŸ¥æ‰¾åŒ¹é…çš„å»é‡è§„åˆ™"""
        import re
        
        for rule_name, rule in self.rules.items():
            if re.search(rule.pattern, entry.message, re.IGNORECASE):
                return rule
        
        return None
    
    def _apply_deduplication_strategy(self, 
                                    entry: LogEntry, 
                                    rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """åº”ç”¨å»é‡ç­–ç•¥"""
        
        if rule.strategy == DeduplicationStrategy.EXACT_MATCH:
            return self._apply_exact_match(entry, rule)
        
        elif rule.strategy == DeduplicationStrategy.CONTENT_HASH:
            return self._apply_content_hash(entry, rule)
        
        elif rule.strategy == DeduplicationStrategy.PATTERN_MATCH:
            return self._apply_pattern_match(entry, rule)
        
        elif rule.strategy == DeduplicationStrategy.TIME_WINDOW:
            return self._apply_time_window(entry, rule)
        
        elif rule.strategy == DeduplicationStrategy.FREQUENCY_LIMIT:
            return self._apply_frequency_limit(entry, rule)
        
        return True, None
    
    def _apply_exact_match(self, entry: LogEntry, rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """ç²¾ç¡®åŒ¹é…å»é‡"""
        exact_key = f"{entry.component}:{entry.message}"
        
        if exact_key in self._last_logged:
            time_since_last = entry.timestamp - self._last_logged[exact_key]
            if time_since_last < rule.time_window:
                self._suppressed_counts[exact_key] += 1
                return False, None
        
        self._last_logged[exact_key] = entry.timestamp
        
        # æ·»åŠ æŠ‘åˆ¶ä¿¡æ¯
        suppressed = self._suppressed_counts.get(exact_key, 0)
        if suppressed > 0:
            self._suppressed_counts[exact_key] = 0
            return True, f"(suppressed {suppressed} identical messages)"
        
        return True, None
    
    def _apply_content_hash(self, entry: LogEntry, rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """å†…å®¹å“ˆå¸Œå»é‡"""
        hash_key = entry.hash_key
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_cache(hash_key, rule.time_window)
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self._cache[hash_key]) >= rule.max_occurrences:
            self._suppressed_counts[hash_key] += 1
            return False, None
        
        # æ·»åŠ åˆ°ç¼“å­˜
        self._cache[hash_key].append(entry)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰¹é‡æŠ¥å‘Š
        if len(self._cache[hash_key]) == rule.max_occurrences:
            suppressed = self._suppressed_counts.get(hash_key, 0)
            if suppressed > 0:
                return True, f"(last occurrence, suppressed {suppressed} similar messages)"
        
        return True, None
    
    def _apply_pattern_match(self, entry: LogEntry, rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """æ¨¡å¼åŒ¹é…å»é‡"""
        pattern_key = f"{entry.component}:{rule.pattern}"
        
        if pattern_key in self._last_logged:
            time_since_last = entry.timestamp - self._last_logged[pattern_key]
            if time_since_last < rule.time_window:
                self._suppressed_counts[pattern_key] += 1
                return False, None
        
        self._last_logged[pattern_key] = entry.timestamp
        
        suppressed = self._suppressed_counts.get(pattern_key, 0)
        if suppressed > 0:
            self._suppressed_counts[pattern_key] = 0
            return True, f"(suppressed {suppressed} similar pattern messages)"
        
        return True, None
    
    def _apply_time_window(self, entry: LogEntry, rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """æ—¶é—´çª—å£å»é‡"""
        window_key = f"{entry.component}:{entry.hash_key}"
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self._cleanup_cache(window_key, rule.time_window)
        
        # æ£€æŸ¥çª—å£å†…çš„è®°å½•æ•°
        if len(self._cache[window_key]) >= rule.max_occurrences:
            self._suppressed_counts[window_key] += 1
            return False, None
        
        self._cache[window_key].append(entry)
        return True, None
    
    def _apply_frequency_limit(self, entry: LogEntry, rule: DeduplicationRule) -> Tuple[bool, Optional[str]]:
        """é¢‘ç‡é™åˆ¶å»é‡"""
        freq_key = f"{entry.component}:{entry.hash_key}"
        
        # æ¸…ç†è¿‡æœŸè®°å½•
        self._cleanup_cache(freq_key, rule.time_window)
        
        # æ·»åŠ å½“å‰è®°å½•
        self._cache[freq_key].append(entry)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡å¤§å°
        if len(self._cache[freq_key]) >= rule.batch_size:
            # æ‰¹é‡æŠ¥å‘Š
            batch_count = len(self._cache[freq_key])
            self._cache[freq_key].clear()
            return True, f"(batch report: {batch_count} similar operations)"
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é¢‘ç‡é™åˆ¶
        if len(self._cache[freq_key]) > rule.max_occurrences:
            self._suppressed_counts[freq_key] += 1
            return False, None
        
        return True, None
    
    def _cleanup_cache(self, key: str, time_window: int):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if key not in self._cache:
            return
        
        cutoff_time = time.time() - time_window
        self._cache[key] = [
            entry for entry in self._cache[key] 
            if entry.timestamp > cutoff_time
        ]
        
        # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œåˆ é™¤é”®
        if not self._cache[key]:
            del self._cache[key]
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        total_suppressed = sum(self._suppressed_counts.values())
        active_patterns = len([k for k, v in self._cache.items() if v])
        
        return {
            "total_suppressed_logs": total_suppressed,
            "active_deduplication_patterns": active_patterns,
            "cache_size": sum(len(entries) for entries in self._cache.values()),
            "suppression_by_pattern": dict(self._suppressed_counts),
            "rules_count": len(self.rules),
            "enabled_rules": len([r for r in self.rules.values() if r.enabled])
        }
    
    def add_rule(self, name: str, rule: DeduplicationRule):
        """æ·»åŠ è‡ªå®šä¹‰å»é‡è§„åˆ™"""
        self.rules[name] = rule
    
    def disable_rule(self, name: str):
        """ç¦ç”¨å»é‡è§„åˆ™"""
        if name in self.rules:
            self.rules[name].enabled = False
    
    def enable_rule(self, name: str):
        """å¯ç”¨å»é‡è§„åˆ™"""
        if name in self.rules:
            self.rules[name].enabled = True


class LogAggregator:
    """æ—¥å¿—èšåˆå™¨"""
    
    def __init__(self, flush_interval: int = 60):
        self.flush_interval = flush_interval
        self._aggregated_logs: Dict[str, List[LogEntry]] = defaultdict(list)
        self._counters: Dict[str, int] = defaultdict(int)
        self._last_flush = time.time()
        self._lock = threading.Lock()
    
    def aggregate(self, entry: LogEntry, aggregation_key: str):
        """èšåˆæ—¥å¿—æ¡ç›®"""
        with self._lock:
            self._aggregated_logs[aggregation_key].append(entry)
            self._counters[aggregation_key] += 1
    
    def should_flush(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°"""
        return time.time() - self._last_flush >= self.flush_interval
    
    def flush_aggregated_logs(self) -> List[Tuple[str, int, List[LogEntry]]]:
        """åˆ·æ–°èšåˆçš„æ—¥å¿—"""
        with self._lock:
            if not self.should_flush():
                return []
            
            result = []
            for key, entries in self._aggregated_logs.items():
                if entries:
                    result.append((key, self._counters[key], entries.copy()))
            
            # æ¸…ç†
            self._aggregated_logs.clear()
            self._counters.clear()
            self._last_flush = time.time()
            
            return result


class SmartLogBatcher:
    """æ™ºèƒ½æ—¥å¿—æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, 
                 batch_size: int = 50,
                 flush_interval: int = 30,
                 max_memory_mb: int = 10):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        
        self._batches: Dict[str, List[LogEntry]] = defaultdict(list)
        self._batch_sizes: Dict[str, int] = defaultdict(int)
        self._last_flush = time.time()
        self._lock = threading.Lock()
    
    def add_to_batch(self, entry: LogEntry, batch_key: str) -> Optional[List[LogEntry]]:
        """æ·»åŠ åˆ°æ‰¹å¤„ç†ï¼Œè¿”å›éœ€è¦åˆ·æ–°çš„æ‰¹æ¬¡"""
        with self._lock:
            self._batches[batch_key].append(entry)
            self._batch_sizes[batch_key] += len(entry.message.encode('utf-8'))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
            if (len(self._batches[batch_key]) >= self.batch_size or
                self._batch_sizes[batch_key] >= self.max_memory_bytes or
                self._should_flush_by_time()):
                
                return self._flush_batch(batch_key)
        
        return None
    
    def _should_flush_by_time(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æŒ‰æ—¶é—´åˆ·æ–°"""
        return time.time() - self._last_flush >= self.flush_interval
    
    def _flush_batch(self, batch_key: str) -> List[LogEntry]:
        """åˆ·æ–°æŒ‡å®šæ‰¹æ¬¡"""
        batch = self._batches[batch_key].copy()
        self._batches[batch_key].clear()
        self._batch_sizes[batch_key] = 0
        self._last_flush = time.time()
        return batch
    
    def flush_all_batches(self) -> Dict[str, List[LogEntry]]:
        """åˆ·æ–°æ‰€æœ‰æ‰¹æ¬¡"""
        with self._lock:
            result = {}
            for batch_key in list(self._batches.keys()):
                if self._batches[batch_key]:
                    result[batch_key] = self._flush_batch(batch_key)
            return result


# å…¨å±€å®ä¾‹
log_deduplicator = LogDeduplicator()
log_aggregator = LogAggregator()
log_batcher = SmartLogBatcher()


def with_deduplication(component: str):
    """æ—¥å¿—å»é‡è£…é¥°å™¨"""
    def decorator(log_func):
        def wrapper(message: str, level: str = "INFO", **kwargs):
            entry = LogEntry(
                timestamp=time.time(),
                level=level,
                component=component,
                message=message,
                context=kwargs
            )
            
            should_log, aggregation_msg = log_deduplicator.should_log(entry)
            
            if should_log:
                final_message = message
                if aggregation_msg:
                    final_message += f" {aggregation_msg}"
                
                return log_func(final_message, level=level, **kwargs)
        
        return wrapper
    return decorator
