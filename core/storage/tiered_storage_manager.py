"""
MarketPrism åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨
å®ç°çƒ­ç«¯-å†·ç«¯æ•°æ®å­˜å‚¨æ¶æ„ï¼Œæ”¯æŒæ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
"""

import asyncio
import json
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum
import structlog

# é€‚é…åˆ°ç»Ÿä¸€å­˜å‚¨ç®¡ç†å™¨ï¼ˆæ›¿ä»£æ—§çš„ UnifiedClickHouseWriter æ¥å£ï¼‰
from .unified_storage_manager import UnifiedStorageManager, UnifiedStorageConfig
from .types import (
    NormalizedOrderBook, NormalizedTrade
)


class StorageTier(Enum):
    """å­˜å‚¨å±‚çº§æšä¸¾"""
    HOT = "hot"      # çƒ­ç«¯å­˜å‚¨ï¼šé«˜é¢‘è®¿é—®ï¼ŒçŸ­æœŸä¿ç•™
    COLD = "cold"    # å†·ç«¯å­˜å‚¨ï¼šä½é¢‘è®¿é—®ï¼Œé•¿æœŸä¿ç•™
    ARCHIVE = "archive"  # å½’æ¡£å­˜å‚¨ï¼šæå°‘è®¿é—®ï¼Œæ°¸ä¹…ä¿ç•™


@dataclass
class TierConfig:
    """å­˜å‚¨å±‚çº§é…ç½®"""
    tier: StorageTier
    clickhouse_host: str
    clickhouse_port: int
    clickhouse_user: str
    clickhouse_password: str
    clickhouse_database: str
    retention_days: int
    batch_size: int = 1000
    flush_interval: int = 5
    max_retries: int = 3


@dataclass
class DataTransferTask:
    """æ•°æ®ä¼ è¾“ä»»åŠ¡"""
    task_id: str
    source_tier: StorageTier
    target_tier: StorageTier
    data_type: str
    exchange: str
    symbol: str
    start_time: datetime
    end_time: datetime
    status: str = "pending"  # pending, running, completed, failed
    created_at: datetime = None
    updated_at: datetime = None
    error_message: Optional[str] = None
    records_count: int = 0

    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now(timezone.utc)
        if self.updated_at is None:
            self.updated_at = self.created_at


class WriterAdapter:
    """å°† UnifiedStorageManager é€‚é…ä¸º TieredStorageManager æœŸæœ›çš„å†™å…¥å™¨æ¥å£"""
    def __init__(self, tier_cfg: TierConfig):
        storage_type = tier_cfg.tier.value
        uni_cfg = UnifiedStorageConfig(
            storage_type=storage_type,
            clickhouse_host=tier_cfg.clickhouse_host,
            clickhouse_port=tier_cfg.clickhouse_port,
            clickhouse_user=tier_cfg.clickhouse_user,
            clickhouse_password=tier_cfg.clickhouse_password,
            clickhouse_database=tier_cfg.clickhouse_database,
            redis_enabled=False
        )
        self.sm = UnifiedStorageManager(config=uni_cfg, config_path=None, storage_type=storage_type)

    async def initialize(self):
        await self.sm.start()

    async def close(self):
        await self.sm.stop()

    async def health_check(self):
        try:
            st = await self.sm.get_status()
            return {"status": "healthy" if st.get("is_running") else "unhealthy", "details": st}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}

    # ---------------- æ‰¹é‡å­˜å‚¨æ–¹æ³•ï¼ˆä¸æ—§æ¥å£åå¯¹é½ï¼‰ ----------------
    async def store_trades(self, data_list):
        for d in data_list:
            await self.sm.store_trade(d)
        return True

    async def store_orderbooks(self, data_list):
        import json as _json
        for d in data_list:
            # å°†JSONå­—ç¬¦ä¸²çš„bids/asksè§£æä¸ºåˆ—è¡¨
            try:
                if isinstance(d.get('bids'), str):
                    d['bids'] = _json.loads(d.get('bids') or '[]')
                if isinstance(d.get('asks'), str):
                    d['asks'] = _json.loads(d.get('asks') or '[]')
            except Exception as _e:
                # ä¿åº•ï¼šè§£æå¤±è´¥æ—¶å°†å…¶ç½®ä¸ºç©ºåˆ—è¡¨ï¼Œé¿å…å†™å…¥æ—¶å‡ºé”™
                d['bids'] = []
                d['asks'] = []
            await self.sm.store_orderbook(d)
        return True

    async def store_funding_rates(self, data_list):
        for d in data_list:
            await self.sm.store_funding_rate(d)
        return True

    async def store_open_interests(self, data_list):
        for d in data_list:
            await self.sm.store_open_interest(d)
        return True

    async def store_liquidations(self, data_list):
        for d in data_list:
            await self.sm.store_liquidation(d)
        return True

    async def store_lsrs(self, data_list):
        # å…¼å®¹ä¸¤ç±»è¡¨ï¼šlsr_top_positions ä¸ lsr_all_accounts
        for d in data_list:
            if 'long_position_ratio' in d or 'short_position_ratio' in d:
                await self.sm.store_lsr_top_position(d)
            elif 'long_account_ratio' in d or 'short_account_ratio' in d:
                await self.sm.store_lsr_all_account(d)
            else:
                # è·³è¿‡æœªçŸ¥ç»“æ„
                continue
        return True

    async def store_volatility_indices(self, data_list):
        for d in data_list:
            await self.sm.store_volatility_index(d)
        return True

    # ---------------- æŸ¥è¯¢é€‚é…ï¼ˆä»…è¦†ç›–æœ¬æ¨¡å—ç”¨åˆ°çš„å‡ ç§SQLï¼‰ ----------------
    async def execute_query(self, query: str, params: Dict[str, Any]):
        # é€‚é…æœ¬æ¨¡å—æ„é€ çš„å¸¸è§ SQLï¼ˆå« INSERT/SELECT/COUNT/DELETEï¼‰
        q = query
        try:
            def _fmt_dt(dt):
                # ç»Ÿä¸€æ ¼å¼åŒ–ä¸ºç§’çº§ï¼ˆå»æ‰æ¯«ç§’ï¼‰ï¼ŒClickHouse 23.8 çš„ toDateTime ä»…æ¥å—ç§’ç²¾åº¦
                if isinstance(dt, datetime):
                    return dt.strftime('%Y-%m-%d %H:%M:%S')
                if isinstance(dt, str):
                    s = dt.strip().replace('T', ' ')
                    # ä¼˜å…ˆå°è¯• fromisoformatï¼ˆå¯å«å¾®ç§’ï¼‰
                    try:
                        return datetime.fromisoformat(s).strftime('%Y-%m-%d %H:%M:%S')
                    except Exception:
                        # å°è¯•æ ‡å‡†ä¸å«å¾®ç§’æ ¼å¼
                        try:
                            return datetime.strptime(s, '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d %H:%M:%S')
                        except Exception:
                            # ç²—ç•¥å»æ‰å°æ•°ç§’
                            if '.' in s:
                                base = s.split('.')[0]
                                try:
                                    datetime.strptime(base, '%Y-%m-%d %H:%M:%S')
                                    return base
                                except Exception:
                                    pass
                return str(dt)
            def _q(s: str) -> str:
                return s.replace("'", "\\'")

            up = q.upper()

            # INSERT INTO <cold> SELECT ... FROM <hot> WHERE exchange/symbol/time
            if 'INSERT INTO' in up and 'SELECT' in up and '%(exchange)s' in q:
                exchange = params.get('exchange', '')
                symbol = params.get('symbol', '')
                start_time = _fmt_dt(params.get('start_time'))
                end_time = _fmt_dt(params.get('end_time'))
                # ç›´æ¥å‚æ•°æ›¿æ¢ï¼Œé¿å…è§£æ/æ‹¼æ¥å¸¦æ¥çš„æ­§ä¹‰
                q_final = (
                    q.replace("%(exchange)s", f"'{_q(exchange)}'")
                     .replace("%(symbol)s", f"'{_q(symbol)}'")
                     .replace("%(start_time)s", f"toDateTime('{start_time}')")
                     .replace("%(end_time)s", f"toDateTime('{end_time}')")
                )
                return await self.sm.clickhouse_client.execute(q_final)

            # SELECT ... WHERE exchange/symbol/time å ä½ç¬¦æ›¿æ¢
            if 'SELECT' in up and '%(exchange)s' in q and '%(start_time)s' in q:
                exchange = params.get('exchange', '')
                symbol = params.get('symbol', '')
                start_time = _fmt_dt(params.get('start_time'))
                end_time = _fmt_dt(params.get('end_time'))
                q_final = (
                    q.replace("%(exchange)s", f"'{_q(exchange)}'")
                     .replace("%(symbol)s", f"'{_q(symbol)}'")
                     .replace("%(start_time)s", f"toDateTime('{start_time}')")
                     .replace("%(end_time)s", f"toDateTime('{end_time}')")
                )
                return await self.sm.clickhouse_client.fetchall(q_final)

            # ALTER TABLE <table> DELETE WHERE timestamp < %(cutoff_time)s
            if 'ALTER TABLE' in up and 'DELETE' in up and '%(cutoff_time)s' in q:
                tbl = q.split('ALTER TABLE')[1].split('DELETE')[0].strip()
                cutoff = _fmt_dt(params.get('cutoff_time'))
                q = f"ALTER TABLE {tbl} DELETE WHERE timestamp < toDateTime('{cutoff}')"
                return await self.sm.clickhouse_client.execute(q)

            # SELECT count() FROM <table> WHERE timestamp < %(cutoff_time)s
            if 'SELECT' in up and 'COUNT()' in up and '%(cutoff_time)s' in q:
                tbl = q.split('FROM')[1].split('WHERE')[0].strip()
                cutoff = _fmt_dt(params.get('cutoff_time'))
                q = f"SELECT count() FROM {tbl} WHERE timestamp < toDateTime('{cutoff}')"
                rows = await self.sm.clickhouse_client.fetchall(q)
                return rows

            # å…œåº•ï¼šæ ¹æ®è¯­å¥ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹æ³•
            if up.strip().startswith('SELECT'):
                return await self.sm.clickhouse_client.fetchall(q)
            else:
                return await self.sm.clickhouse_client.execute(q)
        except Exception as e:
            # å¤±è´¥æŠ›å‡ºï¼Œç”±ä¸Šå±‚å†³å®šå›é€€é€»è¾‘
            raise e


class TieredStorageManager:
    """åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, hot_config: TierConfig, cold_config: TierConfig):
        """
        åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨

        Args:
            hot_config: çƒ­ç«¯å­˜å‚¨é…ç½®
            cold_config: å†·ç«¯å­˜å‚¨é…ç½®
        """
        self.logger = structlog.get_logger("core.storage.tiered_storage_manager")

        # å­˜å‚¨é…ç½®
        self.hot_config = hot_config
        self.cold_config = cold_config

        # é€‚é…åçš„å­˜å‚¨ç®¡ç†å™¨ï¼ˆä½¿ç”¨ç»Ÿä¸€å­˜å‚¨ç®¡ç†å™¨åŒ…è£…æˆå†™å…¥å™¨æ¥å£ï¼‰
        self.hot_writer: Optional["WriterAdapter"] = None
        self.cold_writer: Optional["WriterAdapter"] = None

        # æ•°æ®ä¼ è¾“ä»»åŠ¡é˜Ÿåˆ—
        self.transfer_tasks: Dict[str, DataTransferTask] = {}
        self.transfer_queue = asyncio.Queue()

        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.transfer_worker_task: Optional[asyncio.Task] = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "hot_storage": {
                "total_writes": 0,
                "failed_writes": 0,
                "last_write_time": None
            },
            "cold_storage": {
                "total_writes": 0,
                "failed_writes": 0,
                "last_write_time": None
            },
            "data_transfers": {
                "total_tasks": 0,
                "completed_tasks": 0,
                "failed_tasks": 0,
                "last_transfer_time": None
            }
        }

    async def initialize(self):
        """åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨"""
        try:
            self.logger.info("ğŸš€ åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨")

            # åˆå§‹åŒ–çƒ­ç«¯å­˜å‚¨ï¼ˆä½¿ç”¨é€‚é…å™¨ï¼‰
            self.hot_writer = WriterAdapter(self.hot_config)
            await self.hot_writer.initialize()
            try:
                hc = await self.hot_writer.health_check()
                self.logger.info("âœ… çƒ­ç«¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ",
                                 host=self.hot_config.clickhouse_host,
                                 port=self.hot_config.clickhouse_port,
                                 database=self.hot_config.clickhouse_database,
                                 health=hc)
            except Exception:
                self.logger.info("âœ… çƒ­ç«¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ",
                                 host=self.hot_config.clickhouse_host,
                                 port=self.hot_config.clickhouse_port,
                                 database=self.hot_config.clickhouse_database)

            # åˆå§‹åŒ–å†·ç«¯å­˜å‚¨ï¼ˆä½¿ç”¨é€‚é…å™¨ï¼‰
            self.cold_writer = WriterAdapter(self.cold_config)
            await self.cold_writer.initialize()
            try:
                hc2 = await self.cold_writer.health_check()
                self.logger.info("âœ… å†·ç«¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ",
                                 host=self.cold_config.clickhouse_host,
                                 port=self.cold_config.clickhouse_port,
                                 database=self.cold_config.clickhouse_database,
                                 health=hc2)
            except Exception:
                self.logger.info("âœ… å†·ç«¯å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ",
                                 host=self.cold_config.clickhouse_host,
                                 port=self.cold_config.clickhouse_port,
                                 database=self.cold_config.clickhouse_database)

            # å¯åŠ¨æ•°æ®ä¼ è¾“å·¥ä½œå™¨
            self.is_running = True
            self.transfer_worker_task = asyncio.create_task(self._transfer_worker())

            self.logger.info("âœ… åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error("âŒ åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise

    async def close(self):
        """å…³é—­åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨"""
        try:
            self.logger.info("ğŸ›‘ å…³é—­åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨")

            # åœæ­¢ä¼ è¾“å·¥ä½œå™¨
            self.is_running = False
            if self.transfer_worker_task:
                self.transfer_worker_task.cancel()
                try:
                    await self.transfer_worker_task
                except asyncio.CancelledError:
                    pass

            # å…³é—­å­˜å‚¨å†™å…¥å™¨
            if self.hot_writer:
                await self.hot_writer.close()
                self.logger.info("âœ… çƒ­ç«¯å­˜å‚¨å·²å…³é—­")

            if self.cold_writer:
                await self.cold_writer.close()
                self.logger.info("âœ… å†·ç«¯å­˜å‚¨å·²å…³é—­")

            self.logger.info("âœ… åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨å·²å…³é—­")

        except Exception as e:
            self.logger.error("âŒ å…³é—­åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨å¤±è´¥", error=str(e))

    # ==================== æ•°æ®å†™å…¥æ–¹æ³• ====================

    async def store_to_hot(self, data_type: str, data: Union[Dict, List[Dict]]) -> bool:
        """å­˜å‚¨æ•°æ®åˆ°çƒ­ç«¯"""
        try:
            if not self.hot_writer:
                self.logger.error("âŒ çƒ­ç«¯å­˜å‚¨æœªåˆå§‹åŒ–")
                return False

            # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©å­˜å‚¨æ–¹æ³•
            success = await self._store_by_type(self.hot_writer, data_type, data)

            # æ›´æ–°ç»Ÿè®¡
            if success:
                self.stats["hot_storage"]["total_writes"] += 1
                self.stats["hot_storage"]["last_write_time"] = datetime.now(timezone.utc)
            else:
                self.stats["hot_storage"]["failed_writes"] += 1

            return success

        except Exception as e:
            self.logger.error("âŒ çƒ­ç«¯å­˜å‚¨å¤±è´¥", data_type=data_type, error=str(e))
            self.stats["hot_storage"]["failed_writes"] += 1
            return False

    async def store_to_cold(self, data_type: str, data: Union[Dict, List[Dict]]) -> bool:
        """å­˜å‚¨æ•°æ®åˆ°å†·ç«¯"""
        try:
            if not self.cold_writer:
                self.logger.error("âŒ å†·ç«¯å­˜å‚¨æœªåˆå§‹åŒ–")
                return False

            # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©å­˜å‚¨æ–¹æ³•
            success = await self._store_by_type(self.cold_writer, data_type, data)

            # æ›´æ–°ç»Ÿè®¡
            if success:
                self.stats["cold_storage"]["total_writes"] += 1
                self.stats["cold_storage"]["last_write_time"] = datetime.now(timezone.utc)
            else:
                self.stats["cold_storage"]["failed_writes"] += 1

            return success

        except Exception as e:
            self.logger.error("âŒ å†·ç«¯å­˜å‚¨å¤±è´¥", data_type=data_type, error=str(e))
            self.stats["cold_storage"]["failed_writes"] += 1
            return False

    async def _store_by_type(self, writer: "WriterAdapter", data_type: str, data: Union[Dict, List[Dict]]) -> bool:
        """æ ¹æ®æ•°æ®ç±»å‹å­˜å‚¨æ•°æ®"""
        try:
            # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(data, dict):
                data = [data]

            # æ ¹æ®æ•°æ®ç±»å‹è°ƒç”¨ç›¸åº”çš„å­˜å‚¨æ–¹æ³•
            if data_type == "orderbook":
                # å†·ç«¯ï¼šä¼˜å…ˆé‡‡ç”¨æ‰¹é‡ INSERT SELECTï¼Œé¿å…é€è¡Œè½¬æ¢ä¸schemaä¸ä¸€è‡´
                if writer is self.cold_writer and isinstance(data, list) and data:
                    try:
                        exch = data[0].get('exchange')
                        sym = data[0].get('symbol')
                        ts_list = [row.get('timestamp') for row in data if row.get('timestamp') is not None]
                        start_time = min(ts_list)
                        end_time = max(ts_list)
                        sql = (
                            "INSERT INTO marketprism_cold.orderbooks ("
                            "timestamp, exchange, market_type, symbol, last_update_id, bids_count, asks_count, "
                            "best_bid_price, best_ask_price, best_bid_quantity, best_ask_quantity, bids, asks, data_source, created_at) "
                            "SELECT timestamp, exchange, market_type, symbol, last_update_id, bids_count, asks_count, "
                            "best_bid_price, best_ask_price, best_bid_quantity, best_ask_quantity, bids, asks, 'marketprism', now() "
                            "FROM marketprism_hot.orderbooks "
                            "WHERE exchange = %(exchange)s AND symbol = %(symbol)s "
                            "AND timestamp >= %(start_time)s AND timestamp <= %(end_time)s"
                        )
                        params = {"exchange": exch, "symbol": sym, "start_time": start_time, "end_time": end_time}
                        await writer.execute_query(sql, params)
                        return True
                    except Exception as be:
                        self.logger.error("âŒ è®¢å•ç°¿æ‰¹é‡è¿ç§»å¤±è´¥ï¼ˆå†·ç«¯ï¼‰", error=str(be))
                        # å†·ç«¯ä¸å†å›é€€åˆ°é€è¡Œå†™å…¥ï¼Œé¿å…å™ªéŸ³ä¸ç±»å‹ä¸ä¸€è‡´
                        return False
                # å…¶ä»–æƒ…å†µï¼ˆçƒ­ç«¯ç­‰ï¼‰ï¼šé€è¡Œ
                return await writer.store_orderbooks(data)
            elif data_type == "trade":
                if writer is self.cold_writer and isinstance(data, list) and data:
                    try:
                        exch = data[0].get('exchange')
                        sym = data[0].get('symbol')
                        ts_list = [row.get('timestamp') for row in data if row.get('timestamp') is not None]
                        start_time = min(ts_list)
                        end_time = max(ts_list)
                        sql = (
                            "INSERT INTO marketprism_cold.trades ("
                            "timestamp, exchange, market_type, symbol, trade_id, price, quantity, side, is_maker, trade_time, data_source, created_at) "
                            "SELECT timestamp, exchange, market_type, symbol, trade_id, price, quantity, side, is_maker, trade_time, 'marketprism', now() "
                            "FROM marketprism_hot.trades "
                            "WHERE exchange = %(exchange)s AND symbol = %(symbol)s "
                            "AND timestamp >= %(start_time)s AND timestamp <= %(end_time)s"
                        )
                        params = {"exchange": exch, "symbol": sym, "start_time": start_time, "end_time": end_time}
                        await writer.execute_query(sql, params)
                        return True
                    except Exception as be:
                        self.logger.error("âŒ äº¤æ˜“æ•°æ®æ‰¹é‡è¿ç§»å¤±è´¥ï¼ˆå†·ç«¯ï¼‰", error=str(be))
                        # å†·ç«¯ä¸å†å›é€€åˆ°é€è¡Œå†™å…¥ï¼Œé¿å…å™ªéŸ³ä¸ç±»å‹ä¸ä¸€è‡´
                        return False
                # å…¶ä»–æƒ…å†µï¼ˆçƒ­ç«¯ç­‰ï¼‰ï¼šé€è¡Œ
                return await writer.store_trades(data)
            elif data_type == "funding_rate":
                return await writer.store_funding_rates(data)
            elif data_type == "open_interest":
                return await writer.store_open_interests(data)
            elif data_type == "liquidation":
                return await writer.store_liquidations(data)
            elif data_type == "lsr":
                return await writer.store_lsrs(data)
            elif data_type == "volatility_index":
                return await writer.store_volatility_indices(data)
            else:
                self.logger.error("âŒ ä¸æ”¯æŒçš„æ•°æ®ç±»å‹", data_type=data_type)
                return False

        except Exception as e:
            self.logger.error("âŒ æ•°æ®å­˜å‚¨å¤±è´¥", data_type=data_type, error=str(e))
            return False

    # ==================== æ•°æ®ä¼ è¾“æ–¹æ³• ====================

    async def schedule_data_transfer(self, data_type: str, exchange: str, symbol: str,
                                   start_time: datetime, end_time: datetime) -> str:
        """è°ƒåº¦æ•°æ®ä¼ è¾“ä»»åŠ¡"""
        try:
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = f"transfer_{data_type}_{exchange}_{symbol}_{int(time.time())}"

            # åˆ›å»ºä¼ è¾“ä»»åŠ¡
            task = DataTransferTask(
                task_id=task_id,
                source_tier=StorageTier.HOT,
                target_tier=StorageTier.COLD,
                data_type=data_type,
                exchange=exchange,
                symbol=symbol,
                start_time=start_time,
                end_time=end_time
            )

            # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
            self.transfer_tasks[task_id] = task
            await self.transfer_queue.put(task)

            self.stats["data_transfers"]["total_tasks"] += 1

            self.logger.info("ğŸ“‹ æ•°æ®ä¼ è¾“ä»»åŠ¡å·²è°ƒåº¦",
                           task_id=task_id,
                           data_type=data_type,
                           exchange=exchange,
                           symbol=symbol)

            return task_id

        except Exception as e:
            self.logger.error("âŒ è°ƒåº¦æ•°æ®ä¼ è¾“ä»»åŠ¡å¤±è´¥", error=str(e))
            raise

    async def _transfer_worker(self):
        """æ•°æ®ä¼ è¾“å·¥ä½œå™¨"""
        self.logger.info("ğŸ”„ æ•°æ®ä¼ è¾“å·¥ä½œå™¨å·²å¯åŠ¨")

        while self.is_running:
            try:
                # ç­‰å¾…ä¼ è¾“ä»»åŠ¡
                task = await asyncio.wait_for(self.transfer_queue.get(), timeout=1.0)

                # æ‰§è¡Œæ•°æ®ä¼ è¾“
                await self._execute_transfer_task(task)

            except asyncio.TimeoutError:
                # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­å¾ªç¯
                continue
            except Exception as e:
                self.logger.error("âŒ æ•°æ®ä¼ è¾“å·¥ä½œå™¨å¼‚å¸¸", error=str(e))
                await asyncio.sleep(5)  # é”™è¯¯æ—¶ç­‰å¾…5ç§’

        self.logger.info("ğŸ›‘ æ•°æ®ä¼ è¾“å·¥ä½œå™¨å·²åœæ­¢")

    async def _execute_transfer_task(self, task: DataTransferTask):
        """æ‰§è¡Œæ•°æ®ä¼ è¾“ä»»åŠ¡"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®ä¼ è¾“ä»»åŠ¡", task_id=task.task_id)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = "running"
            task.updated_at = datetime.now(timezone.utc)

            # ä»çƒ­ç«¯æŸ¥è¯¢æ•°æ®
            hot_data = await self._query_hot_data(
                task.data_type, task.exchange, task.symbol,
                task.start_time, task.end_time
            )

            if not hot_data:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°éœ€è¦ä¼ è¾“çš„æ•°æ®", task_id=task.task_id)
                task.status = "completed"
                task.records_count = 0
                task.updated_at = datetime.now(timezone.utc)
                return

            # å­˜å‚¨åˆ°å†·ç«¯
            success = await self.store_to_cold(task.data_type, hot_data)

            if success:
                task.status = "completed"
                task.records_count = len(hot_data) if isinstance(hot_data, list) else 1
                self.stats["data_transfers"]["completed_tasks"] += 1
                self.stats["data_transfers"]["last_transfer_time"] = datetime.now(timezone.utc)

                self.logger.info("âœ… æ•°æ®ä¼ è¾“ä»»åŠ¡å®Œæˆ",
                               task_id=task.task_id,
                               records_count=task.records_count)

                # å¯é€‰ï¼šåˆ é™¤çƒ­ç«¯æ•°æ®ï¼ˆæ ¹æ®é…ç½®å†³å®šï¼‰
                # await self._cleanup_hot_data(task)

            else:
                task.status = "failed"
                task.error_message = "å†·ç«¯å­˜å‚¨å¤±è´¥"
                self.stats["data_transfers"]["failed_tasks"] += 1

                self.logger.error("âŒ æ•°æ®ä¼ è¾“ä»»åŠ¡å¤±è´¥", task_id=task.task_id)

            task.updated_at = datetime.now(timezone.utc)

        except Exception as e:
            task.status = "failed"
            task.error_message = str(e)
            task.updated_at = datetime.now(timezone.utc)
            self.stats["data_transfers"]["failed_tasks"] += 1

            self.logger.error("âŒ æ‰§è¡Œæ•°æ®ä¼ è¾“ä»»åŠ¡å¼‚å¸¸",
                            task_id=task.task_id, error=str(e))

    async def _query_hot_data(self, data_type: str, exchange: str, symbol: str,
                            start_time: datetime, end_time: datetime) -> List[Dict]:
        """ä»çƒ­ç«¯æŸ¥è¯¢æ•°æ®"""
        try:
            if not self.hot_writer:
                return []

            # æ„å»ºæŸ¥è¯¢SQL
            table_name = self._get_table_name(data_type)
            query = f"""
                SELECT * FROM {table_name}
                WHERE exchange = %(exchange)s
                AND symbol = %(symbol)s
                AND timestamp >= %(start_time)s
                AND timestamp < %(end_time)s
                ORDER BY timestamp
            """

            params = {
                'exchange': exchange,
                'symbol': symbol,
                'start_time': start_time,
                'end_time': end_time
            }

            # æ‰§è¡ŒæŸ¥è¯¢
            result = await self.hot_writer.execute_query(query, params)

            self.logger.debug("ğŸ“Š çƒ­ç«¯æ•°æ®æŸ¥è¯¢å®Œæˆ",
                            data_type=data_type,
                            exchange=exchange,
                            symbol=symbol,
                            records_count=len(result))

            return result

        except Exception as e:
            self.logger.error("âŒ çƒ­ç«¯æ•°æ®æŸ¥è¯¢å¤±è´¥", error=str(e))
            return []

    def _get_table_name(self, data_type: str) -> str:
        """è·å–æ•°æ®ç±»å‹å¯¹åº”çš„è¡¨å"""
        table_mapping = {
            "orderbook": "orderbooks",
            "trade": "trades",
            "funding_rate": "funding_rates",
            "open_interest": "open_interests",
            "liquidation": "liquidations",
            #   LSR  
            #    "lsrs"   "lsr_top_positions"
            "lsr": "lsr_top_positions",
            "volatility_index": "volatility_indices"
        }
        return table_mapping.get(data_type, data_type)

    # ==================== æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

    async def cleanup_expired_hot_data(self) -> Dict[str, Any]:
        """æ¸…ç†è¿‡æœŸçš„çƒ­ç«¯æ•°æ®"""
        try:
            self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸçƒ­ç«¯æ•°æ®")

            cutoff_time = datetime.now(timezone.utc) - timedelta(days=self.hot_config.retention_days)

            cleanup_summary = {
                "cutoff_time": cutoff_time.isoformat(),
                "tables_cleaned": {},
                "total_records_deleted": 0
            }

            # æ¸…ç†å„ç§æ•°æ®ç±»å‹çš„è¡¨
            data_types = ["orderbook", "trade", "funding_rate", "open_interest",
                         "liquidation", "lsr", "volatility_index"]

            for data_type in data_types:
                try:
                    table_name = self._get_table_name(data_type)
                    deleted_count = await self._cleanup_table_data(table_name, cutoff_time)

                    cleanup_summary["tables_cleaned"][table_name] = deleted_count
                    cleanup_summary["total_records_deleted"] += deleted_count

                    self.logger.info("âœ… è¡¨æ•°æ®æ¸…ç†å®Œæˆ",
                                   table=table_name,
                                   deleted_count=deleted_count)

                except Exception as e:
                    self.logger.error("âŒ è¡¨æ•°æ®æ¸…ç†å¤±è´¥",
                                    table=table_name, error=str(e))
                    cleanup_summary["tables_cleaned"][table_name] = f"error: {str(e)}"

            self.logger.info("âœ… çƒ­ç«¯æ•°æ®æ¸…ç†å®Œæˆ",
                           total_deleted=cleanup_summary["total_records_deleted"])

            return cleanup_summary

        except Exception as e:
            self.logger.error("âŒ æ¸…ç†è¿‡æœŸçƒ­ç«¯æ•°æ®å¤±è´¥", error=str(e))
            raise

    async def _cleanup_table_data(self, table_name: str, cutoff_time: datetime) -> int:
        """æ¸…ç†æŒ‡å®šè¡¨çš„è¿‡æœŸæ•°æ®"""
        try:
            if not self.hot_writer:
                return 0

            # æ„å»ºåˆ é™¤SQL
            delete_query = f"""
                ALTER TABLE {table_name} DELETE
                WHERE timestamp < %(cutoff_time)s
            """

            params = {'cutoff_time': cutoff_time}

            # æ‰§è¡Œåˆ é™¤
            await self.hot_writer.execute_query(delete_query, params)

            # æŸ¥è¯¢åˆ é™¤çš„è®°å½•æ•°ï¼ˆClickHouseçš„DELETEæ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œè¿”å›ä¼°ç®—å€¼ï¼‰
            count_query = f"""
                SELECT count() FROM {table_name}
                WHERE timestamp < %(cutoff_time)s
            """

            result = await self.hot_writer.execute_query(count_query, params)
            deleted_count = result[0]['count()'] if result else 0

            return deleted_count

        except Exception as e:
            self.logger.error("âŒ æ¸…ç†è¡¨æ•°æ®å¤±è´¥", table=table_name, error=str(e))
            return 0

    # ==================== çŠ¶æ€æŸ¥è¯¢æ–¹æ³• ====================

    def get_transfer_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä¼ è¾“ä»»åŠ¡çŠ¶æ€"""
        task = self.transfer_tasks.get(task_id)
        if not task:
            return None

        return {
            "task_id": task.task_id,
            "source_tier": task.source_tier.value,
            "target_tier": task.target_tier.value,
            "data_type": task.data_type,
            "exchange": task.exchange,
            "symbol": task.symbol,
            "start_time": task.start_time.isoformat(),
            "end_time": task.end_time.isoformat(),
            "status": task.status,
            "created_at": task.created_at.isoformat(),
            "updated_at": task.updated_at.isoformat(),
            "error_message": task.error_message,
            "records_count": task.records_count
        }

    def get_all_transfer_tasks(self, status_filter: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä¼ è¾“ä»»åŠ¡"""
        tasks = []
        for task in self.transfer_tasks.values():
            if status_filter is None or task.status == status_filter:
                tasks.append(self.get_transfer_task_status(task.task_id))

        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        tasks.sort(key=lambda x: x["created_at"], reverse=True)
        return tasks

    def get_storage_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "hot_storage": {
                **self.stats["hot_storage"],
                "config": {
                    "host": self.hot_config.clickhouse_host,
                    "database": self.hot_config.clickhouse_database,
                    "retention_days": self.hot_config.retention_days
                },
                "status": "connected" if self.hot_writer else "disconnected"
            },
            "cold_storage": {
                **self.stats["cold_storage"],
                "config": {
                    "host": self.cold_config.clickhouse_host,
                    "database": self.cold_config.clickhouse_database,
                    "retention_days": self.cold_config.retention_days
                },
                "status": "connected" if self.cold_writer else "disconnected"
            },
            "data_transfers": {
                **self.stats["data_transfers"],
                "pending_tasks": len([t for t in self.transfer_tasks.values() if t.status == "pending"]),
                "running_tasks": len([t for t in self.transfer_tasks.values() if t.status == "running"]),
                "queue_size": self.transfer_queue.qsize()
            },
            "system": {
                "is_running": self.is_running,
                "transfer_worker_active": self.transfer_worker_task is not None and not self.transfer_worker_task.done()
            }
        }

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "components": {}
        }

        # æ£€æŸ¥çƒ­ç«¯å­˜å‚¨
        try:
            if self.hot_writer:
                await self.hot_writer.health_check()
                health_status["components"]["hot_storage"] = {"status": "healthy"}
            else:
                health_status["components"]["hot_storage"] = {"status": "not_initialized"}
        except Exception as e:
            health_status["components"]["hot_storage"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            health_status["status"] = "degraded"

        # æ£€æŸ¥å†·ç«¯å­˜å‚¨
        try:
            if self.cold_writer:
                await self.cold_writer.health_check()
                health_status["components"]["cold_storage"] = {"status": "healthy"}
            else:
                health_status["components"]["cold_storage"] = {"status": "not_initialized"}
        except Exception as e:
            health_status["components"]["cold_storage"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            health_status["status"] = "degraded"

        # æ£€æŸ¥ä¼ è¾“å·¥ä½œå™¨
        if self.is_running and self.transfer_worker_task and not self.transfer_worker_task.done():
            health_status["components"]["transfer_worker"] = {"status": "healthy"}
        else:
            health_status["components"]["transfer_worker"] = {"status": "stopped"}
            if health_status["status"] == "healthy":
                health_status["status"] = "degraded"

        return health_status

    # ==================== ä¾¿æ·æ–¹æ³• ====================

    async def auto_schedule_transfers(self, data_types: List[str] = None,
                                    exchanges: List[str] = None,
                                    lookback_hours: int = 24) -> List[str]:
        """è‡ªåŠ¨è°ƒåº¦æ•°æ®ä¼ è¾“ä»»åŠ¡"""
        try:
            if data_types is None:
                data_types = ["orderbook", "trade", "funding_rate", "open_interest",
                             "liquidation", "lsr", "volatility_index"]

            if exchanges is None:
                exchanges = ["binance_spot", "binance_derivatives", "okx_spot",
                           "okx_derivatives", "deribit_derivatives"]

            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now(timezone.utc)
            start_time = end_time - timedelta(hours=lookback_hours)

            task_ids = []

            for data_type in data_types:
                for exchange in exchanges:
                    # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è·å–symbolåˆ—è¡¨
                    # æš‚æ—¶ä½¿ç”¨é€šç”¨symbol
                    symbol = "BTC-USDT"  # å¯ä»¥ä»é…ç½®æˆ–æ•°æ®åº“è·å–

                    try:
                        task_id = await self.schedule_data_transfer(
                            data_type, exchange, symbol, start_time, end_time
                        )
                        task_ids.append(task_id)
                    except Exception as e:
                        self.logger.error("âŒ è‡ªåŠ¨è°ƒåº¦ä¼ è¾“ä»»åŠ¡å¤±è´¥",
                                        data_type=data_type,
                                        exchange=exchange,
                                        error=str(e))

            self.logger.info("âœ… è‡ªåŠ¨è°ƒåº¦ä¼ è¾“ä»»åŠ¡å®Œæˆ",
                           scheduled_tasks=len(task_ids),
                           lookback_hours=lookback_hours)

            return task_ids

        except Exception as e:
            self.logger.error("âŒ è‡ªåŠ¨è°ƒåº¦ä¼ è¾“ä»»åŠ¡å¤±è´¥", error=str(e))
            return []
