# ğŸ† MarketPrism ç»Ÿä¸€æ•°æ®å¤„ç†æœ€ä½³å®è·µ

## ğŸ¯ **æ¦‚è¿°**

æœ¬æ–‡æ¡£æä¾›äº†MarketPrismç»Ÿä¸€äº¤æ˜“æ•°æ®æ ‡å‡†åŒ–å™¨çš„æœ€ä½³å®è·µæŒ‡å—ï¼ŒåŒ…æ‹¬æ€§èƒ½ä¼˜åŒ–ã€é”™è¯¯å¤„ç†ã€æ•°æ®è´¨é‡ä¿è¯ã€ç›‘æ§å‘Šè­¦ç­‰æ–¹é¢çš„å®è·µç»éªŒã€‚

## ğŸ”§ **æ•°æ®æ ‡å‡†åŒ–æœ€ä½³å®è·µ**

### **1. æ ‡å‡†åŒ–å™¨ä½¿ç”¨åŸåˆ™**

```python
# âœ… æ¨èï¼šä½¿ç”¨å•ä¾‹æ¨¡å¼
class NormalizerManager:
    _instance = None
    _normalizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._normalizer = DataNormalizer()
        return cls._instance
    
    def get_normalizer(self):
        return self._normalizer

# ä½¿ç”¨æ–¹å¼
normalizer_manager = NormalizerManager()
normalizer = normalizer_manager.get_normalizer()

# âŒ é¿å…ï¼šé¢‘ç¹åˆ›å»ºæ ‡å‡†åŒ–å™¨å®ä¾‹
# for data in data_list:
#     normalizer = DataNormalizer()  # æ€§èƒ½æµªè´¹
#     result = normalizer.normalize_binance_spot_trade(data)
```

### **2. æ•°æ®éªŒè¯ç­–ç•¥**

```python
def validate_input_data(data, data_type):
    """è¾“å…¥æ•°æ®éªŒè¯"""
    validation_rules = {
        "binance_spot": {
            "required_fields": ["e", "s", "t", "p", "q", "T", "m"],
            "field_types": {
                "p": (str, float),
                "q": (str, float),
                "T": int,
                "m": bool
            }
        },
        "okx": {
            "required_fields": ["data"],
            "nested_required": ["instId", "tradeId", "px", "sz", "side", "ts"]
        }
    }
    
    rules = validation_rules.get(data_type)
    if not rules:
        return False, f"æœªçŸ¥æ•°æ®ç±»å‹: {data_type}"
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    for field in rules["required_fields"]:
        if field not in data:
            return False, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
    
    # æ£€æŸ¥å­—æ®µç±»å‹
    if "field_types" in rules:
        for field, expected_types in rules["field_types"].items():
            if field in data and not isinstance(data[field], expected_types):
                return False, f"å­—æ®µç±»å‹é”™è¯¯: {field}"
    
    return True, "éªŒè¯é€šè¿‡"

# ä½¿ç”¨ç¤ºä¾‹
def safe_normalize(normalizer, data, data_type):
    is_valid, message = validate_input_data(data, data_type)
    if not is_valid:
        print(f"æ•°æ®éªŒè¯å¤±è´¥: {message}")
        return None
    
    # æ‰§è¡Œæ ‡å‡†åŒ–
    if data_type == "binance_spot":
        return normalizer.normalize_binance_spot_trade(data)
    elif data_type == "okx":
        return normalizer.normalize_okx_trade(data)
```

### **3. é”™è¯¯å¤„ç†ç­–ç•¥**

```python
import logging
from enum import Enum

class NormalizationError(Exception):
    """æ ‡å‡†åŒ–é”™è¯¯åŸºç±»"""
    pass

class DataValidationError(NormalizationError):
    """æ•°æ®éªŒè¯é”™è¯¯"""
    pass

class NormalizationResult(Enum):
    SUCCESS = "success"
    VALIDATION_ERROR = "validation_error"
    PROCESSING_ERROR = "processing_error"
    UNKNOWN_ERROR = "unknown_error"

def robust_normalize(normalizer, data, data_type, logger=None):
    """å¥å£®çš„æ•°æ®æ ‡å‡†åŒ–å¤„ç†"""
    if logger is None:
        logger = logging.getLogger(__name__)
    
    try:
        # æ•°æ®éªŒè¯
        is_valid, validation_message = validate_input_data(data, data_type)
        if not is_valid:
            logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥: {validation_message}")
            return None, NormalizationResult.VALIDATION_ERROR, validation_message
        
        # æ‰§è¡Œæ ‡å‡†åŒ–
        if data_type == "binance_spot":
            result = normalizer.normalize_binance_spot_trade(data)
        elif data_type == "binance_futures":
            result = normalizer.normalize_binance_futures_trade(data)
        elif data_type == "okx":
            result = normalizer.normalize_okx_trade(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")
        
        if result is None:
            logger.error(f"æ ‡å‡†åŒ–è¿”å›None: {data}")
            return None, NormalizationResult.PROCESSING_ERROR, "æ ‡å‡†åŒ–å¤±è´¥"
        
        logger.debug(f"æ ‡å‡†åŒ–æˆåŠŸ: {result.trade_id}")
        return result, NormalizationResult.SUCCESS, "æˆåŠŸ"
        
    except DataValidationError as e:
        logger.error(f"æ•°æ®éªŒè¯å¼‚å¸¸: {e}")
        return None, NormalizationResult.VALIDATION_ERROR, str(e)
    except Exception as e:
        logger.error(f"æ ‡å‡†åŒ–å¼‚å¸¸: {e}", exc_info=True)
        return None, NormalizationResult.UNKNOWN_ERROR, str(e)
```

## ğŸ“Š **æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ**

### **1. æ‰¹é‡å¤„ç†ä¼˜åŒ–**

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple

class BatchNormalizer:
    """æ‰¹é‡æ ‡å‡†åŒ–å¤„ç†å™¨"""
    
    def __init__(self, normalizer, max_workers=4):
        self.normalizer = normalizer
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    def process_batch_sync(self, data_batch: List[Tuple[dict, str]], batch_size=1000):
        """åŒæ­¥æ‰¹é‡å¤„ç†"""
        results = []
        errors = []
        
        for i in range(0, len(data_batch), batch_size):
            batch = data_batch[i:i + batch_size]
            
            for data, data_type in batch:
                result, status, message = robust_normalize(self.normalizer, data, data_type)
                
                if status == NormalizationResult.SUCCESS:
                    results.append(result)
                else:
                    errors.append({
                        "data": data,
                        "data_type": data_type,
                        "error": message,
                        "status": status
                    })
        
        return results, errors
    
    async def process_batch_async(self, data_batch: List[Tuple[dict, str]]):
        """å¼‚æ­¥æ‰¹é‡å¤„ç†"""
        loop = asyncio.get_event_loop()
        
        tasks = []
        for data, data_type in data_batch:
            task = loop.run_in_executor(
                self.executor,
                robust_normalize,
                self.normalizer,
                data,
                data_type
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful_results = []
        errors = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                errors.append({
                    "index": i,
                    "error": str(result),
                    "data": data_batch[i][0]
                })
            elif result[1] == NormalizationResult.SUCCESS:
                successful_results.append(result[0])
            else:
                errors.append({
                    "index": i,
                    "error": result[2],
                    "status": result[1],
                    "data": data_batch[i][0]
                })
        
        return successful_results, errors

# ä½¿ç”¨ç¤ºä¾‹
batch_normalizer = BatchNormalizer(normalizer)

# åŒæ­¥å¤„ç†
results, errors = batch_normalizer.process_batch_sync(data_batch)

# å¼‚æ­¥å¤„ç†
# results, errors = await batch_normalizer.process_batch_async(data_batch)
```

### **2. å†…å­˜ç®¡ç†ä¼˜åŒ–**

```python
import gc
from memory_profiler import profile

class MemoryEfficientProcessor:
    """å†…å­˜é«˜æ•ˆçš„æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, normalizer, memory_limit_mb=500):
        self.normalizer = normalizer
        self.memory_limit_bytes = memory_limit_mb * 1024 * 1024
        self.processed_count = 0
        self.gc_interval = 1000  # æ¯å¤„ç†1000æ¡æ•°æ®æ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
    
    def process_stream(self, data_stream):
        """æµå¼å¤„ç†æ•°æ®"""
        for data_item in data_stream:
            try:
                # å¤„ç†å•æ¡æ•°æ®
                result = self._process_single_item(data_item)
                
                if result:
                    yield result
                
                self.processed_count += 1
                
                # å®šæœŸåƒåœ¾å›æ”¶
                if self.processed_count % self.gc_interval == 0:
                    gc.collect()
                    
            except Exception as e:
                print(f"å¤„ç†æ•°æ®é¡¹å¤±è´¥: {e}")
                continue
    
    def _process_single_item(self, data_item):
        """å¤„ç†å•æ¡æ•°æ®é¡¹"""
        data, data_type = data_item
        
        result, status, message = robust_normalize(self.normalizer, data, data_type)
        
        # ç«‹å³æ¸…ç†åŸå§‹æ•°æ®å¼•ç”¨
        del data
        
        return result if status == NormalizationResult.SUCCESS else None

# ä½¿ç”¨ç¤ºä¾‹
processor = MemoryEfficientProcessor(normalizer)

# æµå¼å¤„ç†å¤§é‡æ•°æ®
for normalized_trade in processor.process_stream(large_data_stream):
    # ç«‹å³å¤„ç†ç»“æœï¼Œä¸å­˜å‚¨åœ¨å†…å­˜ä¸­
    process_trade_immediately(normalized_trade)
```

## ğŸ” **æ•°æ®è´¨é‡ä¿è¯**

### **1. æ•°æ®è´¨é‡ç›‘æ§**

```python
from dataclasses import dataclass
from typing import Dict, List
import time

@dataclass
class QualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    total_processed: int = 0
    successful_normalizations: int = 0
    validation_errors: int = 0
    processing_errors: int = 0
    unknown_errors: int = 0
    avg_processing_time: float = 0.0
    
    @property
    def success_rate(self) -> float:
        return self.successful_normalizations / self.total_processed if self.total_processed > 0 else 0
    
    @property
    def error_rate(self) -> float:
        return 1 - self.success_rate

class QualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = QualityMetrics()
        self.processing_times = []
        self.error_samples = []
        self.max_error_samples = 100
    
    def record_processing(self, result, status, processing_time, data_sample=None):
        """è®°å½•å¤„ç†ç»“æœ"""
        self.metrics.total_processed += 1
        self.processing_times.append(processing_time)
        
        if status == NormalizationResult.SUCCESS:
            self.metrics.successful_normalizations += 1
        elif status == NormalizationResult.VALIDATION_ERROR:
            self.metrics.validation_errors += 1
            self._record_error_sample(data_sample, "validation_error")
        elif status == NormalizationResult.PROCESSING_ERROR:
            self.metrics.processing_errors += 1
            self._record_error_sample(data_sample, "processing_error")
        else:
            self.metrics.unknown_errors += 1
            self._record_error_sample(data_sample, "unknown_error")
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        self.metrics.avg_processing_time = sum(self.processing_times) / len(self.processing_times)
    
    def _record_error_sample(self, data_sample, error_type):
        """è®°å½•é”™è¯¯æ ·æœ¬"""
        if len(self.error_samples) >= self.max_error_samples:
            self.error_samples.pop(0)  # ç§»é™¤æœ€æ—§çš„æ ·æœ¬
        
        self.error_samples.append({
            "timestamp": time.time(),
            "error_type": error_type,
            "data_sample": data_sample
        })
    
    def get_quality_report(self) -> Dict:
        """è·å–è´¨é‡æŠ¥å‘Š"""
        return {
            "metrics": {
                "total_processed": self.metrics.total_processed,
                "success_rate": f"{self.metrics.success_rate:.2%}",
                "error_rate": f"{self.metrics.error_rate:.2%}",
                "avg_processing_time_ms": f"{self.metrics.avg_processing_time * 1000:.2f}",
            },
            "error_breakdown": {
                "validation_errors": self.metrics.validation_errors,
                "processing_errors": self.metrics.processing_errors,
                "unknown_errors": self.metrics.unknown_errors,
            },
            "recent_errors": self.error_samples[-10:],  # æœ€è¿‘10ä¸ªé”™è¯¯
            "performance": {
                "min_processing_time_ms": f"{min(self.processing_times) * 1000:.2f}" if self.processing_times else "N/A",
                "max_processing_time_ms": f"{max(self.processing_times) * 1000:.2f}" if self.processing_times else "N/A",
                "p95_processing_time_ms": f"{sorted(self.processing_times)[int(len(self.processing_times) * 0.95)] * 1000:.2f}" if len(self.processing_times) > 20 else "N/A"
            }
        }

# ä½¿ç”¨ç¤ºä¾‹
quality_monitor = QualityMonitor()

def monitored_normalize(normalizer, data, data_type):
    """å¸¦ç›‘æ§çš„æ ‡å‡†åŒ–å¤„ç†"""
    start_time = time.time()
    
    result, status, message = robust_normalize(normalizer, data, data_type)
    
    processing_time = time.time() - start_time
    quality_monitor.record_processing(result, status, processing_time, data)
    
    return result, status, message

# å®šæœŸè·å–è´¨é‡æŠ¥å‘Š
def print_quality_report():
    report = quality_monitor.get_quality_report()
    print("æ•°æ®è´¨é‡æŠ¥å‘Š:")
    print(f"  æˆåŠŸç‡: {report['metrics']['success_rate']}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {report['metrics']['avg_processing_time_ms']}ms")
    print(f"  éªŒè¯é”™è¯¯: {report['error_breakdown']['validation_errors']}")
```

## ğŸš¨ **ç›‘æ§å’Œå‘Šè­¦**

### **1. å®æ—¶ç›‘æ§æŒ‡æ ‡**

```python
import time
from collections import deque, defaultdict

class RealTimeMonitor:
    """å®æ—¶ç›‘æ§å™¨"""
    
    def __init__(self, window_size=300):  # 5åˆ†é’Ÿçª—å£
        self.window_size = window_size
        self.metrics_window = deque(maxlen=window_size)
        self.exchange_metrics = defaultdict(lambda: deque(maxlen=window_size))
        self.alert_thresholds = {
            "error_rate": 0.05,  # 5%é”™è¯¯ç‡é˜ˆå€¼
            "processing_time": 0.1,  # 100mså¤„ç†æ—¶é—´é˜ˆå€¼
            "throughput": 100,  # æ¯ç§’100æ¡æ•°æ®é˜ˆå€¼
        }
    
    def record_metric(self, exchange, success, processing_time):
        """è®°å½•æŒ‡æ ‡"""
        timestamp = time.time()
        metric = {
            "timestamp": timestamp,
            "exchange": exchange,
            "success": success,
            "processing_time": processing_time
        }
        
        self.metrics_window.append(metric)
        self.exchange_metrics[exchange].append(metric)
        
        # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
        self._check_alerts()
    
    def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        current_time = time.time()
        
        # æ£€æŸ¥æ•´ä½“é”™è¯¯ç‡
        recent_metrics = [m for m in self.metrics_window if current_time - m["timestamp"] < 60]  # æœ€è¿‘1åˆ†é’Ÿ
        if recent_metrics:
            error_rate = 1 - (sum(1 for m in recent_metrics if m["success"]) / len(recent_metrics))
            if error_rate > self.alert_thresholds["error_rate"]:
                self._trigger_alert("HIGH_ERROR_RATE", f"é”™è¯¯ç‡: {error_rate:.2%}")
        
        # æ£€æŸ¥å¤„ç†æ—¶é—´
        if recent_metrics:
            avg_processing_time = sum(m["processing_time"] for m in recent_metrics) / len(recent_metrics)
            if avg_processing_time > self.alert_thresholds["processing_time"]:
                self._trigger_alert("HIGH_PROCESSING_TIME", f"å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.3f}s")
        
        # æ£€æŸ¥ååé‡
        if len(recent_metrics) < self.alert_thresholds["throughput"] / 60:  # æ¯åˆ†é’ŸæœŸæœ›çš„æœ€å°æ•°æ®é‡
            self._trigger_alert("LOW_THROUGHPUT", f"ååé‡è¿‡ä½: {len(recent_metrics)}/min")
    
    def _trigger_alert(self, alert_type, message):
        """è§¦å‘å‘Šè­¦"""
        print(f"ğŸš¨ å‘Šè­¦ [{alert_type}]: {message}")
        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿï¼Œå¦‚å‘é€é‚®ä»¶ã€Slacké€šçŸ¥ç­‰
    
    def get_dashboard_data(self):
        """è·å–ä»ªè¡¨æ¿æ•°æ®"""
        current_time = time.time()
        recent_metrics = [m for m in self.metrics_window if current_time - m["timestamp"] < 300]  # æœ€è¿‘5åˆ†é’Ÿ
        
        if not recent_metrics:
            return {"status": "no_data"}
        
        # æŒ‰äº¤æ˜“æ‰€ç»Ÿè®¡
        exchange_stats = {}
        for exchange in self.exchange_metrics:
            exchange_recent = [m for m in self.exchange_metrics[exchange] if current_time - m["timestamp"] < 300]
            if exchange_recent:
                success_count = sum(1 for m in exchange_recent if m["success"])
                exchange_stats[exchange] = {
                    "total": len(exchange_recent),
                    "success": success_count,
                    "success_rate": success_count / len(exchange_recent),
                    "avg_processing_time": sum(m["processing_time"] for m in exchange_recent) / len(exchange_recent)
                }
        
        return {
            "status": "active",
            "overall": {
                "total_processed": len(recent_metrics),
                "success_rate": sum(1 for m in recent_metrics if m["success"]) / len(recent_metrics),
                "avg_processing_time": sum(m["processing_time"] for m in recent_metrics) / len(recent_metrics),
                "throughput_per_minute": len(recent_metrics)
            },
            "by_exchange": exchange_stats,
            "timestamp": current_time
        }

# ä½¿ç”¨ç¤ºä¾‹
monitor = RealTimeMonitor()

def monitored_process(normalizer, data, data_type, exchange):
    """å¸¦å®æ—¶ç›‘æ§çš„å¤„ç†"""
    start_time = time.time()
    
    result, status, message = robust_normalize(normalizer, data, data_type)
    
    processing_time = time.time() - start_time
    success = (status == NormalizationResult.SUCCESS)
    
    monitor.record_metric(exchange, success, processing_time)
    
    return result, status, message

# å®šæœŸè·å–ä»ªè¡¨æ¿æ•°æ®
dashboard_data = monitor.get_dashboard_data()
print(f"å®æ—¶ç›‘æ§æ•°æ®: {dashboard_data}")
```

## ğŸ“ **é…ç½®ç®¡ç†æœ€ä½³å®è·µ**

### **1. ç¯å¢ƒé…ç½®ç®¡ç†**

```python
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class NormalizationConfig:
    """æ ‡å‡†åŒ–é…ç½®"""
    # æ€§èƒ½é…ç½®
    batch_size: int = 1000
    max_workers: int = 4
    memory_limit_mb: int = 500
    gc_interval: int = 1000
    
    # è´¨é‡é…ç½®
    max_error_samples: int = 100
    error_rate_threshold: float = 0.05
    processing_time_threshold: float = 0.1
    
    # ç›‘æ§é…ç½®
    monitoring_window_size: int = 300
    alert_cooldown_seconds: int = 60
    
    # æ—¥å¿—é…ç½®
    log_level: str = "INFO"
    log_format: str = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    @classmethod
    def from_env(cls):
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return cls(
            batch_size=int(os.getenv("NORM_BATCH_SIZE", 1000)),
            max_workers=int(os.getenv("NORM_MAX_WORKERS", 4)),
            memory_limit_mb=int(os.getenv("NORM_MEMORY_LIMIT_MB", 500)),
            gc_interval=int(os.getenv("NORM_GC_INTERVAL", 1000)),
            max_error_samples=int(os.getenv("NORM_MAX_ERROR_SAMPLES", 100)),
            error_rate_threshold=float(os.getenv("NORM_ERROR_RATE_THRESHOLD", 0.05)),
            processing_time_threshold=float(os.getenv("NORM_PROCESSING_TIME_THRESHOLD", 0.1)),
            monitoring_window_size=int(os.getenv("NORM_MONITORING_WINDOW_SIZE", 300)),
            alert_cooldown_seconds=int(os.getenv("NORM_ALERT_COOLDOWN_SECONDS", 60)),
            log_level=os.getenv("NORM_LOG_LEVEL", "INFO"),
            log_format=os.getenv("NORM_LOG_FORMAT", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )

# ä½¿ç”¨ç¤ºä¾‹
config = NormalizationConfig.from_env()
batch_normalizer = BatchNormalizer(normalizer, max_workers=config.max_workers)
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2024-12-19  
**ç»´æŠ¤è€…**: MarketPrismå¼€å‘å›¢é˜Ÿ
