# MarketPrism æµ‹è¯•è¦†ç›–ç‡æå‡æ–¹æ¡ˆ

## ğŸ§ª æµ‹è¯•è¦†ç›–ç‡æå‡å®Œæ•´æ–¹æ¡ˆ

**åˆ¶å®šæ—¶é—´**: 2025-05-24  
**é€‚ç”¨èŒƒå›´**: å…¨é¡¹ç›®æµ‹è¯•ä½“ç³»  
**ä¼˜åŒ–ç›®æ ‡**: ä¼ä¸šçº§æµ‹è¯•æ ‡å‡†  

## ğŸ¯ æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

### æ ¸å¿ƒæµ‹è¯•æŒ‡æ ‡
- **å•å…ƒæµ‹è¯•è¦†ç›–ç‡**: å½“å‰60% â†’ ç›®æ ‡90%+
- **é›†æˆæµ‹è¯•è¦†ç›–ç‡**: å½“å‰30% â†’ ç›®æ ‡85%+
- **ç«¯åˆ°ç«¯æµ‹è¯•è¦†ç›–ç‡**: å½“å‰10% â†’ ç›®æ ‡70%+
- **æ€§èƒ½åŸºå‡†æµ‹è¯•**: å»ºç«‹å®Œæ•´åŸºå‡†ä½“ç³»
- **æ•…éšœæ¢å¤æµ‹è¯•**: è¦†ç›–95%æ•…éšœåœºæ™¯

## ğŸ“Š å½“å‰æµ‹è¯•çŠ¶å†µåˆ†æ

### 1. ç°æœ‰æµ‹è¯•ç»“æ„
```
tests/
â”œâ”€â”€ unit/                    # å•å…ƒæµ‹è¯• (60%è¦†ç›–ç‡)
â”‚   â”œâ”€â”€ api/                # APIå±‚æµ‹è¯•
â”‚   â”œâ”€â”€ config/             # é…ç½®ç®¡ç†æµ‹è¯•
â”‚   â”œâ”€â”€ models/             # æ•°æ®æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ monitoring/         # ç›‘æ§æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ services/           # æœåŠ¡å±‚æµ‹è¯•
â”‚   â”œâ”€â”€ storage/            # å­˜å‚¨å±‚æµ‹è¯•
â”‚   â””â”€â”€ utils/              # å·¥å…·å‡½æ•°æµ‹è¯•
â”œâ”€â”€ integration/             # é›†æˆæµ‹è¯• (30%è¦†ç›–ç‡)
â”‚   â”œâ”€â”€ api/                # APIé›†æˆæµ‹è¯•
â”‚   â””â”€â”€ services/           # æœåŠ¡é›†æˆæµ‹è¯•
â”œâ”€â”€ performance/             # æ€§èƒ½æµ‹è¯• (åŸºç¡€)
â”œâ”€â”€ load_testing/           # è´Ÿè½½æµ‹è¯• (åŸºç¡€)
â””â”€â”€ fixtures/               # æµ‹è¯•æ•°æ®
```

### 2. æµ‹è¯•è¦†ç›–ç‡ç¼ºå£
- **Python-Collector**: ç¼ºå°‘å®Œæ•´çš„å¼‚æ­¥å¤„ç†æµ‹è¯•
- **ç›‘æ§ç³»ç»Ÿ**: ç¼ºå°‘PrometheusæŒ‡æ ‡æµ‹è¯•
- **é…ç½®ç®¡ç†**: ç¼ºå°‘é…ç½®éªŒè¯æµ‹è¯•
- **é”™è¯¯å¤„ç†**: ç¼ºå°‘å¼‚å¸¸åœºæ™¯æµ‹è¯•
- **æ€§èƒ½åŸºå‡†**: ç¼ºå°‘ç³»ç»Ÿæ€§èƒ½åŸºå‡†

## ğŸ”§ é›†æˆæµ‹è¯•å®Œå–„

### 1. Python-Collectoré›†æˆæµ‹è¯•
```python
# tests/integration/test_python_collector_comprehensive.py
import pytest
import asyncio
import time
from unittest.mock import AsyncMock, patch
from testcontainers.compose import DockerCompose
import structlog

from marketprism_collector.main import CollectorApp
from marketprism_collector.exchanges.binance import BinanceCollector
from marketprism_collector.exchanges.okx import OKXCollector

class TestPythonCollectorIntegration:
    """Python-Collectorå®Œæ•´é›†æˆæµ‹è¯•"""
    
    @pytest.fixture(scope="class")
    async def docker_services(self):
        """å¯åŠ¨æµ‹è¯•ç¯å¢ƒæœåŠ¡"""
        with DockerCompose("tests/docker", compose_file_name="test-compose.yml") as compose:
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            compose.wait_for("http://localhost:4222/healthz")  # NATS
            compose.wait_for("http://localhost:8123/ping")     # ClickHouse
            yield compose
    
    @pytest.fixture
    async def collector_app(self, docker_services):
        """åˆ›å»ºæ”¶é›†å™¨åº”ç”¨å®ä¾‹"""
        app = CollectorApp()
        await app.initialize()
        yield app
        await app.shutdown()
    
    @pytest.mark.asyncio
    async def test_full_data_flow(self, collector_app):
        """æµ‹è¯•å®Œæ•´æ•°æ®æµï¼šæ¥æ”¶ â†’ å¤„ç† â†’ å‘å¸ƒ"""
        # æ¨¡æ‹Ÿäº¤æ˜“æ‰€æ•°æ®
        mock_trade_data = {
            "stream": "btcusdt@trade",
            "data": {
                "e": "trade",
                "E": int(time.time() * 1000),
                "s": "BTCUSDT",
                "t": 12345,
                "p": "50000.00",
                "q": "0.001",
                "b": 88,
                "a": 50,
                "T": int(time.time() * 1000),
                "m": True
            }
        }
        
        # æ³¨å…¥æµ‹è¯•æ•°æ®
        binance_collector = collector_app.exchanges['binance']
        await binance_collector._handle_message(mock_trade_data)
        
        # éªŒè¯æ•°æ®å¤„ç†
        await asyncio.sleep(0.1)  # ç­‰å¾…å¼‚æ­¥å¤„ç†
        
        # æ£€æŸ¥æŒ‡æ ‡æ›´æ–°
        metrics = collector_app.metrics
        assert metrics.collector_messages_total._value._value > 0
        
        # æ£€æŸ¥NATSå‘å¸ƒ
        nats_client = collector_app.nats_client
        assert nats_client.publish_count > 0
    
    @pytest.mark.asyncio
    async def test_multi_exchange_coordination(self, collector_app):
        """æµ‹è¯•å¤šäº¤æ˜“æ‰€åè°ƒ"""
        exchanges = ['binance', 'okx']
        
        # å¯åŠ¨æ‰€æœ‰äº¤æ˜“æ‰€æ”¶é›†å™¨
        tasks = []
        for exchange in exchanges:
            collector = collector_app.exchanges[exchange]
            task = asyncio.create_task(collector.start())
            tasks.append(task)
        
        # ç­‰å¾…è¿æ¥å»ºç«‹
        await asyncio.sleep(2)
        
        # éªŒè¯è¿æ¥çŠ¶æ€
        for exchange in exchanges:
            collector = collector_app.exchanges[exchange]
            assert collector.is_connected()
        
        # æ¸…ç†
        for task in tasks:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass
    
    @pytest.mark.asyncio
    async def test_error_recovery(self, collector_app):
        """æµ‹è¯•é”™è¯¯æ¢å¤æœºåˆ¶"""
        binance_collector = collector_app.exchanges['binance']
        
        # æ¨¡æ‹Ÿè¿æ¥æ–­å¼€
        await binance_collector.disconnect()
        assert not binance_collector.is_connected()
        
        # è§¦å‘é‡è¿
        await binance_collector.reconnect()
        
        # éªŒè¯æ¢å¤
        await asyncio.sleep(1)
        assert binance_collector.is_connected()
    
    @pytest.mark.asyncio
    async def test_rate_limiting(self, collector_app):
        """æµ‹è¯•é€Ÿç‡é™åˆ¶"""
        binance_collector = collector_app.exchanges['binance']
        
        # å¿«é€Ÿå‘é€å¤§é‡è¯·æ±‚
        start_time = time.time()
        for i in range(100):
            await binance_collector._make_request("/api/v3/time")
        end_time = time.time()
        
        # éªŒè¯é€Ÿç‡é™åˆ¶ç”Ÿæ•ˆ
        duration = end_time - start_time
        assert duration >= 1.0  # åº”è¯¥è¢«é™åˆ¶
    
    @pytest.mark.asyncio
    async def test_memory_usage_stability(self, collector_app):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨ç¨³å®šæ€§"""
        import psutil
        process = psutil.Process()
        
        initial_memory = process.memory_info().rss
        
        # è¿è¡Œä¸€æ®µæ—¶é—´çš„æ•°æ®å¤„ç†
        for i in range(1000):
            mock_data = {
                "stream": f"symbol{i % 10}@trade",
                "data": {"price": f"{50000 + i}", "quantity": "0.001"}
            }
            await collector_app.exchanges['binance']._handle_message(mock_data)
            
            if i % 100 == 0:
                await asyncio.sleep(0.01)  # è®©GCæœ‰æœºä¼šè¿è¡Œ
        
        final_memory = process.memory_info().rss
        memory_growth = (final_memory - initial_memory) / 1024 / 1024  # MB
        
        # å†…å­˜å¢é•¿åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert memory_growth < 50  # ä¸è¶…è¿‡50MB
```

### 2. ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•
```python
# tests/integration/test_monitoring_system.py
import pytest
import requests
import time
from prometheus_client.parser import text_string_to_metric_families

class TestMonitoringIntegration:
    """ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def prometheus_url(self):
        return "http://localhost:9090"
    
    @pytest.fixture
    def grafana_url(self):
        return "http://localhost:3000"
    
    def test_prometheus_metrics_collection(self, prometheus_url):
        """æµ‹è¯•PrometheusæŒ‡æ ‡æ”¶é›†"""
        # æŸ¥è¯¢MarketPrismæŒ‡æ ‡
        query = "marketprism_collector_messages_total"
        response = requests.get(
            f"{prometheus_url}/api/v1/query",
            params={"query": query}
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "success"
        assert len(data["data"]["result"]) > 0
    
    def test_alert_rules_validation(self, prometheus_url):
        """æµ‹è¯•å‘Šè­¦è§„åˆ™éªŒè¯"""
        response = requests.get(f"{prometheus_url}/api/v1/rules")
        
        assert response.status_code == 200
        data = response.json()
        
        # éªŒè¯å‘Šè­¦è§„åˆ™å­˜åœ¨
        rule_names = []
        for group in data["data"]["groups"]:
            for rule in group["rules"]:
                if rule["type"] == "alerting":
                    rule_names.append(rule["name"])
        
        expected_alerts = [
            "MarketPrismSystemDown",
            "MarketPrismHighErrorRate",
            "MarketPrismHighMemoryUsage"
        ]
        
        for alert in expected_alerts:
            assert alert in rule_names
    
    def test_grafana_dashboard_accessibility(self, grafana_url):
        """æµ‹è¯•Grafanaä»ªè¡¨æ¿å¯è®¿é—®æ€§"""
        # æµ‹è¯•ç³»ç»Ÿæ¦‚è§ˆä»ªè¡¨æ¿
        response = requests.get(
            f"{grafana_url}/api/dashboards/uid/marketprism-overview",
            auth=("admin", "admin")
        )
        
        assert response.status_code == 200
        dashboard = response.json()
        assert dashboard["dashboard"]["title"] == "MarketPrism - ç³»ç»Ÿæ¦‚è§ˆ"
    
    def test_metrics_accuracy(self):
        """æµ‹è¯•æŒ‡æ ‡å‡†ç¡®æ€§"""
        from marketprism_collector.monitoring.metrics import CollectorMetrics
        
        metrics = CollectorMetrics()
        
        # è®°å½•ä¸€äº›æŒ‡æ ‡
        metrics.record_message_processed("binance", "trade", "success")
        metrics.record_message_processed("binance", "trade", "success")
        metrics.record_message_processed("binance", "trade", "error")
        
        # éªŒè¯è®¡æ•°å™¨
        success_count = metrics.collector_messages_total._value._value
        error_count = metrics.collector_errors_total._value._value
        
        assert success_count >= 2
        assert error_count >= 1
```

## âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. ååé‡åŸºå‡†æµ‹è¯•
```python
# tests/performance/test_throughput_benchmarks.py
import pytest
import asyncio
import time
import statistics
from typing import List
import structlog

class TestThroughputBenchmarks:
    """ååé‡åŸºå‡†æµ‹è¯•"""
    
    @pytest.mark.benchmark
    async def test_message_processing_throughput(self):
        """æµ‹è¯•æ¶ˆæ¯å¤„ç†ååé‡"""
        from marketprism_collector.main import CollectorApp
        
        app = CollectorApp()
        await app.initialize()
        
        try:
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_messages = []
            for i in range(10000):
                test_messages.append({
                    "stream": "btcusdt@trade",
                    "data": {
                        "p": f"{50000 + i % 1000}",
                        "q": "0.001",
                        "T": int(time.time() * 1000)
                    }
                })
            
            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            start_time = time.time()
            
            for message in test_messages:
                await app.exchanges['binance']._handle_message(message)
            
            end_time = time.time()
            
            # è®¡ç®—ååé‡
            duration = end_time - start_time
            throughput = len(test_messages) / duration
            
            # è®°å½•åŸºå‡†ç»“æœ
            logger = structlog.get_logger(__name__)
            logger.info(
                "ååé‡åŸºå‡†æµ‹è¯•å®Œæˆ",
                messages_count=len(test_messages),
                duration_seconds=duration,
                throughput_msg_per_sec=throughput
            )
            
            # éªŒè¯æ€§èƒ½ç›®æ ‡
            assert throughput >= 80  # ç›®æ ‡ï¼š80 msg/s
            
        finally:
            await app.shutdown()
    
    @pytest.mark.benchmark
    async def test_concurrent_processing_throughput(self):
        """æµ‹è¯•å¹¶å‘å¤„ç†ååé‡"""
        from marketprism_collector.main import CollectorApp
        
        app = CollectorApp()
        await app.initialize()
        
        try:
            # å¹¶å‘å¤„ç†æµ‹è¯•
            async def process_batch(batch_id: int, message_count: int):
                for i in range(message_count):
                    message = {
                        "stream": f"symbol{batch_id}@trade",
                        "data": {
                            "p": f"{50000 + i}",
                            "q": "0.001",
                            "T": int(time.time() * 1000)
                        }
                    }
                    await app.exchanges['binance']._handle_message(message)
            
            # å¯åŠ¨å¤šä¸ªå¹¶å‘æ‰¹æ¬¡
            start_time = time.time()
            
            tasks = []
            batch_count = 10
            messages_per_batch = 1000
            
            for batch_id in range(batch_count):
                task = asyncio.create_task(
                    process_batch(batch_id, messages_per_batch)
                )
                tasks.append(task)
            
            await asyncio.gather(*tasks)
            
            end_time = time.time()
            
            # è®¡ç®—å¹¶å‘ååé‡
            total_messages = batch_count * messages_per_batch
            duration = end_time - start_time
            concurrent_throughput = total_messages / duration
            
            logger = structlog.get_logger(__name__)
            logger.info(
                "å¹¶å‘ååé‡åŸºå‡†æµ‹è¯•å®Œæˆ",
                total_messages=total_messages,
                batch_count=batch_count,
                duration_seconds=duration,
                concurrent_throughput_msg_per_sec=concurrent_throughput
            )
            
            # éªŒè¯å¹¶å‘æ€§èƒ½
            assert concurrent_throughput >= 200  # ç›®æ ‡ï¼š200 msg/s
            
        finally:
            await app.shutdown()
```

### 2. å»¶è¿ŸåŸºå‡†æµ‹è¯•
```python
# tests/performance/test_latency_benchmarks.py
import pytest
import asyncio
import time
import statistics
from typing import List

class TestLatencyBenchmarks:
    """å»¶è¿ŸåŸºå‡†æµ‹è¯•"""
    
    @pytest.mark.benchmark
    async def test_end_to_end_latency(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯å»¶è¿Ÿ"""
        from marketprism_collector.main import CollectorApp
        
        app = CollectorApp()
        await app.initialize()
        
        try:
            latencies: List[float] = []
            
            # æ‰§è¡Œå¤šæ¬¡å»¶è¿Ÿæµ‹è¯•
            for i in range(1000):
                start_time = time.time()
                
                message = {
                    "stream": "btcusdt@trade",
                    "data": {
                        "p": f"{50000 + i}",
                        "q": "0.001",
                        "T": int(time.time() * 1000)
                    }
                }
                
                await app.exchanges['binance']._handle_message(message)
                
                end_time = time.time()
                latency = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                latencies.append(latency)
                
                # é¿å…è¿‡å¿«æ‰§è¡Œ
                if i % 100 == 0:
                    await asyncio.sleep(0.01)
            
            # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
            avg_latency = statistics.mean(latencies)
            p50_latency = statistics.median(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
            p99_latency = statistics.quantiles(latencies, n=100)[98]  # 99th percentile
            
            logger = structlog.get_logger(__name__)
            logger.info(
                "å»¶è¿ŸåŸºå‡†æµ‹è¯•å®Œæˆ",
                sample_count=len(latencies),
                avg_latency_ms=avg_latency,
                p50_latency_ms=p50_latency,
                p95_latency_ms=p95_latency,
                p99_latency_ms=p99_latency
            )
            
            # éªŒè¯å»¶è¿Ÿç›®æ ‡
            assert p95_latency <= 100  # P95 < 100ms
            assert p99_latency <= 500  # P99 < 500ms
            
        finally:
            await app.shutdown()
    
    @pytest.mark.benchmark
    async def test_websocket_connection_latency(self):
        """æµ‹è¯•WebSocketè¿æ¥å»¶è¿Ÿ"""
        from marketprism_collector.exchanges.binance import BinanceCollector
        
        connection_times: List[float] = []
        
        # æµ‹è¯•å¤šæ¬¡è¿æ¥
        for i in range(10):
            collector = BinanceCollector()
            
            start_time = time.time()
            await collector.connect()
            end_time = time.time()
            
            connection_time = (end_time - start_time) * 1000
            connection_times.append(connection_time)
            
            await collector.disconnect()
            await asyncio.sleep(0.1)  # é¿å…è¿æ¥è¿‡å¿«
        
        # è®¡ç®—è¿æ¥å»¶è¿Ÿç»Ÿè®¡
        avg_connection_time = statistics.mean(connection_times)
        max_connection_time = max(connection_times)
        
        logger = structlog.get_logger(__name__)
        logger.info(
            "WebSocketè¿æ¥å»¶è¿ŸåŸºå‡†æµ‹è¯•å®Œæˆ",
            sample_count=len(connection_times),
            avg_connection_time_ms=avg_connection_time,
            max_connection_time_ms=max_connection_time
        )
        
        # éªŒè¯è¿æ¥å»¶è¿Ÿ
        assert avg_connection_time <= 1000  # å¹³å‡è¿æ¥æ—¶é—´ < 1ç§’
        assert max_connection_time <= 3000  # æœ€å¤§è¿æ¥æ—¶é—´ < 3ç§’
```

## ğŸ›¡ï¸ æ•…éšœæ¢å¤æµ‹è¯•

### 1. ç½‘ç»œæ•…éšœæ¢å¤æµ‹è¯•
```python
# tests/resilience/test_network_failure_recovery.py
import pytest
import asyncio
import time
from unittest.mock import patch, AsyncMock

class TestNetworkFailureRecovery:
    """ç½‘ç»œæ•…éšœæ¢å¤æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_websocket_reconnection(self):
        """æµ‹è¯•WebSocketé‡è¿æœºåˆ¶"""
        from marketprism_collector.exchanges.binance import BinanceCollector
        
        collector = BinanceCollector()
        await collector.connect()
        
        # éªŒè¯åˆå§‹è¿æ¥
        assert collector.is_connected()
        
        # æ¨¡æ‹Ÿç½‘ç»œæ–­å¼€
        await collector._websocket.close()
        
        # ç­‰å¾…é‡è¿æœºåˆ¶è§¦å‘
        await asyncio.sleep(5)
        
        # éªŒè¯è‡ªåŠ¨é‡è¿
        assert collector.is_connected()
        
        await collector.disconnect()
    
    @pytest.mark.asyncio
    async def test_api_request_retry(self):
        """æµ‹è¯•APIè¯·æ±‚é‡è¯•æœºåˆ¶"""
        from marketprism_collector.exchanges.binance import BinanceCollector
        
        collector = BinanceCollector()
        
        # æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯
        with patch('aiohttp.ClientSession.get') as mock_get:
            # å‰ä¸¤æ¬¡è¯·æ±‚å¤±è´¥ï¼Œç¬¬ä¸‰æ¬¡æˆåŠŸ
            mock_get.side_effect = [
                Exception("Network error"),
                Exception("Timeout"),
                AsyncMock(status=200, json=AsyncMock(return_value={"serverTime": 1234567890}))
            ]
            
            # æ‰§è¡Œè¯·æ±‚
            result = await collector._make_request("/api/v3/time")
            
            # éªŒè¯é‡è¯•æœºåˆ¶
            assert mock_get.call_count == 3
            assert result is not None
    
    @pytest.mark.asyncio
    async def test_partial_service_failure(self):
        """æµ‹è¯•éƒ¨åˆ†æœåŠ¡æ•…éšœå¤„ç†"""
        from marketprism_collector.main import CollectorApp
        
        app = CollectorApp()
        await app.initialize()
        
        try:
            # æ¨¡æ‹ŸNATSæœåŠ¡æ•…éšœ
            with patch.object(app.nats_client, 'publish') as mock_publish:
                mock_publish.side_effect = Exception("NATS connection failed")
                
                # å‘é€æ¶ˆæ¯
                message = {
                    "stream": "btcusdt@trade",
                    "data": {"p": "50000", "q": "0.001"}
                }
                
                # åº”è¯¥èƒ½å¤Ÿå¤„ç†æ¶ˆæ¯ï¼Œå³ä½¿NATSå‘å¸ƒå¤±è´¥
                await app.exchanges['binance']._handle_message(message)
                
                # éªŒè¯é”™è¯¯è¢«æ­£ç¡®å¤„ç†
                assert app.metrics.collector_errors_total._value._value > 0
                
        finally:
            await app.shutdown()
```

### 2. èµ„æºè€—å°½æ¢å¤æµ‹è¯•
```python
# tests/resilience/test_resource_exhaustion.py
import pytest
import asyncio
import gc
from unittest.mock import patch

class TestResourceExhaustionRecovery:
    """èµ„æºè€—å°½æ¢å¤æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_memory_pressure_handling(self):
        """æµ‹è¯•å†…å­˜å‹åŠ›å¤„ç†"""
        from marketprism_collector.main import CollectorApp
        
        app = CollectorApp()
        await app.initialize()
        
        try:
            # æ¨¡æ‹Ÿå†…å­˜å‹åŠ›
            large_objects = []
            
            # åˆ›å»ºå¤§é‡å¯¹è±¡
            for i in range(10000):
                large_objects.append([0] * 1000)  # æ¯ä¸ªå¯¹è±¡çº¦4KB
                
                # æ¯1000ä¸ªå¯¹è±¡å¤„ç†ä¸€æ¬¡æ¶ˆæ¯
                if i % 1000 == 0:
                    message = {
                        "stream": "btcusdt@trade",
                        "data": {"p": f"{50000 + i}", "q": "0.001"}
                    }
                    await app.exchanges['binance']._handle_message(message)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            del large_objects
            gc.collect()
            
            # éªŒè¯ç³»ç»Ÿä»ç„¶æ­£å¸¸å·¥ä½œ
            message = {
                "stream": "btcusdt@trade",
                "data": {"p": "50000", "q": "0.001"}
            }
            await app.exchanges['binance']._handle_message(message)
            
            # éªŒè¯æŒ‡æ ‡æ­£å¸¸
            assert app.metrics.collector_messages_total._value._value > 0
            
        finally:
            await app.shutdown()
    
    @pytest.mark.asyncio
    async def test_connection_pool_exhaustion(self):
        """æµ‹è¯•è¿æ¥æ± è€—å°½å¤„ç†"""
        from marketprism_collector.connections.websocket_pool import WebSocketPool
        
        pool = WebSocketPool(max_connections_per_exchange=2)
        
        # è€—å°½è¿æ¥æ± 
        conn1 = await pool.get_connection("test", "ws://echo.websocket.org")
        conn2 = await pool.get_connection("test", "ws://echo.websocket.org")
        
        # å°è¯•è·å–ç¬¬ä¸‰ä¸ªè¿æ¥åº”è¯¥å¤±è´¥
        with pytest.raises(Exception, match="è¾¾åˆ°.*æœ€å¤§è¿æ¥æ•°é™åˆ¶"):
            await pool.get_connection("test", "ws://echo.websocket.org")
        
        # é‡Šæ”¾ä¸€ä¸ªè¿æ¥
        await pool.release_connection("test", conn1)
        
        # ç°åœ¨åº”è¯¥èƒ½å¤Ÿè·å–æ–°è¿æ¥
        conn3 = await pool.get_connection("test", "ws://echo.websocket.org")
        assert conn3 is not None
        
        # æ¸…ç†
        await pool.close_all()
```

## ğŸ“ˆ æµ‹è¯•è‡ªåŠ¨åŒ–å’ŒCI/CDé›†æˆ

### 1. GitHub Actionsæµ‹è¯•å·¥ä½œæµ
```yaml
# .github/workflows/comprehensive-tests.yml
name: ç»¼åˆæµ‹è¯•å¥—ä»¶

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Pythonç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: è¿è¡Œå•å…ƒæµ‹è¯•
      run: |
        pytest tests/unit/ \
          --cov=services/python-collector/src \
          --cov-report=xml \
          --cov-report=html \
          --cov-fail-under=90
    
    - name: ä¸Šä¼ è¦†ç›–ç‡æŠ¥å‘Š
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      nats:
        image: nats:latest
        ports:
          - 4222:4222
      
      clickhouse:
        image: clickhouse/clickhouse-server:latest
        ports:
          - 8123:8123
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Pythonç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: ç­‰å¾…æœåŠ¡å¯åŠ¨
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:4222/healthz; do sleep 2; done'
        timeout 60 bash -c 'until curl -f http://localhost:8123/ping; do sleep 2; done'
    
    - name: è¿è¡Œé›†æˆæµ‹è¯•
      run: |
        pytest tests/integration/ \
          --cov=services/python-collector/src \
          --cov-append \
          --cov-report=xml

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Pythonç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
      run: |
        pytest tests/performance/ \
          -m benchmark \
          --benchmark-json=benchmark.json
    
    - name: ä¸Šä¼ åŸºå‡†æµ‹è¯•ç»“æœ
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json

  resilience-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: è®¾ç½®Pythonç¯å¢ƒ
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: å®‰è£…ä¾èµ–
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: è¿è¡Œæ•…éšœæ¢å¤æµ‹è¯•
      run: |
        pytest tests/resilience/ \
          --timeout=300 \
          --maxfail=1
```

### 2. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
```python
# tests/utils/test_reporter.py
import json
import time
from typing import Dict, List
from pathlib import Path

class TestReporter:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.results = {
            'timestamp': time.time(),
            'summary': {},
            'unit_tests': {},
            'integration_tests': {},
            'performance_tests': {},
            'resilience_tests': {}
        }
    
    def add_unit_test_results(self, coverage_data: Dict):
        """æ·»åŠ å•å…ƒæµ‹è¯•ç»“æœ"""
        self.results['unit_tests'] = {
            'coverage_percentage': coverage_data.get('coverage', 0),
            'lines_covered': coverage_data.get('lines_covered', 0),
            'lines_total': coverage_data.get('lines_total', 0),
            'missing_lines': coverage_data.get('missing_lines', [])
        }
    
    def add_performance_results(self, benchmark_data: Dict):
        """æ·»åŠ æ€§èƒ½æµ‹è¯•ç»“æœ"""
        self.results['performance_tests'] = {
            'throughput_msg_per_sec': benchmark_data.get('throughput', 0),
            'avg_latency_ms': benchmark_data.get('avg_latency', 0),
            'p95_latency_ms': benchmark_data.get('p95_latency', 0),
            'p99_latency_ms': benchmark_data.get('p99_latency', 0)
        }
    
    def generate_report(self, output_path: str = "test_report.json"):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        # è®¡ç®—æ€»ä½“æ‘˜è¦
        self.results['summary'] = {
            'total_coverage': self._calculate_total_coverage(),
            'performance_score': self._calculate_performance_score(),
            'resilience_score': self._calculate_resilience_score(),
            'overall_score': self._calculate_overall_score()
        }
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        return self.results
    
    def _calculate_total_coverage(self) -> float:
        """è®¡ç®—æ€»ä½“è¦†ç›–ç‡"""
        unit_coverage = self.results['unit_tests'].get('coverage_percentage', 0)
        # å¯ä»¥åŠ å…¥é›†æˆæµ‹è¯•è¦†ç›–ç‡ç­‰
        return unit_coverage
    
    def _calculate_performance_score(self) -> float:
        """è®¡ç®—æ€§èƒ½å¾—åˆ†"""
        perf = self.results['performance_tests']
        throughput = perf.get('throughput_msg_per_sec', 0)
        latency = perf.get('p95_latency_ms', 1000)
        
        # åŸºäºç›®æ ‡è®¡ç®—å¾—åˆ†
        throughput_score = min(throughput / 80, 1.0) * 50  # ç›®æ ‡80 msg/s
        latency_score = max(0, (200 - latency) / 200) * 50  # ç›®æ ‡<100ms
        
        return throughput_score + latency_score
    
    def _calculate_resilience_score(self) -> float:
        """è®¡ç®—å¼¹æ€§å¾—åˆ†"""
        # åŸºäºæ•…éšœæ¢å¤æµ‹è¯•é€šè¿‡ç‡
        return 85.0  # å ä½ç¬¦
    
    def _calculate_overall_score(self) -> float:
        """è®¡ç®—æ€»ä½“å¾—åˆ†"""
        coverage_weight = 0.4
        performance_weight = 0.3
        resilience_weight = 0.3
        
        coverage_score = self._calculate_total_coverage()
        performance_score = self._calculate_performance_score()
        resilience_score = self._calculate_resilience_score()
        
        return (coverage_score * coverage_weight + 
                performance_score * performance_weight + 
                resilience_score * resilience_weight)
```

## ğŸ¯ å®æ–½è®¡åˆ’

### ç¬¬ä¸€å‘¨ï¼šé›†æˆæµ‹è¯•å®Œå–„
- [ ] å®æ–½Python-Collectorå®Œæ•´é›†æˆæµ‹è¯•
- [ ] æ·»åŠ ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•
- [ ] åˆ›å»ºå¤šæœåŠ¡åè°ƒæµ‹è¯•
- [ ] å»ºç«‹æµ‹è¯•æ•°æ®ç®¡ç†

### ç¬¬äºŒå‘¨ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] å®æ–½ååé‡åŸºå‡†æµ‹è¯•
- [ ] æ·»åŠ å»¶è¿ŸåŸºå‡†æµ‹è¯•
- [ ] åˆ›å»ºå¹¶å‘æ€§èƒ½æµ‹è¯•
- [ ] å»ºç«‹æ€§èƒ½å›å½’æ£€æµ‹

### ç¬¬ä¸‰å‘¨ï¼šæ•…éšœæ¢å¤æµ‹è¯•
- [ ] å®æ–½ç½‘ç»œæ•…éšœæ¢å¤æµ‹è¯•
- [ ] æ·»åŠ èµ„æºè€—å°½æ¢å¤æµ‹è¯•
- [ ] åˆ›å»ºéƒ¨åˆ†æœåŠ¡æ•…éšœæµ‹è¯•
- [ ] å»ºç«‹æ•…éšœæ³¨å…¥æ¡†æ¶

### ç¬¬å››å‘¨ï¼šæµ‹è¯•è‡ªåŠ¨åŒ–
- [ ] é…ç½®CI/CDæµ‹è¯•æµæ°´çº¿
- [ ] å®æ–½æµ‹è¯•æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
- [ ] æ·»åŠ æµ‹è¯•è¦†ç›–ç‡ç›‘æ§
- [ ] å»ºç«‹æµ‹è¯•è´¨é‡é—¨ç¦

## ğŸ“Š é¢„æœŸæµ‹è¯•è¦†ç›–ç‡æå‡æ•ˆæœ

### è¦†ç›–ç‡æå‡
- **å•å…ƒæµ‹è¯•**: 60% â†’ 90%+ (+50%)
- **é›†æˆæµ‹è¯•**: 30% â†’ 85%+ (+183%)
- **ç«¯åˆ°ç«¯æµ‹è¯•**: 10% â†’ 70%+ (+600%)
- **æ€»ä½“è¦†ç›–ç‡**: 45% â†’ 85%+ (+89%)

### è´¨é‡ä¿éšœ
- **ç¼ºé™·æ£€å‡ºç‡**: æå‡70%
- **å›å½’æµ‹è¯•æ•ˆç‡**: æå‡80%
- **å‘å¸ƒä¿¡å¿ƒåº¦**: æå‡90%
- **æ•…éšœæ¢å¤æ—¶é—´**: å‡å°‘60%

---

## âœ… æµ‹è¯•è¦†ç›–ç‡æå‡æ–¹æ¡ˆçŠ¶æ€: **å·²åˆ¶å®šå®Œæˆ**

**åˆ¶å®šæ—¶é—´**: 2025-05-24  
**è¦†ç›–èŒƒå›´**: âœ… å…¨é¢  
**å¯æ‰§è¡Œæ€§**: âœ… é«˜  
**é¢„æœŸæ•ˆæœ**: âœ… æ˜¾è‘—  

æµ‹è¯•è¦†ç›–ç‡æå‡æ–¹æ¡ˆå·²åˆ¶å®šå®Œæˆï¼Œä¸ºä¼ä¸šçº§è´¨é‡ä¿éšœæä¾›äº†å®Œæ•´çš„æµ‹è¯•ä½“ç³»ã€‚ 