#!/usr/bin/env python3
"""
MarketPrism ç³»ç»Ÿæ·±åº¦éªŒè¯è„šæœ¬
æ‰§è¡Œå…¨é¢çš„é…ç½®ä¸€è‡´æ€§ã€æ•°æ®è´¨é‡ã€æ€§èƒ½å’Œç¨³å®šæ€§æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    source venv/bin/activate
    python scripts/comprehensive_system_validation.py [--duration 30] [--stress-test]

åŠŸèƒ½æ¨¡å—:
1. é…ç½®ä¸€è‡´æ€§æ·±åº¦éªŒè¯
2. æ•°æ®è´¨é‡æ·±åº¦åˆ†æ  
3. ç³»ç»Ÿå‹åŠ›å’Œç¨³å®šæ€§æµ‹è¯•
4. é”™è¯¯å¤„ç†å’Œç›‘æ§éªŒè¯
5. æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import os
import sys
import json
import time
import argparse
import threading
import subprocess
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import psutil

# é…ç½®å¸¸é‡
CH_URL = os.getenv("CLICKHOUSE_HTTP", "http://localhost:8123")
DB = os.getenv("CLICKHOUSE_DB", "marketprism_hot")
NATS_URL = os.getenv("MARKETPRISM_NATS_URL", os.getenv("NATS_URL", "nats://localhost:4222"))
STORAGE_HEALTH = "http://localhost:18080/health"
COLLECTOR_HEALTH = "http://localhost:8086/health"
NATS_MONITORING = "http://localhost:8222"

# æ•°æ®è¡¨é…ç½®
HIGH_FREQ_TABLES = ["trades", "orderbooks"]
LOW_FREQ_TABLES = ["funding_rates", "open_interests", "liquidations", 
                   "lsr_top_positions", "lsr_all_accounts", "volatility_indices"]
ALL_TABLES = HIGH_FREQ_TABLES + LOW_FREQ_TABLES

# äº¤æ˜“æ‰€å’Œäº¤æ˜“å¯¹é…ç½®
EXPECTED_EXCHANGES = ["binance_spot", "binance_derivatives", "okx_spot", "okx_derivatives", "deribit"]
EXPECTED_SYMBOLS = ["BTC-USDT", "ETH-USDT", "BTC-USD", "ETH-USD"]

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœæ•°æ®ç»“æ„"""
    module: str
    test_name: str
    status: str  # PASS, FAIL, WARNING
    details: Dict[str, Any]
    timestamp: str
    duration_ms: int
    
@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_sent_mb: float
    network_recv_mb: float
    
class ValidationRunner:
    """éªŒè¯æ‰§è¡Œå™¨"""
    
    def __init__(self, duration_minutes: int = 30, stress_test: bool = False):
        self.duration_minutes = duration_minutes
        self.stress_test = stress_test
        self.results: List[ValidationResult] = []
        self.metrics: List[SystemMetrics] = []
        self.start_time = datetime.now(timezone.utc)
        
    def log(self, message: str, level: str = "INFO"):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        timestamp = datetime.now(timezone.utc).strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def add_result(self, module: str, test_name: str, status: str, details: Dict[str, Any], duration_ms: int = 0):
        """æ·»åŠ éªŒè¯ç»“æœ"""
        result = ValidationResult(
            module=module,
            test_name=test_name,
            status=status,
            details=details,
            timestamp=datetime.now(timezone.utc).isoformat(),
            duration_ms=duration_ms
        )
        self.results.append(result)
        
        # å®æ—¶è¾“å‡ºç»“æœ
        status_icon = {"PASS": "âœ…", "FAIL": "âŒ", "WARNING": "âš ï¸"}.get(status, "â“")
        self.log(f"{status_icon} {module}.{test_name}: {status}")
        if details.get("summary"):
            self.log(f"   {details['summary']}")
            
    def ch_query(self, sql: str, timeout: int = 30) -> Tuple[int, str]:
        """ClickHouseæŸ¥è¯¢"""
        try:
            resp = requests.post(f"{CH_URL}/?database={DB}", 
                               data=sql.encode("utf-8"), timeout=timeout)
            return resp.status_code, resp.text.strip()
        except Exception as e:
            return 599, str(e)
            
    def http_get(self, url: str, timeout: int = 10) -> Tuple[int, Dict]:
        """HTTP GETè¯·æ±‚"""
        try:
            resp = requests.get(url, timeout=timeout)
            if resp.headers.get('content-type', '').startswith('application/json'):
                return resp.status_code, resp.json()
            return resp.status_code, {"text": resp.text}
        except Exception as e:
            return 599, {"error": str(e)}
            
    def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        try:
            cpu = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk_io = psutil.disk_io_counters()
            net_io = psutil.net_io_counters()
            
            metrics = SystemMetrics(
                timestamp=datetime.now(timezone.utc).isoformat(),
                cpu_percent=cpu,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / 1024 / 1024,
                disk_io_read_mb=disk_io.read_bytes / 1024 / 1024 if disk_io else 0,
                disk_io_write_mb=disk_io.write_bytes / 1024 / 1024 if disk_io else 0,
                network_sent_mb=net_io.bytes_sent / 1024 / 1024 if net_io else 0,
                network_recv_mb=net_io.bytes_recv / 1024 / 1024 if net_io else 0
            )
            self.metrics.append(metrics)
        except Exception as e:
            self.log(f"Failed to collect system metrics: {e}", "WARNING")
            
    def validate_config_consistency(self):
        """1. é…ç½®ä¸€è‡´æ€§æ·±åº¦éªŒè¯"""
        self.log("å¼€å§‹é…ç½®ä¸€è‡´æ€§éªŒè¯...")
        start_time = time.time()
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®æ–‡ä»¶
        env_file = "services/message-broker/.env.docker"
        config_details = {}
        
        try:
            with open(env_file, 'r') as f:
                content = f.read()
                
            # æå–LSRé…ç½®
            lsr_config = {}
            for line in content.split('\n'):
                if line.startswith('LSR_') and '=' in line:
                    key, value = line.split('=', 1)
                    lsr_config[key] = value
                    
            config_details["env_file_lsr"] = lsr_config
            
            # æ£€æŸ¥è¿è¡Œæ—¶é…ç½®ä¸€è‡´æ€§
            code, storage_health = self.http_get(STORAGE_HEALTH)
            if code == 200 and isinstance(storage_health, dict):
                config_details["storage_runtime"] = {
                    "nats_connected": storage_health.get("nats_connected"),
                    "subscriptions": storage_health.get("subscriptions")
                }
                
            # æ£€æŸ¥NATS JetStreamé…ç½®ï¼ˆä½¿ç”¨è¯¦ç»†æ¨¡å¼ï¼Œé¿å… /jsz è¿”å› streams ä¸ºæ•´æ•°è®¡æ•°å¯¼è‡´ä¸å¯è¿­ä»£ï¼‰
            code, nats_info = self.http_get(f"{NATS_MONITORING}/jsz?streams=true&config=true")
            if code == 200 and isinstance(nats_info, dict):
                streams = nats_info.get("streams", [])
                if not isinstance(streams, list):
                    streams = []
                config_details["jetstream_streams"] = {
                    stream.get("config", {}).get("name"): {
                        "subjects": stream.get("config", {}).get("subjects", []),
                        "max_msgs": stream.get("config", {}).get("max_msgs"),
                        "max_bytes": stream.get("config", {}).get("max_bytes"),
                        "max_age": stream.get("config", {}).get("max_age")
                    } for stream in streams
                }
                
            status = "PASS" if config_details else "FAIL"
            summary = f"æ£€æŸ¥äº†{len(config_details)}ä¸ªé…ç½®æº"
            
        except Exception as e:
            status = "FAIL"
            summary = f"é…ç½®æ£€æŸ¥å¤±è´¥: {e}"
            config_details = {"error": str(e)}
            
        duration_ms = int((time.time() - start_time) * 1000)
        self.add_result("config", "consistency_check", status,
                       {**config_details, "summary": summary}, duration_ms)

    def validate_data_quality(self):
        """2. æ•°æ®è´¨é‡æ·±åº¦åˆ†æ"""
        self.log("å¼€å§‹æ•°æ®è´¨é‡åˆ†æ...")

        for table in ALL_TABLES:
            start_time = time.time()
            details = {"table": table}

            try:
                # åŸºç¡€ç»Ÿè®¡
                code, count_str = self.ch_query(f"SELECT count() FROM {table}")
                if code == 200:
                    total_count = int(count_str)
                    details["total_count"] = total_count
                else:
                    raise Exception(f"Count query failed: {count_str}")

                if total_count == 0:
                    status = "WARNING"
                    summary = "è¡¨ä¸ºç©º"
                else:
                    # æ•°æ®è´¨é‡æ£€æŸ¥
                    quality_checks = []

                    # 1. ä»·æ ¼/æ•°é‡åˆç†æ€§æ£€æŸ¥ï¼ˆé’ˆå¯¹tradesè¡¨ï¼‰
                    if table == "trades":
                        code, result = self.ch_query(f"""
                            SELECT
                                countIf(price <= 0) as invalid_price,
                                countIf(amount <= 0) as invalid_amount,
                                countIf(price > 1000000) as extreme_price,
                                min(price) as min_price,
                                max(price) as max_price,
                                avg(price) as avg_price
                            FROM {table}
                            WHERE timestamp > now() - INTERVAL 1 HOUR
                        """)
                        if code == 200:
                            values = result.split('\t')
                            if len(values) >= 6:
                                invalid_price, invalid_amount, extreme_price = map(int, values[:3])
                                min_price, max_price, avg_price = map(float, values[3:6])

                                details["price_quality"] = {
                                    "invalid_price_count": invalid_price,
                                    "invalid_amount_count": invalid_amount,
                                    "extreme_price_count": extreme_price,
                                    "price_range": [min_price, max_price],
                                    "avg_price": avg_price
                                }

                                if invalid_price > 0 or invalid_amount > 0:
                                    quality_checks.append("ä»·æ ¼/æ•°é‡å¼‚å¸¸")

                    # 2. äº¤æ˜“æ‰€å’Œäº¤æ˜“å¯¹å®Œæ•´æ€§æ£€æŸ¥
                    code, exchanges = self.ch_query(f"""
                        SELECT DISTINCT exchange FROM {table}
                        WHERE timestamp > now() - INTERVAL 1 HOUR
                    """)
                    if code == 200:
                        found_exchanges = [e.strip() for e in exchanges.split('\n') if e.strip()]
                        details["exchanges"] = found_exchanges
                        missing_exchanges = set(EXPECTED_EXCHANGES) - set(found_exchanges)
                        if missing_exchanges:
                            quality_checks.append(f"ç¼ºå¤±äº¤æ˜“æ‰€: {missing_exchanges}")

                    # 3. æ—¶é—´åºåˆ—è¿ç»­æ€§åˆ†æ
                    if table in HIGH_FREQ_TABLES:
                        code, gaps = self.ch_query(f"""
                            SELECT count() FROM (
                                SELECT timestamp,
                                       lag(timestamp) OVER (ORDER BY timestamp) as prev_ts
                                FROM {table}
                                WHERE timestamp > now() - INTERVAL 30 MINUTE
                                ORDER BY timestamp
                            ) WHERE dateDiff('second', prev_ts, timestamp) > 60
                        """)
                        if code == 200:
                            gap_count = int(gaps)
                            details["time_gaps"] = gap_count
                            if gap_count > 10:
                                quality_checks.append(f"æ—¶é—´é—´éš”å¼‚å¸¸: {gap_count}ä¸ªå¤§äº60ç§’çš„é—´éš”")

                    # 4. é‡å¤æ•°æ®æ£€æµ‹
                    if table in ["trades", "orderbooks"]:
                        code, duplicates = self.ch_query(f"""
                            SELECT count() FROM (
                                SELECT exchange, symbol, timestamp, count() as cnt
                                FROM {table}
                                WHERE timestamp > now() - INTERVAL 1 HOUR
                                GROUP BY exchange, symbol, timestamp
                                HAVING cnt > 1
                            )
                        """)
                        if code == 200:
                            dup_count = int(duplicates)
                            details["duplicates"] = dup_count
                            if dup_count > 0:
                                quality_checks.append(f"é‡å¤æ•°æ®: {dup_count}æ¡")

                    # ç»¼åˆè¯„ä¼°
                    if not quality_checks:
                        status = "PASS"
                        summary = f"æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ€»è®¡{total_count}æ¡è®°å½•"
                    else:
                        status = "WARNING"
                        summary = f"å‘ç°è´¨é‡é—®é¢˜: {'; '.join(quality_checks)}"

            except Exception as e:
                status = "FAIL"
                summary = f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}"
                details["error"] = str(e)

            duration_ms = int((time.time() - start_time) * 1000)
            self.add_result("data_quality", f"{table}_analysis", status,
                           {**details, "summary": summary}, duration_ms)

    def validate_system_stability(self):
        """3. ç³»ç»Ÿå‹åŠ›å’Œç¨³å®šæ€§æµ‹è¯•"""
        self.log(f"å¼€å§‹{self.duration_minutes}åˆ†é’Ÿç¨³å®šæ€§æµ‹è¯•...")

        start_time = time.time()
        end_time = start_time + (self.duration_minutes * 60)

        # å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†çº¿ç¨‹
        metrics_thread = threading.Thread(target=self._collect_metrics_continuously,
                                         args=(end_time,), daemon=True)
        metrics_thread.start()

        # è¿æ¥çŠ¶æ€ç›‘æ§
        connection_issues = []
        performance_issues = []

        sample_count = 0
        while time.time() < end_time:
            sample_count += 1

            # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
            services_health = {}
            for service, url in [("storage", STORAGE_HEALTH), ("collector", COLLECTOR_HEALTH)]:
                code, response = self.http_get(url, timeout=5)
                services_health[service] = {
                    "status_code": code,
                    "healthy": code == 200 and response.get("status") == "healthy"
                }
                if not services_health[service]["healthy"]:
                    connection_issues.append(f"{service} unhealthy at sample {sample_count}")

            # æ£€æŸ¥NATSè¿æ¥çŠ¶æ€
            code, nats_health = self.http_get(f"{NATS_MONITORING}/healthz", timeout=5)
            if code != 200:
                connection_issues.append(f"NATS unhealthy at sample {sample_count}")

            # æ£€æŸ¥ClickHouseå“åº”æ—¶é—´
            ch_start = time.time()
            code, _ = self.ch_query("SELECT 1", timeout=10)
            ch_duration = time.time() - ch_start

            if code != 200:
                connection_issues.append(f"ClickHouse error at sample {sample_count}")
            elif ch_duration > 2.0:
                performance_issues.append(f"ClickHouse slow response: {ch_duration:.2f}s at sample {sample_count}")

            # é‡‡æ ·é—´éš”
            time.sleep(30)

        # ç­‰å¾…æŒ‡æ ‡æ”¶é›†å®Œæˆ
        metrics_thread.join(timeout=10)

        # åˆ†æç»“æœ
        details = {
            "duration_minutes": self.duration_minutes,
            "total_samples": sample_count,
            "connection_issues": connection_issues,
            "performance_issues": performance_issues,
            "metrics_collected": len(self.metrics)
        }

        if connection_issues or performance_issues:
            status = "WARNING" if len(connection_issues) < sample_count * 0.1 else "FAIL"
            summary = f"å‘ç°{len(connection_issues)}ä¸ªè¿æ¥é—®é¢˜ï¼Œ{len(performance_issues)}ä¸ªæ€§èƒ½é—®é¢˜"
        else:
            status = "PASS"
            summary = f"ç³»ç»Ÿç¨³å®šè¿è¡Œ{self.duration_minutes}åˆ†é’Ÿï¼Œæ— å¼‚å¸¸"

        duration_ms = int((time.time() - start_time) * 1000)
        self.add_result("stability", "long_run_test", status,
                       {**details, "summary": summary}, duration_ms)

    def _collect_metrics_continuously(self, end_time: float):
        """æŒç»­æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        while time.time() < end_time:
            self.collect_system_metrics()
            time.sleep(10)  # æ¯10ç§’æ”¶é›†ä¸€æ¬¡

    def validate_error_handling(self):
        """4. é”™è¯¯å¤„ç†å’Œç›‘æ§éªŒè¯"""
        self.log("å¼€å§‹é”™è¯¯å¤„ç†éªŒè¯...")

        error_scenarios = []

        # åœºæ™¯1: æµ‹è¯•ClickHouseè¿æ¥ä¸­æ–­æ¢å¤
        start_time = time.time()
        try:
            # æš‚åœClickHouseå®¹å™¨
            self.log("æµ‹è¯•ClickHouseè¿æ¥ä¸­æ–­...")
            subprocess.run(["docker", "pause", "marketprism-clickhouse-hot"],
                          check=True, capture_output=True)

            # ç­‰å¾…å‡ ç§’è®©ç³»ç»Ÿæ£€æµ‹åˆ°ä¸­æ–­
            time.sleep(5)

            # æ£€æŸ¥StorageæœåŠ¡çš„å¥åº·çŠ¶æ€
            code, health = self.http_get(STORAGE_HEALTH, timeout=5)
            ch_disconnected = code != 200 or not health.get("healthy", False)

            # æ¢å¤ClickHouse
            subprocess.run(["docker", "unpause", "marketprism-clickhouse-hot"],
                          check=True, capture_output=True)

            # ç­‰å¾…æ¢å¤
            recovery_time = 0
            max_recovery_time = 30
            while recovery_time < max_recovery_time:
                time.sleep(2)
                recovery_time += 2
                code, _ = self.ch_query("SELECT 1", timeout=5)
                if code == 200:
                    break

            error_scenarios.append({
                "scenario": "clickhouse_disconnect",
                "detected_disconnect": ch_disconnected,
                "recovery_time_seconds": recovery_time,
                "recovered": code == 200
            })

        except Exception as e:
            error_scenarios.append({
                "scenario": "clickhouse_disconnect",
                "error": str(e),
                "recovered": False
            })

        # åœºæ™¯2: æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†
        try:
            self.log("æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†...")
            # è¿™é‡Œå¯ä»¥é€šè¿‡NATSå‘é€æ— æ•ˆæ•°æ®æ¥æµ‹è¯•é”™è¯¯å¤„ç†
            # ç”±äºå¤æ‚æ€§ï¼Œè¿™é‡Œåªæ£€æŸ¥æ—¥å¿—ä¸­çš„é”™è¯¯å¤„ç†è®°å½•

            # æ£€æŸ¥StorageæœåŠ¡æ—¥å¿—ä¸­çš„é”™è¯¯å¤„ç†
            try:
                with open("services/data-storage-service/production.log", "r") as f:
                    log_content = f.read()
                    error_count = log_content.count("ERROR")
                    warning_count = log_content.count("WARNING")

                error_scenarios.append({
                    "scenario": "log_error_analysis",
                    "error_count": error_count,
                    "warning_count": warning_count,
                    "has_error_handling": error_count > 0 or warning_count > 0
                })
            except FileNotFoundError:
                error_scenarios.append({
                    "scenario": "log_error_analysis",
                    "error": "Log file not found"
                })

        except Exception as e:
            error_scenarios.append({
                "scenario": "invalid_data_test",
                "error": str(e)
            })

        # è¯„ä¼°é”™è¯¯å¤„ç†èƒ½åŠ›
        recovery_success = sum(1 for s in error_scenarios
                              if s.get("recovered", False) or s.get("has_error_handling", False))

        if recovery_success >= len(error_scenarios) * 0.8:
            status = "PASS"
            summary = f"é”™è¯¯å¤„ç†è‰¯å¥½ï¼Œ{recovery_success}/{len(error_scenarios)}ä¸ªåœºæ™¯é€šè¿‡"
        elif recovery_success > 0:
            status = "WARNING"
            summary = f"é”™è¯¯å¤„ç†éƒ¨åˆ†æœ‰æ•ˆï¼Œ{recovery_success}/{len(error_scenarios)}ä¸ªåœºæ™¯é€šè¿‡"
        else:
            status = "FAIL"
            summary = "é”™è¯¯å¤„ç†æœºåˆ¶å­˜åœ¨é—®é¢˜"

        duration_ms = int((time.time() - start_time) * 1000)
        self.add_result("error_handling", "fault_tolerance", status,
                       {"scenarios": error_scenarios, "summary": summary}, duration_ms)

    def validate_performance_benchmarks(self):
        """5. æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        self.log("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")

        benchmarks = {}

        # 1. ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•
        start_time = time.time()
        try:
            # æµ‹é‡ClickHouseæŸ¥è¯¢æ€§èƒ½
            query_times = []
            for _ in range(10):
                query_start = time.time()
                code, _ = self.ch_query("SELECT count() FROM trades WHERE timestamp > now() - INTERVAL 5 MINUTE")
                query_time = time.time() - query_start
                if code == 200:
                    query_times.append(query_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

            if query_times:
                benchmarks["clickhouse_query"] = {
                    "avg_ms": sum(query_times) / len(query_times),
                    "min_ms": min(query_times),
                    "max_ms": max(query_times),
                    "samples": len(query_times)
                }

        except Exception as e:
            benchmarks["clickhouse_query"] = {"error": str(e)}

        # 2. ååé‡æµ‹è¯•
        try:
            # è®¡ç®—æœ€è¿‘5åˆ†é’Ÿçš„æ¶ˆæ¯å¤„ç†é€Ÿç‡
            throughput_queries = {
                "trades_per_minute": "SELECT count() / 5 FROM trades WHERE timestamp > now() - INTERVAL 5 MINUTE",
                "orderbooks_per_minute": "SELECT count() / 5 FROM orderbooks WHERE timestamp > now() - INTERVAL 5 MINUTE"
            }

            for metric, query in throughput_queries.items():
                code, result = self.ch_query(query)
                if code == 200:
                    try:
                        rate = float(result)
                        benchmarks[metric] = rate
                    except ValueError:
                        benchmarks[metric] = 0

        except Exception as e:
            benchmarks["throughput_error"] = str(e)

        # 3. ç³»ç»Ÿèµ„æºä½¿ç”¨åˆ†æ
        if self.metrics:
            cpu_values = [m.cpu_percent for m in self.metrics]
            memory_values = [m.memory_percent for m in self.metrics]

            benchmarks["resource_usage"] = {
                "avg_cpu_percent": sum(cpu_values) / len(cpu_values),
                "max_cpu_percent": max(cpu_values),
                "avg_memory_percent": sum(memory_values) / len(memory_values),
                "max_memory_percent": max(memory_values),
                "samples": len(self.metrics)
            }

        # 4. NATS JetStreamæ€§èƒ½
        try:
            #   NATS jsz   streams  int  /jsz?streams=true&config=true 
            code, js_info = self.http_get(f"{NATS_MONITORING}/jsz?streams=true&config=true")
            if code == 200 and isinstance(js_info, dict):
                streams = js_info.get("streams", [])
                if not isinstance(streams, list):
                    streams = []
                for stream in streams:
                    stream_name = stream.get("config", {}).get("name", "unknown")
                    benchmarks[f"jetstream_{stream_name.lower()}"] = {
                        "messages": stream.get("state", {}).get("messages", 0),
                        "bytes": stream.get("state", {}).get("bytes", 0),
                        "consumers": stream.get("state", {}).get("consumers", 0)
                    }
        except Exception as e:
            benchmarks["jetstream_error"] = str(e)

        # æ€§èƒ½è¯„ä¼°
        performance_score = 0
        max_score = 0

        # ClickHouseæŸ¥è¯¢æ€§èƒ½è¯„åˆ†
        if "clickhouse_query" in benchmarks and "avg_ms" in benchmarks["clickhouse_query"]:
            avg_ms = benchmarks["clickhouse_query"]["avg_ms"]
            if avg_ms < 100:
                performance_score += 25
            elif avg_ms < 500:
                performance_score += 15
            elif avg_ms < 1000:
                performance_score += 10
            max_score += 25

        # ååé‡è¯„åˆ†
        trades_rate = benchmarks.get("trades_per_minute", 0)
        if trades_rate > 1000:
            performance_score += 25
        elif trades_rate > 500:
            performance_score += 15
        elif trades_rate > 100:
            performance_score += 10
        max_score += 25

        # èµ„æºä½¿ç”¨è¯„åˆ†
        if "resource_usage" in benchmarks:
            avg_cpu = benchmarks["resource_usage"]["avg_cpu_percent"]
            avg_memory = benchmarks["resource_usage"]["avg_memory_percent"]
            if avg_cpu < 50 and avg_memory < 70:
                performance_score += 25
            elif avg_cpu < 80 and avg_memory < 85:
                performance_score += 15
            max_score += 25

        # JetStreamè¯„åˆ†
        if any(k.startswith("jetstream_") for k in benchmarks.keys()):
            performance_score += 25
        max_score += 25

        # ç»¼åˆè¯„ä¼°
        if max_score > 0:
            score_percentage = (performance_score / max_score) * 100
            if score_percentage >= 80:
                status = "PASS"
                summary = f"æ€§èƒ½ä¼˜ç§€ï¼Œå¾—åˆ†{score_percentage:.1f}%"
            elif score_percentage >= 60:
                status = "WARNING"
                summary = f"æ€§èƒ½è‰¯å¥½ï¼Œå¾—åˆ†{score_percentage:.1f}%"
            else:
                status = "FAIL"
                summary = f"æ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œå¾—åˆ†{score_percentage:.1f}%"
        else:
            status = "FAIL"
            summary = "æ— æ³•è¯„ä¼°æ€§èƒ½"

        duration_ms = int((time.time() - start_time) * 1000)
        self.add_result("performance", "benchmark_test", status,
                       {**benchmarks, "score_percentage": score_percentage if max_score > 0 else 0,
                        "summary": summary}, duration_ms)

    def run_all_validations(self):
        """æ‰§è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        self.log("ğŸš€ å¼€å§‹MarketPrismç³»ç»Ÿæ·±åº¦éªŒè¯")
        self.log(f"æµ‹è¯•æŒç»­æ—¶é—´: {self.duration_minutes}åˆ†é’Ÿ")
        self.log(f"å‹åŠ›æµ‹è¯•æ¨¡å¼: {'å¯ç”¨' if self.stress_test else 'ç¦ç”¨'}")

        total_start = time.time()

        try:
            # 1. é…ç½®ä¸€è‡´æ€§éªŒè¯
            self.validate_config_consistency()

            # 2. æ•°æ®è´¨é‡åˆ†æ
            self.validate_data_quality()

            # 3. ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•
            self.validate_system_stability()

            # 4. é”™è¯¯å¤„ç†éªŒè¯
            self.validate_error_handling()

            # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
            self.validate_performance_benchmarks()

        except KeyboardInterrupt:
            self.log("éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­", "WARNING")
        except Exception as e:
            self.log(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}", "ERROR")

        total_duration = time.time() - total_start
        self.log(f"âœ… éªŒè¯å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.1f}ç§’")

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

    def generate_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š"""
        self.log("ğŸ“Š ç”ŸæˆéªŒè¯æŠ¥å‘Š...")

        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.results)
        passed_tests = len([r for r in self.results if r.status == "PASS"])
        warning_tests = len([r for r in self.results if r.status == "WARNING"])
        failed_tests = len([r for r in self.results if r.status == "FAIL"])

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "validation_summary": {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "duration_minutes": self.duration_minutes,
                "stress_test_enabled": self.stress_test,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "warning_tests": warning_tests,
                "failed_tests": failed_tests,
                "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0
            },
            "test_results": [asdict(r) for r in self.results],
            "system_metrics": [asdict(m) for m in self.metrics],
            "recommendations": self._generate_recommendations()
        }

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.log(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            self.log(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}", "ERROR")

        # æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        self._print_summary_report(report)

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        failed_modules = set()
        warning_modules = set()

        for result in self.results:
            if result.status == "FAIL":
                failed_modules.add(result.module)
            elif result.status == "WARNING":
                warning_modules.add(result.module)

        if "config" in failed_modules:
            recommendations.append("é…ç½®ä¸€è‡´æ€§å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç¯å¢ƒå˜é‡å’ŒJetStreamé…ç½®")

        if "data_quality" in warning_modules or "data_quality" in failed_modules:
            recommendations.append("æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®å¢å¼ºæ•°æ®éªŒè¯å’Œæ¸…æ´—æœºåˆ¶")

        if "stability" in failed_modules:
            recommendations.append("ç³»ç»Ÿç¨³å®šæ€§ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–è¿æ¥ç®¡ç†å’Œé”™è¯¯æ¢å¤æœºåˆ¶")

        if "performance" in warning_modules or "performance" in failed_modules:
            recommendations.append("æ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œå»ºè®®è°ƒæ•´æ‰¹å¤„ç†å‚æ•°å’Œèµ„æºé…ç½®")

        # åŸºäºç³»ç»ŸæŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if self.metrics:
            avg_cpu = sum(m.cpu_percent for m in self.metrics) / len(self.metrics)
            avg_memory = sum(m.memory_percent for m in self.metrics) / len(self.metrics)

            if avg_cpu > 80:
                recommendations.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜({avg_cpu:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æˆ–å¢åŠ è®¡ç®—èµ„æº")
            if avg_memory > 85:
                recommendations.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜({avg_memory:.1f}%)ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†æˆ–å¢åŠ å†…å­˜")

        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§å…³é”®æŒ‡æ ‡")

        return recommendations

    def _print_summary_report(self, report: Dict):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        summary = report["validation_summary"]

        print("\n" + "="*80)
        print("ğŸ¯ MarketPrism ç³»ç»Ÿæ·±åº¦éªŒè¯æŠ¥å‘Š")
        print("="*80)

        print(f"ğŸ“… éªŒè¯æ—¶é—´: {summary['timestamp']}")
        print(f"â±ï¸  æµ‹è¯•æ—¶é•¿: {summary['duration_minutes']}åˆ†é’Ÿ")
        print(f"ğŸ§ª æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… é€šè¿‡: {summary['passed_tests']}")
        print(f"âš ï¸  è­¦å‘Š: {summary['warning_tests']}")
        print(f"âŒ å¤±è´¥: {summary['failed_tests']}")
        print(f"ğŸ“Š æˆåŠŸç‡: {summary['success_rate']:.1f}%")

        print("\nğŸ“‹ æµ‹è¯•ç»“æœè¯¦æƒ…:")
        print("-" * 60)

        for result in self.results:
            status_icon = {"PASS": "âœ…", "FAIL": "âŒ", "WARNING": "âš ï¸"}.get(result.status, "â“")
            print(f"{status_icon} {result.module}.{result.test_name}: {result.status}")
            if result.details.get("summary"):
                print(f"   â””â”€ {result.details['summary']}")

        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        print("-" * 60)
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"{i}. {rec}")

        if self.metrics:
            print(f"\nğŸ“ˆ ç³»ç»ŸæŒ‡æ ‡æ‘˜è¦ (åŸºäº{len(self.metrics)}ä¸ªé‡‡æ ·ç‚¹):")
            print("-" * 60)
            avg_cpu = sum(m.cpu_percent for m in self.metrics) / len(self.metrics)
            avg_memory = sum(m.memory_percent for m in self.metrics) / len(self.metrics)
            print(f"CPUå¹³å‡ä½¿ç”¨ç‡: {avg_cpu:.1f}%")
            print(f"å†…å­˜å¹³å‡ä½¿ç”¨ç‡: {avg_memory:.1f}%")

        print("\n" + "="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MarketPrismç³»ç»Ÿæ·±åº¦éªŒè¯")
    parser.add_argument("--duration", type=int, default=30,
                       help="ç¨³å®šæ€§æµ‹è¯•æŒç»­æ—¶é—´(åˆ†é’Ÿ)ï¼Œé»˜è®¤30åˆ†é’Ÿ")
    parser.add_argument("--stress-test", action="store_true",
                       help="å¯ç”¨å‹åŠ›æµ‹è¯•æ¨¡å¼")

    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–
    try:
        import psutil
        import requests
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·å®‰è£…: pip install psutil requests")
        sys.exit(1)

    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡Œ
    validator = ValidationRunner(duration_minutes=args.duration,
                               stress_test=args.stress_test)
    validator.run_all_validations()


if __name__ == "__main__":
    main()
