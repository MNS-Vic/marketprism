"""
MarketPrism æ€§èƒ½åŸºå‡†æµ‹è¯•
Phase 4: ä¼˜åŒ–ä¸éƒ¨ç½² - æ€§èƒ½æµ‹è¯•ç»„ä»¶

æä¾›å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
1. å•æœåŠ¡æ€§èƒ½æµ‹è¯•
2. æ•´ä½“ç³»ç»Ÿå‹åŠ›æµ‹è¯•  
3. ç“¶é¢ˆè¯†åˆ«å’Œåˆ†æ
4. æ€§èƒ½æŒ‡æ ‡åŸºçº¿å»ºç«‹
5. è´Ÿè½½æµ‹è¯•å’Œç¨³å®šæ€§éªŒè¯
"""

import asyncio
import aiohttp
import time
import statistics
import json
import psutil
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import yaml
import concurrent.futures
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    service_name: str
    test_type: str
    start_time: float
    end_time: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    response_times: List[float]
    errors: List[str]
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time
    
    @property
    def success_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return (self.successful_requests / self.total_requests) * 100
    
    @property
    def requests_per_second(self) -> float:
        if self.duration == 0:
            return 0.0
        return self.total_requests / self.duration
    
    @property
    def avg_response_time(self) -> float:
        if not self.response_times:
            return 0.0
        return statistics.mean(self.response_times)
    
    @property
    def p95_response_time(self) -> float:
        if not self.response_times:
            return 0.0
        return statistics.quantiles(self.response_times, n=20)[18]  # 95th percentile
    
    @property
    def p99_response_time(self) -> float:
        if not self.response_times:
            return 0.0
        return statistics.quantiles(self.response_times, n=100)[98]  # 99th percentile


class SystemResourceMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.resource_data = []
    
    async def start_monitoring(self, interval: float = 1.0):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        self.monitoring = True
        self.resource_data = []
        
        while self.monitoring:
            try:
                resource_info = {
                    'timestamp': time.time(),
                    'cpu_percent': psutil.cpu_percent(interval=None),
                    'memory_percent': psutil.virtual_memory().percent,
                    'memory_available_mb': psutil.virtual_memory().available / (1024**2),
                    'disk_io_read_mb': psutil.disk_io_counters().read_bytes / (1024**2),
                    'disk_io_write_mb': psutil.disk_io_counters().write_bytes / (1024**2),
                    'network_sent_mb': psutil.net_io_counters().bytes_sent / (1024**2),
                    'network_recv_mb': psutil.net_io_counters().bytes_recv / (1024**2),
                }
                self.resource_data.append(resource_info)
                await asyncio.sleep(interval)
            except Exception as e:
                print(f"èµ„æºç›‘æ§é”™è¯¯: {e}")
                await asyncio.sleep(interval)
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æ€»ç»“"""
        if not self.resource_data:
            return {}
        
        cpu_values = [d['cpu_percent'] for d in self.resource_data]
        memory_values = [d['memory_percent'] for d in self.resource_data]
        
        return {
            'duration_seconds': len(self.resource_data),
            'cpu_usage': {
                'avg': statistics.mean(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            },
            'memory_usage': {
                'avg': statistics.mean(memory_values),
                'max': max(memory_values),
                'min': min(memory_values)
            },
            'peak_memory_available_mb': min(d['memory_available_mb'] for d in self.resource_data)
        }


class ServiceBenchmark:
    """å•æœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, service_name: str, base_url: str):
        self.service_name = service_name
        self.base_url = base_url
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def health_check_test(self, num_requests: int = 100) -> PerformanceMetrics:
        """å¥åº·æ£€æŸ¥æ€§èƒ½æµ‹è¯•"""
        print(f"ğŸ” {self.service_name} å¥åº·æ£€æŸ¥æµ‹è¯• ({num_requests} è¯·æ±‚)")
        
        start_time = time.time()
        response_times = []
        successful_requests = 0
        failed_requests = 0
        errors = []
        
        tasks = []
        for i in range(num_requests):
            task = self._single_health_check()
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                failed_requests += 1
                errors.append(str(result))
            else:
                successful_requests += 1
                response_times.append(result)
        
        end_time = time.time()
        
        return PerformanceMetrics(
            service_name=self.service_name,
            test_type="health_check",
            start_time=start_time,
            end_time=end_time,
            total_requests=num_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            response_times=response_times,
            errors=errors
        )
    
    async def _single_health_check(self) -> float:
        """å•æ¬¡å¥åº·æ£€æŸ¥"""
        start = time.time()
        async with self.session.get(f"{self.base_url}/health") as response:
            await response.read()
            if response.status != 200:
                raise Exception(f"Health check failed: {response.status}")
        return time.time() - start
    
    async def api_stress_test(self, endpoint: str, num_requests: int = 500, 
                             concurrent_requests: int = 50) -> PerformanceMetrics:
        """APIå‹åŠ›æµ‹è¯•"""
        print(f"âš¡ {self.service_name} APIå‹åŠ›æµ‹è¯• ({endpoint}, {num_requests} è¯·æ±‚, {concurrent_requests} å¹¶å‘)")
        
        start_time = time.time()
        response_times = []
        successful_requests = 0
        failed_requests = 0
        errors = []
        
        # åˆ†æ‰¹æ‰§è¡Œè¯·æ±‚
        semaphore = asyncio.Semaphore(concurrent_requests)
        
        async def single_request():
            async with semaphore:
                try:
                    start = time.time()
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        await response.read()
                        response_time = time.time() - start
                        if response.status == 200:
                            return True, response_time, None
                        else:
                            return False, response_time, f"HTTP {response.status}"
                except Exception as e:
                    return False, 0, str(e)
        
        tasks = [single_request() for _ in range(num_requests)]
        results = await asyncio.gather(*tasks)
        
        for success, response_time, error in results:
            if success:
                successful_requests += 1
                response_times.append(response_time)
            else:
                failed_requests += 1
                if error:
                    errors.append(error)
        
        end_time = time.time()
        
        return PerformanceMetrics(
            service_name=self.service_name,
            test_type=f"api_stress_{endpoint.replace('/', '_')}",
            start_time=start_time,
            end_time=end_time,
            total_requests=num_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            response_times=response_times,
            errors=errors
        )


class SystemBenchmark:
    """æ•´ä½“ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, services_config: Dict[str, Dict[str, Any]]):
        self.services_config = services_config
        self.resource_monitor = SystemResourceMonitor()
    
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹MarketPrismç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        # å¯åŠ¨èµ„æºç›‘æ§
        monitor_task = asyncio.create_task(
            self.resource_monitor.start_monitoring(interval=0.5)
        )
        
        benchmark_results = {
            'test_start_time': datetime.utcnow().isoformat(),
            'service_results': {},
            'system_resources': {},
            'overall_metrics': {}
        }
        
        try:
            # æµ‹è¯•å„ä¸ªæœåŠ¡
            for service_name, config in self.services_config.items():
                print(f"\nğŸ“Š æµ‹è¯•æœåŠ¡: {service_name}")
                service_results = await self._test_service(service_name, config)
                benchmark_results['service_results'][service_name] = service_results
            
            # è¿è¡Œæ•´ä½“ç³»ç»Ÿæµ‹è¯•
            print(f"\nğŸ”„ ç³»ç»Ÿæ•´ä½“å‹åŠ›æµ‹è¯•")
            overall_results = await self._test_system_integration()
            benchmark_results['overall_metrics'] = overall_results
            
        finally:
            # åœæ­¢èµ„æºç›‘æ§
            self.resource_monitor.stop_monitoring()
            monitor_task.cancel()
            
            # è·å–èµ„æºä½¿ç”¨æ€»ç»“
            benchmark_results['system_resources'] = self.resource_monitor.get_summary()
        
        benchmark_results['test_end_time'] = datetime.utcnow().isoformat()
        
        return benchmark_results
    
    async def _test_service(self, service_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæœåŠ¡æ€§èƒ½"""
        base_url = f"http://{config.get('host', 'localhost')}:{config['port']}"
        
        service_results = {
            'service_name': service_name,
            'base_url': base_url,
            'tests': {}
        }
        
        try:
            async with ServiceBenchmark(service_name, base_url) as benchmark:
                # å¥åº·æ£€æŸ¥æµ‹è¯•
                health_metrics = await benchmark.health_check_test(num_requests=100)
                service_results['tests']['health_check'] = self._metrics_to_dict(health_metrics)
                
                # APIæµ‹è¯•ï¼ˆæ ¹æ®æœåŠ¡ç±»å‹ï¼‰
                if service_name == 'api-gateway-service':
                    # æµ‹è¯•ç½‘å…³è·¯ç”±
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/storage/status", num_requests=200, concurrent_requests=20
                    )
                    service_results['tests']['gateway_routing'] = self._metrics_to_dict(api_metrics)
                
                elif service_name == 'monitoring-service':
                    # æµ‹è¯•ç›‘æ§æ¦‚è§ˆ
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/overview", num_requests=150, concurrent_requests=15
                    )
                    service_results['tests']['monitoring_overview'] = self._metrics_to_dict(api_metrics)
                
                elif service_name == 'data-storage-service':
                    # æµ‹è¯•å­˜å‚¨çŠ¶æ€
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/storage/status", num_requests=200, concurrent_requests=25
                    )
                    service_results['tests']['storage_status'] = self._metrics_to_dict(api_metrics)
                
                elif service_name == 'market-data-collector':
                    # æµ‹è¯•é‡‡é›†å™¨çŠ¶æ€
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/status", num_requests=100, concurrent_requests=10
                    )
                    service_results['tests']['collector_status'] = self._metrics_to_dict(api_metrics)
                
                elif service_name == 'message-broker-service':
                    # æµ‹è¯•æ¶ˆæ¯ä»£ç†çŠ¶æ€
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/status", num_requests=100, concurrent_requests=10
                    )
                    service_results['tests']['broker_status'] = self._metrics_to_dict(api_metrics)
                
                elif service_name == 'scheduler-service':
                    # æµ‹è¯•è°ƒåº¦å™¨çŠ¶æ€
                    api_metrics = await benchmark.api_stress_test(
                        "/api/v1/status", num_requests=100, concurrent_requests=10
                    )
                    service_results['tests']['scheduler_status'] = self._metrics_to_dict(api_metrics)
        
        except Exception as e:
            service_results['error'] = str(e)
            print(f"âŒ æµ‹è¯• {service_name} å¤±è´¥: {e}")
        
        return service_results
    
    async def _test_system_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•ç³»ç»Ÿæ•´ä½“é›†æˆæ€§èƒ½"""
        print("  ğŸ“ˆ æ‰§è¡Œè·¨æœåŠ¡é›†æˆæµ‹è¯•...")
        
        integration_results = {
            'cross_service_tests': {},
            'concurrent_load_test': {}
        }
        
        # è·¨æœåŠ¡è°ƒç”¨æµ‹è¯•ï¼ˆé€šè¿‡APIç½‘å…³ï¼‰
        try:
            gateway_url = f"http://localhost:{self.services_config.get('api-gateway-service', {}).get('port', 8080)}"
            
            async with ServiceBenchmark("system_integration", gateway_url) as benchmark:
                # æµ‹è¯•é€šè¿‡ç½‘å…³è®¿é—®å„æœåŠ¡
                cross_service_metrics = await benchmark.api_stress_test(
                    "/api/v1/monitoring-service/overview", 
                    num_requests=100, 
                    concurrent_requests=10
                )
                integration_results['cross_service_tests']['gateway_to_monitoring'] = \
                    self._metrics_to_dict(cross_service_metrics)
        
        except Exception as e:
            integration_results['cross_service_tests']['error'] = str(e)
        
        # å¹¶å‘è´Ÿè½½æµ‹è¯•
        try:
            concurrent_results = await self._concurrent_load_test()
            integration_results['concurrent_load_test'] = concurrent_results
        except Exception as e:
            integration_results['concurrent_load_test']['error'] = str(e)
        
        return integration_results
    
    async def _concurrent_load_test(self) -> Dict[str, Any]:
        """å¹¶å‘è´Ÿè½½æµ‹è¯•"""
        print("  âš¡ æ‰§è¡Œå¹¶å‘è´Ÿè½½æµ‹è¯•...")
        
        # åŒæ—¶å¯¹å¤šä¸ªæœåŠ¡æ–½åŠ è´Ÿè½½
        load_tasks = []
        
        for service_name, config in self.services_config.items():
            if service_name in ['api-gateway-service', 'monitoring-service', 'data-storage-service']:
                base_url = f"http://{config.get('host', 'localhost')}:{config['port']}"
                
                async def service_load_test(svc_name, url):
                    async with ServiceBenchmark(svc_name, url) as benchmark:
                        return await benchmark.health_check_test(num_requests=50)
                
                load_tasks.append(service_load_test(service_name, base_url))
        
        start_time = time.time()
        results = await asyncio.gather(*load_tasks, return_exceptions=True)
        end_time = time.time()
        
        concurrent_metrics = {
            'total_duration': end_time - start_time,
            'services_tested': len(load_tasks),
            'results': {}
        }
        
        for i, result in enumerate(results):
            service_name = list(self.services_config.keys())[i]
            if isinstance(result, Exception):
                concurrent_metrics['results'][service_name] = {'error': str(result)}
            else:
                concurrent_metrics['results'][service_name] = self._metrics_to_dict(result)
        
        return concurrent_metrics
    
    def _metrics_to_dict(self, metrics: PerformanceMetrics) -> Dict[str, Any]:
        """è½¬æ¢æ€§èƒ½æŒ‡æ ‡ä¸ºå­—å…¸"""
        return {
            'service_name': metrics.service_name,
            'test_type': metrics.test_type,
            'duration_seconds': metrics.duration,
            'total_requests': metrics.total_requests,
            'successful_requests': metrics.successful_requests,
            'failed_requests': metrics.failed_requests,
            'success_rate_percent': metrics.success_rate,
            'requests_per_second': metrics.requests_per_second,
            'avg_response_time_ms': metrics.avg_response_time * 1000,
            'p95_response_time_ms': metrics.p95_response_time * 1000,
            'p99_response_time_ms': metrics.p99_response_time * 1000,
            'error_count': len(metrics.errors),
            'sample_errors': metrics.errors[:5]  # åªä¿ç•™å‰5ä¸ªé”™è¯¯æ ·ä¾‹
        }


class BenchmarkReporter:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, results: Dict[str, Any]):
        self.results = results
    
    def generate_console_report(self):
        """ç”Ÿæˆæ§åˆ¶å°æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š MarketPrism æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æµ‹è¯•æ¦‚è§ˆ
        print(f"\nğŸ• æµ‹è¯•æ—¶é—´: {self.results.get('test_start_time', 'Unknown')}")
        print(f"ğŸ“‹ æµ‹è¯•æœåŠ¡æ•°: {len(self.results.get('service_results', {}))}")
        
        # ç³»ç»Ÿèµ„æºä½¿ç”¨
        resource_summary = self.results.get('system_resources', {})
        if resource_summary:
            print(f"\nğŸ’» ç³»ç»Ÿèµ„æºä½¿ç”¨:")
            cpu_usage = resource_summary.get('cpu_usage', {})
            memory_usage = resource_summary.get('memory_usage', {})
            print(f"   - CPU: å¹³å‡ {cpu_usage.get('avg', 0):.1f}%, æœ€é«˜ {cpu_usage.get('max', 0):.1f}%")
            print(f"   - å†…å­˜: å¹³å‡ {memory_usage.get('avg', 0):.1f}%, æœ€é«˜ {memory_usage.get('max', 0):.1f}%")
        
        # å„æœåŠ¡æ€§èƒ½
        print(f"\nğŸ“ˆ å„æœåŠ¡æ€§èƒ½æŒ‡æ ‡:")
        service_results = self.results.get('service_results', {})
        
        for service_name, service_data in service_results.items():
            print(f"\nğŸ” {service_name}:")
            
            if 'error' in service_data:
                print(f"   âŒ æµ‹è¯•å¤±è´¥: {service_data['error']}")
                continue
            
            tests = service_data.get('tests', {})
            for test_name, test_data in tests.items():
                print(f"   ğŸ“Š {test_name}:")
                print(f"      - æˆåŠŸç‡: {test_data.get('success_rate_percent', 0):.1f}%")
                print(f"      - QPS: {test_data.get('requests_per_second', 0):.1f}")
                print(f"      - å¹³å‡å“åº”æ—¶é—´: {test_data.get('avg_response_time_ms', 0):.1f}ms")
                print(f"      - P95å“åº”æ—¶é—´: {test_data.get('p95_response_time_ms', 0):.1f}ms")
                
                if test_data.get('error_count', 0) > 0:
                    print(f"      - é”™è¯¯æ•°: {test_data['error_count']}")
        
        # æ•´ä½“ç³»ç»Ÿæ€§èƒ½
        overall_metrics = self.results.get('overall_metrics', {})
        if overall_metrics:
            print(f"\nğŸ”„ æ•´ä½“ç³»ç»Ÿæ€§èƒ½:")
            concurrent_test = overall_metrics.get('concurrent_load_test', {})
            if 'total_duration' in concurrent_test:
                print(f"   - å¹¶å‘æµ‹è¯•æ—¶é•¿: {concurrent_test['total_duration']:.2f}s")
                print(f"   - å¹¶å‘æµ‹è¯•æœåŠ¡æ•°: {concurrent_test.get('services_tested', 0)}")
        
        # æ€§èƒ½åŸºçº¿å»ºè®®
        self._print_performance_baseline()
    
    def _print_performance_baseline(self):
        """æ‰“å°æ€§èƒ½åŸºçº¿å»ºè®®"""
        print(f"\nğŸ¯ æ€§èƒ½åŸºçº¿å»ºè®®:")
        print(f"   âœ… å¥åº·æ£€æŸ¥å“åº”æ—¶é—´ < 50ms")
        print(f"   âœ… APIå“åº”æ—¶é—´ < 200ms (P95)")
        print(f"   âœ… æˆåŠŸç‡ > 99%")
        print(f"   âœ… QPS > 50 (å¥åº·æ£€æŸ¥)")
        print(f"   âœ… ç³»ç»ŸCPUä½¿ç”¨ç‡ < 80%")
        print(f"   âœ… ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡ < 85%")
    
    def save_json_report(self, file_path: str):
        """ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MarketPrism æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·")
    print("Phase 4: ä¼˜åŒ–ä¸éƒ¨ç½² - æ€§èƒ½éªŒè¯")
    print("-"*50)
    
    # æœåŠ¡é…ç½®
    services_config = {
        'api-gateway-service': {'host': 'localhost', 'port': 8080},
        'monitoring-service': {'host': 'localhost', 'port': 8083},
        'data-storage-service': {'host': 'localhost', 'port': 8082},
        'market-data-collector': {'host': 'localhost', 'port': 8081},
        'message-broker-service': {'host': 'localhost', 'port': 8085},
        'scheduler-service': {'host': 'localhost', 'port': 8084}
    }
    
    # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = SystemBenchmark(services_config)
    
    try:
        print("â±ï¸  å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆé¢„è®¡éœ€è¦2-3åˆ†é’Ÿï¼‰...")
        results = await benchmark.run_comprehensive_benchmark()
        
        # ç”ŸæˆæŠ¥å‘Š
        reporter = BenchmarkReporter(results)
        reporter.generate_console_report()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = project_root / "tests" / "reports" / "performance" / "benchmark_report.json"
        report_file.parent.mkdir(parents=True, exist_ok=True)
        reporter.save_json_report(str(report_file))
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ€§èƒ½æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())