#!/usr/bin/env python3
"""
ç”Ÿäº§çº§åˆ«å®Œæ•´æ•°æ®æµæµ‹è¯•
çœŸå®äº¤æ˜“æ‰€WebSocket â†’ NATS â†’ ClickHouse â†’ å†·å­˜å‚¨

è¿™æ˜¯ä¸€ä¸ªæ¥è¿‘å®ç›˜çš„å®Œæ•´æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
- çœŸå®çš„é«˜é¢‘WebSocketæ•°æ®æµ
- å®Œæ•´çš„NATSæ¶ˆæ¯å¤„ç†
- çœŸå®çš„ClickHouseæ•°æ®å­˜å‚¨
- å®é™…çš„å†·å­˜å‚¨å½’æ¡£
- çœŸå®çš„å¸‚åœºæ•°æ®å‹åŠ›

ä½¿ç”¨æ–¹æ³•:
    source scripts/proxy_config.sh
    python scripts/production_level_test.py --duration 300 --symbols BTC/USDT,ETH/USDT,BNB/USDT
"""

import asyncio
import json
import time
import logging
import os
import sys
import argparse
import signal
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Set
import aiohttp
import nats
import clickhouse_connect
import websockets
from concurrent.futures import ThreadPoolExecutor
import threading
import queue
import gzip
import pickle

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from config.app_config import AppConfig, NetworkConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('production_test.log')
    ]
)
logger = logging.getLogger(__name__)

class ProductionLevelDataFlowTest:
    """ç”Ÿäº§çº§åˆ«æ•°æ®æµæµ‹è¯•å™¨"""
    
    def __init__(self, test_duration: int = 300, symbols: List[str] = None):
        self.test_duration = test_duration
        self.symbols = symbols or ['BTC/USDT', 'ETH/USDT', 'BNB/USDT']
        self.running = False
        self.start_time = None
        
        # è¿æ¥å’Œå®¢æˆ·ç«¯
        self.nats_client = None
        self.clickhouse_client = None
        self.websocket_connections = {}
        self.proxy = None
        
        # æ•°æ®ç»Ÿè®¡ (ç”Ÿäº§çº§åˆ«çš„è¯¦ç»†ç»Ÿè®¡)
        self.production_stats = {
            'start_time': None,
            'end_time': None,
            'test_duration_seconds': test_duration,
            
            # WebSocket è¿æ¥ç»Ÿè®¡
            'websocket_connections': 0,
            'websocket_reconnections': 0,
            'websocket_errors': 0,
            'websocket_messages_total': 0,
            'websocket_bytes_received': 0,
            
            # æ•°æ®ç±»å‹ç»Ÿè®¡
            'trade_messages': 0,
            'orderbook_messages': 0,
            'ticker_messages': 0,
            'other_messages': 0,
            
            # NATS æ¶ˆæ¯ç»Ÿè®¡
            'nats_published': 0,
            'nats_publish_errors': 0,
            'nats_ack_received': 0,
            'nats_bytes_published': 0,
            
            # æ•°æ®åº“ç»Ÿè®¡
            'clickhouse_inserts': 0,
            'clickhouse_rows_inserted': 0,
            'clickhouse_insert_errors': 0,
            'clickhouse_query_time_total': 0.0,
            
            # æ€§èƒ½ç»Ÿè®¡
            'max_message_rate_per_second': 0,
            'avg_message_rate_per_second': 0,
            'max_latency_ms': 0,
            'avg_latency_ms': 0,
            'memory_usage_mb': 0,
            
            # æ•°æ®è´¨é‡ç»Ÿè®¡
            'data_gaps_detected': 0,
            'duplicate_messages': 0,
            'invalid_messages': 0,
            'price_anomalies': 0,
            
            # å„äº¤æ˜“å¯¹ç»Ÿè®¡
            'symbol_stats': {},
            
            # é”™è¯¯è¯¦æƒ…
            'errors': []
        }
        
        # æ•°æ®ç¼“å­˜å’Œå¤„ç†é˜Ÿåˆ—
        self.message_queue = queue.Queue(maxsize=10000)
        self.db_batch_queue = queue.Queue(maxsize=1000)
        self.processed_message_ids = set()  # é˜²é‡å¤
        self.last_message_time = {}  # å»¶è¿Ÿæ£€æµ‹
        self.price_history = {}  # ä»·æ ¼å¼‚å¸¸æ£€æµ‹
        
        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        # ä¼˜é›…é€€å‡º
        self.shutdown_event = threading.Event()
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡ä¼˜é›…é€€å‡º...")
        self.shutdown_event.set()
        self.running = False
    
    async def setup_infrastructure(self):
        """è®¾ç½®åŸºç¡€è®¾æ–½è¿æ¥"""
        logger.info("ğŸš€ è®¾ç½®ç”Ÿäº§çº§åˆ«åŸºç¡€è®¾æ–½...")
        
        try:
            # è®¾ç½®ä»£ç†
            self.proxy = os.getenv('HTTP_PROXY') or os.getenv('HTTPS_PROXY')
            if self.proxy:
                logger.info(f"ğŸ”§ ä½¿ç”¨ä»£ç†: {self.proxy}")
            
            # è¿æ¥NATS (å¯ç”¨JetStream)
            nats_url = os.getenv('NATS_URL', 'nats://localhost:4222')
            self.nats_client = await nats.connect(
                nats_url,
                max_reconnect_attempts=10,
                reconnect_time_wait=2,
                allow_reconnect=True
            )
            
            # åˆ›å»ºJetStreamä¸Šä¸‹æ–‡
            self.js = self.nats_client.jetstream()
            
            # åˆ›å»ºæµ (å¦‚æœä¸å­˜åœ¨)
            try:
                await self.js.stream_info("MARKET_DATA")
            except:
                await self.js.add_stream(name="MARKET_DATA", subjects=["market.>"])
            
            logger.info(f"âœ… NATS JetStreamè¿æ¥æˆåŠŸ: {nats_url}")
            
            # è¿æ¥ClickHouse
            self.clickhouse_client = clickhouse_connect.get_client(
                host=os.getenv('CLICKHOUSE_HOST', 'localhost'),
                port=int(os.getenv('CLICKHOUSE_PORT', '8123')),
                username='default',
                password='',
                connect_timeout=30,
                send_receive_timeout=60
            )
            
            # åˆ›å»ºç”Ÿäº§çº§åˆ«çš„è¡¨ç»“æ„
            await self.create_production_tables()
            
            logger.info("âœ… ClickHouseè¿æ¥æˆåŠŸï¼Œç”Ÿäº§è¡¨å·²åˆ›å»º")
            
            self.production_stats['start_time'] = datetime.now()
            
        except Exception as e:
            logger.error(f"âŒ åŸºç¡€è®¾æ–½è®¾ç½®å¤±è´¥: {e}")
            raise
    
    async def create_production_tables(self):
        """åˆ›å»ºç”Ÿäº§çº§åˆ«çš„æ•°æ®è¡¨"""
        # é«˜æ€§èƒ½çš„äº¤æ˜“æ•°æ®è¡¨
        trades_table = """
        CREATE TABLE IF NOT EXISTS production_trades (
            timestamp DateTime64(3),
            exchange LowCardinality(String),
            symbol LowCardinality(String),
            trade_id String,
            price Decimal64(8),
            quantity Decimal64(8),
            quote_quantity Decimal64(8),
            side LowCardinality(String),
            recv_timestamp DateTime64(3) DEFAULT now64(3),
            INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 8192,
            INDEX idx_symbol symbol TYPE set(100) GRANULARITY 8192
        ) ENGINE = MergeTree()
        PARTITION BY toYYYYMMDD(timestamp)
        ORDER BY (exchange, symbol, timestamp, trade_id)
        TTL timestamp + INTERVAL 30 DAY
        SETTINGS index_granularity = 8192
        """
        
        # é«˜æ€§èƒ½çš„è®¢å•ç°¿æ•°æ®è¡¨  
        orderbook_table = """
        CREATE TABLE IF NOT EXISTS production_orderbook (
            timestamp DateTime64(3),
            exchange LowCardinality(String),
            symbol LowCardinality(String),
            side LowCardinality(String),
            price Decimal64(8),
            quantity Decimal64(8),
            level UInt16,
            recv_timestamp DateTime64(3) DEFAULT now64(3),
            INDEX idx_timestamp timestamp TYPE minmax GRANULARITY 8192,
            INDEX idx_symbol symbol TYPE set(100) GRANULARITY 8192
        ) ENGINE = MergeTree()
        PARTITION BY toYYYYMMDD(timestamp)
        ORDER BY (exchange, symbol, timestamp, side, level)
        TTL timestamp + INTERVAL 7 DAY
        SETTINGS index_granularity = 8192
        """
        
        # æµ‹è¯•ç»Ÿè®¡è¡¨
        test_stats_table = """
        CREATE TABLE IF NOT EXISTS production_test_stats (
            test_id String,
            start_time DateTime64(3),
            end_time DateTime64(3),
            duration_seconds UInt32,
            messages_processed UInt64,
            avg_rate_per_second Float64,
            max_rate_per_second Float64,
            errors_count UInt32,
            success_rate Float64,
            stats_json String
        ) ENGINE = MergeTree()
        ORDER BY start_time
        TTL start_time + INTERVAL 90 DAY
        """
        
        # æ‰§è¡Œè¡¨åˆ›å»º
        tables = [trades_table, orderbook_table, test_stats_table]
        for table_sql in tables:
            try:
                self.clickhouse_client.command(table_sql)
                logger.info(f"âœ… è¡¨åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ è¡¨åˆ›å»ºå¤±è´¥: {e}")
                raise
    
    async def start_real_websocket_streams(self):
        """å¯åŠ¨çœŸå®çš„WebSocketæ•°æ®æµ"""
        logger.info("ğŸ”Œ å¯åŠ¨çœŸå®WebSocketæ•°æ®æµ...")
        
        # ä¸ºæ¯ä¸ªäº¤æ˜“å¯¹åˆ›å»ºè¿æ¥
        for symbol in self.symbols:
            try:
                asyncio.create_task(self._connect_binance_websocket(symbol))
                await asyncio.sleep(1)  # é¿å…åŒæ—¶è¿æ¥è¿‡å¤š
                self.production_stats['websocket_connections'] += 1
            except Exception as e:
                logger.error(f"âŒ å¯åŠ¨{symbol}çš„WebSocketå¤±è´¥: {e}")
                self.production_stats['websocket_errors'] += 1
    
    async def _connect_binance_websocket(self, symbol: str):
        """è¿æ¥Binance WebSocket (çœŸå®ç”Ÿäº§çº§åˆ«)"""
        binance_symbol = symbol.replace('/', '').lower()  # BTC/USDT -> btcusdt
        
        # ç”Ÿäº§çº§åˆ«çš„è®¢é˜…ï¼šäº¤æ˜“æ•°æ® + æ·±åº¦æ•°æ® + Ticker
        streams = [
            f"{binance_symbol}@trade",
            f"{binance_symbol}@depth@100ms",
            f"{binance_symbol}@ticker"
        ]
        
        ws_url = f"wss://stream.binance.com:9443/stream?streams={'/'.join(streams)}"
        
        retry_count = 0
        max_retries = 10
        
        while self.running and retry_count < max_retries:
            try:
                logger.info(f"ğŸ“¡ è¿æ¥{symbol}çš„Binance WebSocket (å°è¯• {retry_count + 1}/{max_retries})")
                
                # ç”Ÿäº§çº§åˆ«çš„WebSocketè¿æ¥
                async with websockets.connect(
                    ws_url,
                    ping_interval=20,
                    ping_timeout=10,
                    close_timeout=10,
                    max_size=10**7,  # 10MB æ¶ˆæ¯é™åˆ¶
                    compression=None  # ç¦ç”¨å‹ç¼©ä»¥æé«˜æ€§èƒ½
                ) as websocket:
                    
                    logger.info(f"âœ… {symbol} WebSocketè¿æ¥æˆåŠŸ")
                    self.websocket_connections[symbol] = websocket
                    
                    # é‡ç½®é‡è¯•è®¡æ•°
                    retry_count = 0
                    
                    # æ¥æ”¶æ•°æ®å¾ªç¯
                    async for message in websocket:
                        if not self.running:
                            break
                        
                        try:
                            # è®°å½•æ¥æ”¶ç»Ÿè®¡
                            self.production_stats['websocket_messages_total'] += 1
                            self.production_stats['websocket_bytes_received'] += len(message)
                            
                            # è§£ææ¶ˆæ¯
                            data = json.loads(message)
                            
                            # å¤„ç†æ¶ˆæ¯
                            await self._process_websocket_message(symbol, data)
                            
                            # æ›´æ–°é¢‘ç‡ç»Ÿè®¡
                            await self._update_rate_stats()
                            
                        except json.JSONDecodeError:
                            self.production_stats['invalid_messages'] += 1
                            logger.warning(f"âš ï¸ {symbol} JSONè§£æå¤±è´¥")
                        except Exception as e:
                            logger.error(f"âŒ {symbol} æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
                            self.production_stats['websocket_errors'] += 1
                            
            except websockets.exceptions.ConnectionClosed:
                logger.warning(f"âš ï¸ {symbol} WebSocketè¿æ¥å…³é—­ï¼Œå‡†å¤‡é‡è¿...")
                retry_count += 1
                self.production_stats['websocket_reconnections'] += 1
                await asyncio.sleep(min(retry_count * 2, 30))  # æŒ‡æ•°é€€é¿
                
            except Exception as e:
                logger.error(f"âŒ {symbol} WebSocketè¿æ¥å¼‚å¸¸: {e}")
                retry_count += 1
                self.production_stats['websocket_errors'] += 1
                await asyncio.sleep(min(retry_count * 2, 30))
        
        if retry_count >= max_retries:
            logger.error(f"âŒ {symbol} WebSocketé‡è¿æ¬¡æ•°è¿‡å¤šï¼Œæ”¾å¼ƒè¿æ¥")
    
    async def _process_websocket_message(self, symbol: str, data: Dict[str, Any]):
        """å¤„ç†WebSocketæ¶ˆæ¯ (ç”Ÿäº§çº§åˆ«)"""
        try:
            # è®°å½•æ¥æ”¶æ—¶é—´ (å»¶è¿Ÿåˆ†æ)
            recv_time = time.time() * 1000
            
            # è§£ææ¶ˆæ¯ç±»å‹
            if 'stream' in data and 'data' in data:
                stream = data['stream']
                message_data = data['data']
                
                # ç”Ÿæˆæ¶ˆæ¯ID (é˜²é‡å¤)
                if '@trade' in stream:
                    msg_id = f"trade_{symbol}_{message_data.get('t', recv_time)}"
                    if msg_id not in self.processed_message_ids:
                        self.processed_message_ids.add(msg_id)
                        await self._process_trade_message(symbol, message_data, recv_time)
                        self.production_stats['trade_messages'] += 1
                    else:
                        self.production_stats['duplicate_messages'] += 1
                
                elif '@depth' in stream:
                    msg_id = f"depth_{symbol}_{message_data.get('u', recv_time)}"
                    if msg_id not in self.processed_message_ids:
                        self.processed_message_ids.add(msg_id)
                        await self._process_orderbook_message(symbol, message_data, recv_time)
                        self.production_stats['orderbook_messages'] += 1
                    else:
                        self.production_stats['duplicate_messages'] += 1
                
                elif '@ticker' in stream:
                    await self._process_ticker_message(symbol, message_data, recv_time)
                    self.production_stats['ticker_messages'] += 1
                
                else:
                    self.production_stats['other_messages'] += 1
            
            # æ›´æ–°å»¶è¿Ÿç»Ÿè®¡
            if symbol not in self.last_message_time:
                self.last_message_time[symbol] = recv_time
            else:
                gap = recv_time - self.last_message_time[symbol]
                if gap > 5000:  # 5ç§’é—´éš”è¡¨ç¤ºæ•°æ®ä¸­æ–­
                    self.production_stats['data_gaps_detected'] += 1
                    logger.warning(f"âš ï¸ {symbol} æ•°æ®ä¸­æ–­ {gap:.1f}ms")
                
                self.last_message_time[symbol] = recv_time
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†{symbol}æ¶ˆæ¯å¤±è´¥: {e}")
            self.production_stats['websocket_errors'] += 1
    
    async def _process_trade_message(self, symbol: str, data: Dict[str, Any], recv_time: float):
        """å¤„ç†äº¤æ˜“æ¶ˆæ¯ (ç”Ÿäº§çº§åˆ«)"""
        try:
            # æå–äº¤æ˜“æ•°æ®
            trade_data = {
                'timestamp': datetime.fromtimestamp(data['T'] / 1000),
                'exchange': 'binance',
                'symbol': symbol,
                'trade_id': str(data['t']),
                'price': float(data['p']),
                'quantity': float(data['q']),
                'quote_quantity': float(data['p']) * float(data['q']),
                'side': 'sell' if data['m'] else 'buy',
                'recv_timestamp': datetime.fromtimestamp(recv_time / 1000)
            }
            
            # ä»·æ ¼å¼‚å¸¸æ£€æµ‹
            self._detect_price_anomaly(symbol, trade_data['price'])
            
            # å‘å¸ƒåˆ°NATS (ç”Ÿäº§çº§åˆ«)
            subject = f"market.trades.binance.{symbol.replace('/', '_')}"
            message = json.dumps(trade_data, default=str).encode()
            
            try:
                ack = await self.js.publish(subject, message)
                self.production_stats['nats_published'] += 1
                self.production_stats['nats_bytes_published'] += len(message)
                self.production_stats['nats_ack_received'] += 1
            except Exception as e:
                logger.error(f"âŒ NATSå‘å¸ƒå¤±è´¥: {e}")
                self.production_stats['nats_publish_errors'] += 1
            
            # æ‰¹é‡å†™å…¥ClickHouse
            self.db_batch_queue.put(('trades', trade_data))
            
            # æ›´æ–°ç¬¦å·ç»Ÿè®¡
            if symbol not in self.production_stats['symbol_stats']:
                self.production_stats['symbol_stats'][symbol] = {'trades': 0, 'orderbooks': 0}
            self.production_stats['symbol_stats'][symbol]['trades'] += 1
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†äº¤æ˜“æ•°æ®å¤±è´¥: {e}")
            self.production_stats['invalid_messages'] += 1
    
    async def _process_orderbook_message(self, symbol: str, data: Dict[str, Any], recv_time: float):
        """å¤„ç†è®¢å•ç°¿æ¶ˆæ¯ (ç”Ÿäº§çº§åˆ«)"""
        try:
            timestamp = datetime.fromtimestamp(recv_time / 1000)
            
            # å¤„ç†ä¹°å•
            for i, bid in enumerate(data.get('b', [])[:10]):  # åªå–å‰10æ¡£
                orderbook_data = {
                    'timestamp': timestamp,
                    'exchange': 'binance',
                    'symbol': symbol,
                    'side': 'buy',
                    'price': float(bid[0]),
                    'quantity': float(bid[1]),
                    'level': i + 1,
                    'recv_timestamp': timestamp
                }
                self.db_batch_queue.put(('orderbook', orderbook_data))
            
            # å¤„ç†å–å•
            for i, ask in enumerate(data.get('a', [])[:10]):  # åªå–å‰10æ¡£
                orderbook_data = {
                    'timestamp': timestamp,
                    'exchange': 'binance',
                    'symbol': symbol,
                    'side': 'sell',
                    'price': float(ask[0]),
                    'quantity': float(ask[1]),
                    'level': i + 1,
                    'recv_timestamp': timestamp
                }
                self.db_batch_queue.put(('orderbook', orderbook_data))
            
            # å‘å¸ƒåˆ°NATS
            subject = f"market.orderbook.binance.{symbol.replace('/', '_')}"
            message = json.dumps({
                'symbol': symbol,
                'bids': data.get('b', [])[:10],
                'asks': data.get('a', [])[:10],
                'timestamp': timestamp.isoformat()
            }).encode()
            
            try:
                await self.js.publish(subject, message)
                self.production_stats['nats_published'] += 1
                self.production_stats['nats_bytes_published'] += len(message)
            except Exception as e:
                self.production_stats['nats_publish_errors'] += 1
            
            # æ›´æ–°ç¬¦å·ç»Ÿè®¡
            if symbol not in self.production_stats['symbol_stats']:
                self.production_stats['symbol_stats'][symbol] = {'trades': 0, 'orderbooks': 0}
            self.production_stats['symbol_stats'][symbol]['orderbooks'] += 1
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è®¢å•ç°¿æ•°æ®å¤±è´¥: {e}")
            self.production_stats['invalid_messages'] += 1
    
    async def _process_ticker_message(self, symbol: str, data: Dict[str, Any], recv_time: float):
        """å¤„ç†Tickeræ¶ˆæ¯"""
        # å‘å¸ƒtickeræ•°æ®åˆ°NATS
        subject = f"market.ticker.binance.{symbol.replace('/', '_')}"
        message = json.dumps({
            'symbol': symbol,
            'price': data.get('c'),
            'change_24h': data.get('P'),
            'volume_24h': data.get('v'),
            'timestamp': datetime.fromtimestamp(recv_time / 1000).isoformat()
        }).encode()
        
        try:
            await self.js.publish(subject, message)
            self.production_stats['nats_published'] += 1
        except Exception as e:
            self.production_stats['nats_publish_errors'] += 1
    
    def _detect_price_anomaly(self, symbol: str, price: float):
        """æ£€æµ‹ä»·æ ¼å¼‚å¸¸"""
        if symbol not in self.price_history:
            self.price_history[symbol] = []
        
        # ä¿ç•™æœ€è¿‘10ä¸ªä»·æ ¼
        self.price_history[symbol].append(price)
        if len(self.price_history[symbol]) > 10:
            self.price_history[symbol].pop(0)
        
        # ç®€å•çš„å¼‚å¸¸æ£€æµ‹ï¼šä»·æ ¼å˜åŒ–è¶…è¿‡10%
        if len(self.price_history[symbol]) >= 2:
            prev_price = self.price_history[symbol][-2]
            if abs(price - prev_price) / prev_price > 0.1:  # 10%å˜åŒ–
                self.production_stats['price_anomalies'] += 1
                logger.warning(f"âš ï¸ {symbol} ä»·æ ¼å¼‚å¸¸: {prev_price} -> {price}")
    
    async def _update_rate_stats(self):
        """æ›´æ–°é¢‘ç‡ç»Ÿè®¡"""
        current_time = time.time()
        if not hasattr(self, '_last_rate_update'):
            self._last_rate_update = current_time
            self._messages_in_window = 0
        
        self._messages_in_window += 1
        
        # æ¯ç§’æ›´æ–°ä¸€æ¬¡ç»Ÿè®¡
        if current_time - self._last_rate_update >= 1.0:
            rate = self._messages_in_window / (current_time - self._last_rate_update)
            
            if rate > self.production_stats['max_message_rate_per_second']:
                self.production_stats['max_message_rate_per_second'] = rate
            
            # æ›´æ–°å¹³å‡å€¼
            total_messages = self.production_stats['websocket_messages_total']
            if total_messages > 0 and self.production_stats['start_time']:
                elapsed = (datetime.now() - self.production_stats['start_time']).total_seconds()
                if elapsed > 0:
                    self.production_stats['avg_message_rate_per_second'] = total_messages / elapsed
            
            self._last_rate_update = current_time
            self._messages_in_window = 0
    
    async def start_database_batch_writer(self):
        """å¯åŠ¨æ•°æ®åº“æ‰¹é‡å†™å…¥å™¨"""
        logger.info("ğŸ’¾ å¯åŠ¨ç”Ÿäº§çº§åˆ«æ•°æ®åº“æ‰¹é‡å†™å…¥å™¨...")
        
        async def batch_writer():
            batch_size = 1000
            batch_timeout = 5.0  # 5ç§’è¶…æ—¶
            
            trades_batch = []
            orderbook_batch = []
            last_write = time.time()
            
            while self.running or not self.db_batch_queue.empty():
                try:
                    # è·å–æ•°æ® (éé˜»å¡)
                    try:
                        table_type, data = self.db_batch_queue.get_nowait()
                        
                        if table_type == 'trades':
                            trades_batch.append(data)
                        elif table_type == 'orderbook':
                            orderbook_batch.append(data)
                        
                    except queue.Empty:
                        await asyncio.sleep(0.1)
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å†™å…¥
                    should_write = (
                        len(trades_batch) >= batch_size or
                        len(orderbook_batch) >= batch_size or
                        time.time() - last_write > batch_timeout
                    )
                    
                    if should_write:
                        # å†™å…¥äº¤æ˜“æ•°æ®
                        if trades_batch:
                            start_time = time.time()
                            try:
                                self.clickhouse_client.insert('production_trades', trades_batch)
                                self.production_stats['clickhouse_inserts'] += 1
                                self.production_stats['clickhouse_rows_inserted'] += len(trades_batch)
                                self.production_stats['clickhouse_query_time_total'] += time.time() - start_time
                                logger.info(f"ğŸ’¾ å†™å…¥ {len(trades_batch)} æ¡äº¤æ˜“æ•°æ®")
                                trades_batch = []
                            except Exception as e:
                                logger.error(f"âŒ äº¤æ˜“æ•°æ®å†™å…¥å¤±è´¥: {e}")
                                self.production_stats['clickhouse_insert_errors'] += 1
                                trades_batch = []  # æ¸…ç©ºé¿å…é‡å¤é”™è¯¯
                        
                        # å†™å…¥è®¢å•ç°¿æ•°æ®
                        if orderbook_batch:
                            start_time = time.time()
                            try:
                                self.clickhouse_client.insert('production_orderbook', orderbook_batch)
                                self.production_stats['clickhouse_inserts'] += 1
                                self.production_stats['clickhouse_rows_inserted'] += len(orderbook_batch)
                                self.production_stats['clickhouse_query_time_total'] += time.time() - start_time
                                logger.info(f"ğŸ’¾ å†™å…¥ {len(orderbook_batch)} æ¡è®¢å•ç°¿æ•°æ®")
                                orderbook_batch = []
                            except Exception as e:
                                logger.error(f"âŒ è®¢å•ç°¿æ•°æ®å†™å…¥å¤±è´¥: {e}")
                                self.production_stats['clickhouse_insert_errors'] += 1
                                orderbook_batch = []
                        
                        last_write = time.time()
                
                except Exception as e:
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥å™¨å¼‚å¸¸: {e}")
                    await asyncio.sleep(1)
        
        # å¯åŠ¨æ‰¹é‡å†™å…¥ä»»åŠ¡
        asyncio.create_task(batch_writer())
    
    async def run_production_test(self):
        """è¿è¡Œç”Ÿäº§çº§åˆ«æµ‹è¯•"""
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿäº§çº§åˆ«æ•°æ®æµæµ‹è¯• (æŒç»­{self.test_duration}ç§’)")
        logger.info(f"ğŸ“Š ç›‘æ§äº¤æ˜“å¯¹: {', '.join(self.symbols)}")
        
        self.running = True
        test_id = f"prod_test_{int(time.time())}"
        
        try:
            # è®¾ç½®åŸºç¡€è®¾æ–½
            await self.setup_infrastructure()
            
            # å¯åŠ¨æ•°æ®åº“æ‰¹é‡å†™å…¥å™¨
            await self.start_database_batch_writer()
            
            # å¯åŠ¨WebSocketæ•°æ®æµ
            await self.start_real_websocket_streams()
            
            logger.info(f"âœ… æ‰€æœ‰ç»„ä»¶å¯åŠ¨å®Œæˆï¼Œå¼€å§‹ {self.test_duration} ç§’çš„ç”Ÿäº§çº§åˆ«æµ‹è¯•...")
            
            # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            start_time = time.time()
            last_stats_time = start_time
            
            while self.running and (time.time() - start_time) < self.test_duration:
                if not self.shutdown_event.is_set():
                    await asyncio.sleep(1)
                    
                    # æ¯30ç§’è¾“å‡ºä¸€æ¬¡ç»Ÿè®¡
                    if time.time() - last_stats_time >= 30:
                        await self._print_realtime_stats()
                        last_stats_time = time.time()
                else:
                    logger.info("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æµ‹è¯•...")
                    break
            
            logger.info("â° æµ‹è¯•æ—¶é—´ç»“æŸï¼Œæ­£åœ¨æ”¶é›†æœ€ç»ˆç»Ÿè®¡...")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿäº§æµ‹è¯•å¼‚å¸¸: {e}")
            self.production_stats['errors'].append(str(e))
        
        finally:
            self.running = False
            self.production_stats['end_time'] = datetime.now()
            
            # ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ
            logger.info("â³ ç­‰å¾…æ•°æ®å†™å…¥å®Œæˆ...")
            await asyncio.sleep(10)
            
            # ä¿å­˜æµ‹è¯•ç»Ÿè®¡
            await self._save_test_stats(test_id)
            
            # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
            await self._generate_final_report()
            
            # æ¸…ç†è¿æ¥
            await self._cleanup()
    
    async def _print_realtime_stats(self):
        """æ‰“å°å®æ—¶ç»Ÿè®¡"""
        elapsed = (datetime.now() - self.production_stats['start_time']).total_seconds()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“Š ç”Ÿäº§çº§åˆ«å®æ—¶ç»Ÿè®¡ (å·²è¿è¡Œ {elapsed:.0f}ç§’)")
        logger.info(f"{'='*80}")
        logger.info(f"WebSocketè¿æ¥: {self.production_stats['websocket_connections']}")
        logger.info(f"æ€»æ¶ˆæ¯æ•°: {self.production_stats['websocket_messages_total']}")
        logger.info(f"æ¶ˆæ¯é¢‘ç‡: {self.production_stats['avg_message_rate_per_second']:.1f} msg/s")
        logger.info(f"äº¤æ˜“æ¶ˆæ¯: {self.production_stats['trade_messages']}")
        logger.info(f"è®¢å•ç°¿æ¶ˆæ¯: {self.production_stats['orderbook_messages']}")
        logger.info(f"NATSå‘å¸ƒ: {self.production_stats['nats_published']}")
        logger.info(f"æ•°æ®åº“å†™å…¥: {self.production_stats['clickhouse_rows_inserted']} è¡Œ")
        logger.info(f"é”™è¯¯æ•°: {len(self.production_stats['errors'])}")
        
        # å„äº¤æ˜“å¯¹ç»Ÿè®¡
        logger.info("äº¤æ˜“å¯¹ç»Ÿè®¡:")
        for symbol, stats in self.production_stats['symbol_stats'].items():
            logger.info(f"  {symbol}: äº¤æ˜“={stats['trades']}, è®¢å•ç°¿={stats['orderbooks']}")
    
    async def _save_test_stats(self, test_id: str):
        """ä¿å­˜æµ‹è¯•ç»Ÿè®¡åˆ°æ•°æ®åº“"""
        try:
            duration = (self.production_stats['end_time'] - self.production_stats['start_time']).total_seconds()
            
            stats_record = {
                'test_id': test_id,
                'start_time': self.production_stats['start_time'],
                'end_time': self.production_stats['end_time'],
                'duration_seconds': int(duration),
                'messages_processed': self.production_stats['websocket_messages_total'],
                'avg_rate_per_second': self.production_stats['avg_message_rate_per_second'],
                'max_rate_per_second': self.production_stats['max_message_rate_per_second'],
                'errors_count': len(self.production_stats['errors']),
                'success_rate': (1 - len(self.production_stats['errors']) / max(self.production_stats['websocket_messages_total'], 1)) * 100,
                'stats_json': json.dumps(self.production_stats, default=str)
            }
            
            self.clickhouse_client.insert('production_test_stats', [stats_record])
            logger.info(f"âœ… æµ‹è¯•ç»Ÿè®¡å·²ä¿å­˜: {test_id}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æµ‹è¯•ç»Ÿè®¡å¤±è´¥: {e}")
    
    async def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info(f"\n{'='*100}")
        logger.info(f"ğŸ“ˆ MarketPrism ç”Ÿäº§çº§åˆ«æ•°æ®æµæµ‹è¯•æœ€ç»ˆæŠ¥å‘Š")
        logger.info(f"{'='*100}")
        
        duration = (self.production_stats['end_time'] - self.production_stats['start_time']).total_seconds()
        
        logger.info(f"ğŸ• æµ‹è¯•æ—¶é—´: {self.production_stats['start_time']} åˆ° {self.production_stats['end_time']}")
        logger.info(f"â±ï¸  æŒç»­æ—¶é—´: {duration:.1f} ç§’")
        logger.info(f"ğŸ¯ ç›®æ ‡äº¤æ˜“å¯¹: {', '.join(self.symbols)}")
        logger.info(f"ğŸ”§ ä»£ç†é…ç½®: {'æ˜¯' if self.proxy else 'å¦'}")
        logger.info("")
        
        logger.info("ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  æ€»æ¶ˆæ¯æ•°: {self.production_stats['websocket_messages_total']:,}")
        logger.info(f"  å¹³å‡é¢‘ç‡: {self.production_stats['avg_message_rate_per_second']:.1f} æ¶ˆæ¯/ç§’")
        logger.info(f"  å³°å€¼é¢‘ç‡: {self.production_stats['max_message_rate_per_second']:.1f} æ¶ˆæ¯/ç§’")
        logger.info(f"  äº¤æ˜“æ¶ˆæ¯: {self.production_stats['trade_messages']:,}")
        logger.info(f"  è®¢å•ç°¿æ¶ˆæ¯: {self.production_stats['orderbook_messages']:,}")
        logger.info(f"  Tickeræ¶ˆæ¯: {self.production_stats['ticker_messages']:,}")
        logger.info(f"  æ•°æ®æ€»é‡: {self.production_stats['websocket_bytes_received']/1024/1024:.1f} MB")
        logger.info("")
        
        logger.info("ğŸ”Œ WebSocketè¿æ¥:")
        logger.info(f"  æˆåŠŸè¿æ¥: {self.production_stats['websocket_connections']}")
        logger.info(f"  é‡è¿æ¬¡æ•°: {self.production_stats['websocket_reconnections']}")
        logger.info(f"  è¿æ¥é”™è¯¯: {self.production_stats['websocket_errors']}")
        logger.info("")
        
        logger.info("ğŸ“¤ NATSæ¶ˆæ¯é˜Ÿåˆ—:")
        logger.info(f"  å‘å¸ƒæ¶ˆæ¯: {self.production_stats['nats_published']:,}")
        logger.info(f"  å‘å¸ƒæ•°æ®é‡: {self.production_stats['nats_bytes_published']/1024/1024:.1f} MB")
        logger.info(f"  ç¡®è®¤æ¥æ”¶: {self.production_stats['nats_ack_received']:,}")
        logger.info(f"  å‘å¸ƒé”™è¯¯: {self.production_stats['nats_publish_errors']}")
        logger.info("")
        
        logger.info("ğŸ’¾ ClickHouseæ•°æ®åº“:")
        logger.info(f"  å†™å…¥æ‰¹æ¬¡: {self.production_stats['clickhouse_inserts']}")
        logger.info(f"  å†™å…¥è¡Œæ•°: {self.production_stats['clickhouse_rows_inserted']:,}")
        logger.info(f"  å†™å…¥é”™è¯¯: {self.production_stats['clickhouse_insert_errors']}")
        avg_query_time = self.production_stats['clickhouse_query_time_total'] / max(self.production_stats['clickhouse_inserts'], 1)
        logger.info(f"  å¹³å‡å†™å…¥æ—¶é—´: {avg_query_time:.3f} ç§’")
        logger.info("")
        
        logger.info("ğŸ›ï¸ æ•°æ®è´¨é‡:")
        logger.info(f"  é‡å¤æ¶ˆæ¯: {self.production_stats['duplicate_messages']}")
        logger.info(f"  æ— æ•ˆæ¶ˆæ¯: {self.production_stats['invalid_messages']}")
        logger.info(f"  æ•°æ®ä¸­æ–­: {self.production_stats['data_gaps_detected']}")
        logger.info(f"  ä»·æ ¼å¼‚å¸¸: {self.production_stats['price_anomalies']}")
        logger.info("")
        
        logger.info("ğŸ“ˆ å„äº¤æ˜“å¯¹è¡¨ç°:")
        for symbol, stats in self.production_stats['symbol_stats'].items():
            total = stats['trades'] + stats['orderbooks']
            rate = total / duration if duration > 0 else 0
            logger.info(f"  {symbol}: {total:,} æ¡æ¶ˆæ¯ ({rate:.1f} msg/s)")
            logger.info(f"    - äº¤æ˜“: {stats['trades']:,}")
            logger.info(f"    - è®¢å•ç°¿: {stats['orderbooks']:,}")
        logger.info("")
        
        # è®¡ç®—æˆåŠŸç‡
        total_errors = (self.production_stats['websocket_errors'] + 
                       self.production_stats['nats_publish_errors'] + 
                       self.production_stats['clickhouse_insert_errors'])
        
        success_rate = ((self.production_stats['websocket_messages_total'] - total_errors) / 
                       max(self.production_stats['websocket_messages_total'], 1)) * 100
        
        logger.info("ğŸ† æ•´ä½“è¯„ä¼°:")
        logger.info(f"  æˆåŠŸç‡: {success_rate:.2f}%")
        logger.info(f"  æ•°æ®å®Œæ•´æ€§: {((self.production_stats['websocket_messages_total'] - self.production_stats['invalid_messages']) / max(self.production_stats['websocket_messages_total'], 1)) * 100:.2f}%")
        logger.info(f"  ç³»ç»Ÿç¨³å®šæ€§: {((self.production_stats['websocket_messages_total'] - self.production_stats['data_gaps_detected']) / max(self.production_stats['websocket_messages_total'], 1)) * 100:.2f}%")
        
        if success_rate >= 95:
            logger.info("ğŸ‰ ç”Ÿäº§çº§åˆ«æµ‹è¯•ä¼˜ç§€ï¼Œç³»ç»Ÿå·²è¾¾åˆ°ä¼ä¸šçº§æ ‡å‡†ï¼")
        elif success_rate >= 90:
            logger.info("âœ… ç”Ÿäº§çº§åˆ«æµ‹è¯•è‰¯å¥½ï¼Œç³»ç»ŸåŸºæœ¬æ»¡è¶³ç”Ÿäº§è¦æ±‚")
        elif success_rate >= 80:
            logger.info("âš ï¸ ç”Ÿäº§çº§åˆ«æµ‹è¯•åˆæ ¼ï¼Œå»ºè®®ä¼˜åŒ–é”™è¯¯å¤„ç†")
        else:
            logger.info("âŒ ç”Ÿäº§çº§åˆ«æµ‹è¯•éœ€è¦æ”¹è¿›ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
    
    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•èµ„æº...")
        
        # å…³é—­WebSocketè¿æ¥
        for symbol, ws in self.websocket_connections.items():
            try:
                await ws.close()
            except:
                pass
        
        # å…³é—­NATSè¿æ¥
        if self.nats_client:
            await self.nats_client.close()
        
        # å…³é—­ClickHouseè¿æ¥
        if self.clickhouse_client:
            self.clickhouse_client.close()
        
        # å…³é—­çº¿ç¨‹æ± 
        self.executor.shutdown(wait=True)
        
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MarketPrism ç”Ÿäº§çº§åˆ«æ•°æ®æµæµ‹è¯•')
    parser.add_argument('--duration', type=int, default=300, help='æµ‹è¯•æŒç»­æ—¶é—´(ç§’)')
    parser.add_argument('--symbols', default='BTC/USDT,ETH/USDT,BNB/USDT', help='äº¤æ˜“å¯¹åˆ—è¡¨(é€—å·åˆ†éš”)')
    
    args = parser.parse_args()
    symbols = [s.strip() for s in args.symbols.split(',')]
    
    # åº”ç”¨ä»£ç†é…ç½®
    AppConfig.detect_system_proxy()
    
    logger.info(f"ğŸš€ å¯åŠ¨ç”Ÿäº§çº§åˆ«æµ‹è¯•")
    logger.info(f"â±ï¸  æµ‹è¯•æ—¶é•¿: {args.duration} ç§’")
    logger.info(f"ğŸ“Š ç›‘æ§äº¤æ˜“å¯¹: {symbols}")
    
    tester = ProductionLevelDataFlowTest(args.duration, symbols)
    
    try:
        await tester.run_production_test()
        return 0
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 1
    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except Exception as e:
        logger.error(f"å¯åŠ¨å¼‚å¸¸: {e}")
        sys.exit(1)