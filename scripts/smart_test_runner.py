#!/usr/bin/env python3
"""
æ™ºèƒ½æµ‹è¯•è¿è¡Œå™¨ - åŸºäºä»£ç å˜æ›´æ™ºèƒ½é€‰æ‹©éœ€è¦è¿è¡Œçš„æµ‹è¯•
"""

import os
import sys
import subprocess
import json
from pathlib import Path
from typing import Set, List, Dict
import ast
import re

class SmartTestRunner:
    def __init__(self, base_branch: str = "main"):
        self.base_branch = base_branch
        self.project_root = Path(__file__).parent.parent
        self.is_ci = os.getenv('CI') or os.getenv('GITHUB_ACTIONS')
        self.test_mapping = self._build_test_mapping()
    
    def _build_test_mapping(self) -> Dict[str, Set[str]]:
        """æ„å»ºä»£ç æ–‡ä»¶åˆ°æµ‹è¯•æ–‡ä»¶çš„æ˜ å°„å…³ç³»"""
        mapping = {}
        
        # æ‰«ææ‰€æœ‰Pythonæ–‡ä»¶ï¼Œåˆ†æå¯¼å…¥å…³ç³»
        for py_file in self.project_root.rglob("*.py"):
            if "test" in str(py_file) or "__pycache__" in str(py_file):
                continue
            
            relative_path = str(py_file.relative_to(self.project_root))
            mapping[relative_path] = set()
            
            # æŸ¥æ‰¾å¯¹åº”çš„æµ‹è¯•æ–‡ä»¶
            test_patterns = [
                f"tests/unit/{relative_path.replace('.py', '')}/test_*.py",
                f"tests/unit/test_{py_file.stem}.py",
                f"tests/integration/test_{py_file.stem}.py",
            ]
            
            for pattern in test_patterns:
                test_files = list(self.project_root.glob(pattern))
                for test_file in test_files:
                    mapping[relative_path].add(str(test_file.relative_to(self.project_root)))
        
        return mapping
    
    def get_changed_files(self) -> Set[str]:
        """è·å–ç›¸å¯¹äºåŸºç¡€åˆ†æ”¯çš„å˜æ›´æ–‡ä»¶"""
        try:
            result = subprocess.run(
                ["git", "diff", "--name-only", f"{self.base_branch}...HEAD"],
                capture_output=True,
                text=True,
                cwd=self.project_root
            )
            
            if result.returncode != 0:
                print(f"Warning: Could not get git diff: {result.stderr}")
                return set()
            
            changed_files = set()
            for line in result.stdout.strip().split('\n'):
                if line and line.endswith('.py'):
                    changed_files.add(line)
            
            return changed_files
        
        except Exception as e:
            print(f"Error getting changed files: {e}")
            return set()
    
    def get_affected_tests(self, changed_files: Set[str]) -> Set[str]:
        """æ ¹æ®å˜æ›´æ–‡ä»¶è·å–éœ€è¦è¿è¡Œçš„æµ‹è¯•"""
        affected_tests = set()
        
        for changed_file in changed_files:
            # ç›´æ¥æ˜ å°„çš„æµ‹è¯•
            if changed_file in self.test_mapping:
                affected_tests.update(self.test_mapping[changed_file])
            
            # å¦‚æœæ˜¯æµ‹è¯•æ–‡ä»¶æœ¬èº«
            if "test" in changed_file:
                affected_tests.add(changed_file)
            
            # åˆ†æä¾èµ–å…³ç³»
            affected_tests.update(self._find_dependent_tests(changed_file))
        
        return affected_tests
    
    def _find_dependent_tests(self, changed_file: str) -> Set[str]:
        """æŸ¥æ‰¾ä¾èµ–äºå˜æ›´æ–‡ä»¶çš„æµ‹è¯•"""
        dependent_tests = set()
        
        # ç®€åŒ–çš„ä¾èµ–åˆ†æ - åŸºäºæ¨¡å—è·¯å¾„
        module_path = changed_file.replace('/', '.').replace('.py', '')
        
        # æœç´¢æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ä¸­çš„å¯¼å…¥
        for test_file in self.project_root.rglob("test_*.py"):
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # æ£€æŸ¥æ˜¯å¦å¯¼å…¥äº†å˜æ›´çš„æ¨¡å—
                if module_path in content or changed_file in content:
                    relative_test_path = str(test_file.relative_to(self.project_root))
                    dependent_tests.add(relative_test_path)
            
            except Exception:
                continue
        
        return dependent_tests
    
    def categorize_tests(self, test_files: Set[str]) -> Dict[str, List[str]]:
        """å°†æµ‹è¯•æ–‡ä»¶æŒ‰ç±»å‹åˆ†ç±»"""
        categories = {
            'unit': [],
            'integration': [],
            'performance': [],
            'e2e': []
        }
        
        for test_file in test_files:
            if 'unit' in test_file:
                categories['unit'].append(test_file)
            elif 'integration' in test_file:
                categories['integration'].append(test_file)
            elif 'performance' in test_file:
                categories['performance'].append(test_file)
            elif 'e2e' in test_file:
                categories['e2e'].append(test_file)
            else:
                categories['unit'].append(test_file)  # é»˜è®¤ä¸ºå•å…ƒæµ‹è¯•
        
        return categories
    
    def estimate_test_time(self, test_files: Set[str]) -> Dict[str, float]:
        """ä¼°ç®—æµ‹è¯•è¿è¡Œæ—¶é—´"""
        time_estimates = {}
        
        for test_file in test_files:
            # åŸºäºæµ‹è¯•ç±»å‹å’Œæ–‡ä»¶å¤§å°ä¼°ç®—æ—¶é—´
            try:
                file_path = self.project_root / test_file
                if file_path.exists():
                    file_size = file_path.stat().st_size
                    
                    if 'unit' in test_file:
                        # å•å…ƒæµ‹è¯•ï¼šæ¯KBçº¦0.1ç§’
                        estimated_time = (file_size / 1024) * 0.1
                    elif 'integration' in test_file:
                        # é›†æˆæµ‹è¯•ï¼šæ¯KBçº¦0.5ç§’
                        estimated_time = (file_size / 1024) * 0.5
                    elif 'performance' in test_file:
                        # æ€§èƒ½æµ‹è¯•ï¼šæ¯KBçº¦2ç§’
                        estimated_time = (file_size / 1024) * 2.0
                    else:
                        estimated_time = (file_size / 1024) * 0.2
                    
                    time_estimates[test_file] = max(1.0, estimated_time)
                else:
                    time_estimates[test_file] = 5.0  # é»˜è®¤5ç§’
            
            except Exception:
                time_estimates[test_file] = 5.0
        
        return time_estimates
    
    def generate_test_plan(self) -> Dict:
        """ç”Ÿæˆæ™ºèƒ½æµ‹è¯•è®¡åˆ’"""
        changed_files = self.get_changed_files()
        
        if not changed_files:
            print("No Python files changed, running minimal test suite")
            # è¿è¡Œå…³é”®è·¯å¾„æµ‹è¯•
            critical_tests = {
                'tests/unit/core/test_health_check.py',
                'tests/unit/services/data_collector/test_collector_basic.py'
            }
            affected_tests = {t for t in critical_tests if (self.project_root / t).exists()}
        else:
            affected_tests = self.get_affected_tests(changed_files)
        
        if not affected_tests:
            print("No tests found for changed files, running smoke tests")
            affected_tests = {'tests/unit/core/', 'tests/unit/services/'}
        
        categorized_tests = self.categorize_tests(affected_tests)
        time_estimates = self.estimate_test_time(affected_tests)
        
        total_estimated_time = sum(time_estimates.values())
        
        test_plan = {
            'changed_files': list(changed_files),
            'affected_tests': list(affected_tests),
            'categorized_tests': categorized_tests,
            'time_estimates': time_estimates,
            'total_estimated_time': total_estimated_time,
            'parallel_execution_recommended': total_estimated_time > 60,
            'test_strategy': self._determine_test_strategy(categorized_tests, total_estimated_time)
        }
        
        return test_plan
    
    def _determine_test_strategy(self, categorized_tests: Dict, total_time: float) -> str:
        """ç¡®å®šæµ‹è¯•ç­–ç•¥"""
        unit_count = len(categorized_tests['unit'])
        integration_count = len(categorized_tests['integration'])

        # CIç¯å¢ƒä¸‹ä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
        if self.is_ci:
            if total_time < 60:
                return "ci_fast"  # CIå¿«é€Ÿæ¨¡å¼
            elif total_time < 180:
                return "ci_standard"  # CIæ ‡å‡†æ¨¡å¼
            else:
                return "ci_comprehensive"  # CIå…¨é¢æ¨¡å¼
        else:
            # æœ¬åœ°å¼€å‘ç¯å¢ƒ
            if total_time < 30:
                return "fast"  # å¿«é€Ÿæµ‹è¯•ï¼Œä¸²è¡Œæ‰§è¡Œ
            elif total_time < 120:
                return "standard"  # æ ‡å‡†æµ‹è¯•ï¼Œéƒ¨åˆ†å¹¶è¡Œ
            else:
                return "comprehensive"  # å…¨é¢æµ‹è¯•ï¼Œæœ€å¤§å¹¶è¡Œåº¦
    
    def run_tests(self, test_plan: Dict) -> bool:
        """æ ¹æ®æµ‹è¯•è®¡åˆ’è¿è¡Œæµ‹è¯•"""
        print(f"ğŸš€ Running smart test plan:")
        print(f"  - Strategy: {test_plan['test_strategy']}")
        print(f"  - Estimated time: {test_plan['total_estimated_time']:.1f}s")
        print(f"  - Tests to run: {len(test_plan['affected_tests'])}")
        
        categorized = test_plan['categorized_tests']
        strategy = test_plan['test_strategy']
        
        success = True
        
        # æ ¹æ®ç­–ç•¥æ‰§è¡Œæµ‹è¯•
        if strategy in ["fast", "ci_fast"]:
            success &= self._run_test_category("unit", categorized['unit'], parallel=False)

        elif strategy in ["standard", "ci_standard"]:
            success &= self._run_test_category("unit", categorized['unit'], parallel=True)
            if success and categorized['integration']:
                # CIç¯å¢ƒä¸‹é›†æˆæµ‹è¯•ä¹Ÿå¹¶è¡Œæ‰§è¡Œï¼Œä½†é™åˆ¶å¹¶å‘æ•°
                parallel_integration = strategy == "ci_standard"
                success &= self._run_test_category("integration", categorized['integration'],
                                                 parallel=parallel_integration)

        else:  # comprehensive or ci_comprehensive
            success &= self._run_test_category("unit", categorized['unit'], parallel=True)
            if success and categorized['integration']:
                success &= self._run_test_category("integration", categorized['integration'], parallel=True)
            if success and categorized['performance'] and not self.is_ci:
                # CIç¯å¢ƒè·³è¿‡æ€§èƒ½æµ‹è¯•ï¼Œé¿å…èµ„æºç«äº‰
                success &= self._run_test_category("performance", categorized['performance'], parallel=False)
        
        return success
    
    def _run_test_category(self, category: str, test_files: List[str], parallel: bool = False) -> bool:
        """è¿è¡Œç‰¹å®šç±»åˆ«çš„æµ‹è¯•"""
        if not test_files:
            return True
        
        print(f"\nğŸ“‹ Running {category} tests ({len(test_files)} files)")
        
        cmd = ["python", "-m", "pytest"]

        if parallel and len(test_files) > 1:
            if self.is_ci:
                # CIç¯å¢ƒé™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…èµ„æºç«äº‰
                cmd.extend(["-n", "2"])
            else:
                cmd.extend(["-n", "auto"])  # pytest-xdistå¹¶è¡Œæ‰§è¡Œ

        cmd.extend(test_files)
        cmd.extend(["-v", "--tb=short"])

        # CIç¯å¢ƒæ·»åŠ é¢å¤–çš„æŠ¥å‘Šå’Œè¶…æ—¶è®¾ç½®
        if self.is_ci:
            cmd.extend([
                "--timeout=300",
                "--junitxml=tests/reports/junit-{}.xml".format(category),
                "--html=tests/reports/report-{}.html".format(category),
                "--self-contained-html"
            ])

        if category == "unit":
            cmd.extend(["--cov=core/", "--cov=services/", "--cov-report=term-missing"])
            if self.is_ci:
                cmd.extend([
                    "--cov-report=xml:tests/reports/coverage-{}.xml".format(category),
                    "--cov-report=json:tests/reports/coverage-{}.json".format(category)
                ])

        # æ·»åŠ CIç¯å¢ƒç‰¹å®šçš„æ ‡è®°è¿‡æ»¤
        if self.is_ci and category == "integration":
            cmd.extend(["-m", "not slow and not performance"])
        
        try:
            result = subprocess.run(cmd, cwd=self.project_root)
            success = result.returncode == 0
            
            if success:
                print(f"âœ… {category} tests passed")
            else:
                print(f"âŒ {category} tests failed")
            
            return success
        
        except Exception as e:
            print(f"âŒ Error running {category} tests: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Smart test runner for MarketPrism")
    parser.add_argument("--base-branch", default="main", help="Base branch for comparison")
    parser.add_argument("--plan-only", action="store_true", help="Only generate test plan, don't run tests")
    parser.add_argument("--output", help="Output test plan to JSON file")
    
    args = parser.parse_args()
    
    runner = SmartTestRunner(base_branch=args.base_branch)
    test_plan = runner.generate_test_plan()
    
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(test_plan, f, indent=2)
        print(f"Test plan saved to {args.output}")
    
    if args.plan_only:
        print(json.dumps(test_plan, indent=2))
        return
    
    success = runner.run_tests(test_plan)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
