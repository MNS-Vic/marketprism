#!/usr/bin/env python3
"""
MarketPrismé«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡
ä½¿ç”¨å†…å­˜ç¼“å­˜å’Œæ‰¹å¤„ç†ä¼˜åŒ–é«˜é¢‘æ•°æ®å†™å…¥æ€§èƒ½
"""

import asyncio
import aiohttp
import json
import logging
import signal
import sys
import time
from collections import defaultdict, deque
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import nats
from nats.errors import TimeoutError, NoServersError

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CachedHotStorageService:
    """é«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡"""
    
    def __init__(self):
        self.nc = None
        self.running = False
        
        # ç¼“å­˜é…ç½® - é’ˆå¯¹ä¸åŒæ•°æ®ç±»å‹çš„ä¸åŒç­–ç•¥
        self.cache_config = {
            # é«˜é¢‘æ•°æ® - å¤§æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶
            'orderbook': {'batch_size': 500, 'timeout': 3.0, 'max_queue': 5000},
            'trade': {'batch_size': 200, 'timeout': 2.0, 'max_queue': 2000},
            
            # ä¸­é¢‘æ•°æ® - ä¸­æ‰¹æ¬¡ï¼Œä¸­è¶…æ—¶
            'funding_rate': {'batch_size': 50, 'timeout': 10.0, 'max_queue': 500},
            'open_interest': {'batch_size': 50, 'timeout': 10.0, 'max_queue': 500},
            'lsr_top_position': {'batch_size': 100, 'timeout': 5.0, 'max_queue': 1000},
            'lsr_all_account': {'batch_size': 100, 'timeout': 5.0, 'max_queue': 1000},
            
            # ä½é¢‘æ•°æ® - å°æ‰¹æ¬¡ï¼Œé•¿è¶…æ—¶
            'liquidation': {'batch_size': 20, 'timeout': 30.0, 'max_queue': 200},
            'volatility_index': {'batch_size': 30, 'timeout': 15.0, 'max_queue': 300},
        }
        
        # æ•°æ®ç¼“å­˜é˜Ÿåˆ—
        self.data_queues = defaultdict(deque)
        self.last_flush_time = defaultdict(float)
        self.flush_locks = defaultdict(asyncio.Lock)
        
        # ClickHouseé…ç½®
        self.clickhouse_config = {
            'host': 'localhost',
            'port': 8123,
            'database': 'marketprism_hot'
        }
        
        # è¡¨æ˜ å°„å’Œå­—æ®µæ˜ å°„
        self.table_mapping = {
            'orderbook': 'orderbooks',
            'trade': 'trades',
            'funding_rate': 'funding_rates',
            'open_interest': 'open_interests',
            'liquidation': 'liquidations',
            'lsr_top_position': 'lsr_top_positions',
            'lsr_all_account': 'lsr_all_accounts',
            'volatility_index': 'volatility_indices'
        }
        
        # å­—æ®µæ˜ å°„ - ç¡®ä¿åªæ’å…¥è¡¨ä¸­å­˜åœ¨çš„å­—æ®µ
        self.field_mapping = {
            'orderbooks': ['timestamp', 'exchange', 'market_type', 'symbol', 'last_update_id',
                          'best_bid_price', 'best_ask_price', 'bids', 'asks', 'data_source'],
            'trades': ['timestamp', 'exchange', 'market_type', 'symbol', 'trade_id',
                      'price', 'quantity', 'side', 'data_source', 'is_maker'],
            'funding_rates': ['timestamp', 'exchange', 'market_type', 'symbol', 
                             'current_funding_rate', 'next_funding_time', 'data_source'],
            'open_interests': ['timestamp', 'exchange', 'market_type', 'symbol', 
                              'open_interest', 'data_source'],
            'liquidations': ['timestamp', 'exchange', 'market_type', 'symbol', 
                            'side', 'quantity', 'price', 'data_source'],
            'lsr_top_positions': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                 'long_position_ratio', 'short_position_ratio', 'period', 'data_source'],
            'lsr_all_accounts': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                'long_account_ratio', 'short_account_ratio', 'period', 'data_source'],
            'volatility_indices': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                  'volatility_index', 'data_source']
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_received': 0,
            'total_batches': 0,
            'total_inserted': 0,
            'cache_hits': 0,
            'errors': 0,
            'by_type': defaultdict(lambda: {'received': 0, 'batches': 0, 'inserted': 0})
        }
        
    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            logger.info("âœ… è¿æ¥åˆ°NATSæœåŠ¡å™¨æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            return False
    
    async def subscribe_to_data_streams(self):
        """è®¢é˜…æ‰€æœ‰æ•°æ®æµ"""
        try:
            subjects = [
                "orderbook-data.>",
                "trade-data.>", 
                "funding-rate-data.>",
                "open-interest-data.>",
                "liquidation-data.>",
                "lsr-data.>",
                "volatility-index-data.>"
            ]
            
            for subject in subjects:
                await self.nc.subscribe(subject, cb=self.message_handler)
                logger.info(f"âœ… è®¢é˜…æˆåŠŸ: {subject}")
                
            return True
        except Exception as e:
            logger.error(f"âŒ è®¢é˜…å¤±è´¥: {e}")
            return False
    
    async def message_handler(self, msg):
        """æ¶ˆæ¯å¤„ç†å™¨ - æ™ºèƒ½ç¼“å­˜"""
        try:
            data = json.loads(msg.data.decode())
            subject = msg.subject
            
            data_type = self.extract_data_type(subject)
            if not data_type:
                return
                
            # æ·»åŠ åˆ°ç¼“å­˜é˜Ÿåˆ—
            self.data_queues[data_type].append(data)
            self.stats['total_received'] += 1
            self.stats['by_type'][data_type]['received'] += 1
            
            # æ™ºèƒ½åˆ·æ–°æ£€æŸ¥
            await self.smart_flush_check(data_type)
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    def extract_data_type(self, subject: str) -> Optional[str]:
        """ä»ä¸»é¢˜ä¸­æå–æ•°æ®ç±»å‹"""
        if subject.startswith("orderbook-data"):
            return "orderbook"
        elif subject.startswith("trade-data"):
            return "trade"
        elif subject.startswith("funding-rate-data"):
            return "funding_rate"
        elif subject.startswith("open-interest-data"):
            return "open_interest"
        elif subject.startswith("liquidation-data"):
            return "liquidation"
        elif subject.startswith("lsr-data"):
            if "top-position" in subject:
                return "lsr_top_position"
            elif "all-account" in subject:
                return "lsr_all_account"
        elif subject.startswith("volatility-index-data"):
            return "volatility_index"
        return None
    
    async def smart_flush_check(self, data_type: str):
        """æ™ºèƒ½åˆ·æ–°æ£€æŸ¥ - æ ¹æ®æ•°æ®ç±»å‹ä½¿ç”¨ä¸åŒç­–ç•¥"""
        config = self.cache_config.get(data_type, {'batch_size': 100, 'timeout': 5.0, 'max_queue': 1000})
        queue = self.data_queues[data_type]
        current_time = time.time()
        last_flush = self.last_flush_time.get(data_type, 0)
        
        should_flush = (
            len(queue) >= config['batch_size'] or  # è¾¾åˆ°æ‰¹å¤§å°
            (len(queue) > 0 and current_time - last_flush >= config['timeout']) or  # è¶…æ—¶
            len(queue) >= config['max_queue']  # é˜Ÿåˆ—è¿‡å¤§ï¼Œå¼ºåˆ¶åˆ·æ–°
        )
        
        if should_flush:
            # ä½¿ç”¨é”é˜²æ­¢å¹¶å‘åˆ·æ–°
            if not self.flush_locks[data_type].locked():
                asyncio.create_task(self.flush_batch(data_type))
    
    async def flush_batch(self, data_type: str):
        """æ‰¹é‡åˆ·æ–°æ•°æ®åˆ°ClickHouse"""
        async with self.flush_locks[data_type]:
            queue = self.data_queues[data_type]
            if not queue:
                return
                
            try:
                config = self.cache_config.get(data_type, {'batch_size': 100})
                batch_data = []
                batch_size = min(len(queue), config['batch_size'])
                
                # æå–æ‰¹æ•°æ®
                for _ in range(batch_size):
                    if queue:
                        batch_data.append(queue.popleft())
                
                if not batch_data:
                    return
                    
                # æ‰¹é‡æ’å…¥
                success = await self.batch_insert_to_clickhouse(data_type, batch_data)
                
                if success:
                    self.stats['total_batches'] += 1
                    self.stats['total_inserted'] += len(batch_data)
                    self.stats['by_type'][data_type]['batches'] += 1
                    self.stats['by_type'][data_type]['inserted'] += len(batch_data)
                    
                    if len(batch_data) >= 100:  # åªè®°å½•å¤§æ‰¹æ¬¡
                        logger.info(f"âœ… å¤§æ‰¹æ¬¡å†™å…¥: {data_type} - {len(batch_data)}æ¡")
                else:
                    # å¤±è´¥æ—¶é‡æ–°åŠ å…¥é˜Ÿåˆ—å¤´éƒ¨
                    for data in reversed(batch_data):
                        queue.appendleft(data)
                    logger.error(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {data_type}")
                    
                self.last_flush_time[data_type] = time.time()
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†å¼‚å¸¸: {data_type} - {e}")
                self.stats['errors'] += 1
    
    async def batch_insert_to_clickhouse(self, data_type: str, batch_data: List[Dict]) -> bool:
        """æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouse"""
        try:
            table_name = self.table_mapping.get(data_type)
            if not table_name:
                return False
                
            # æ„å»ºæ‰¹é‡æ’å…¥SQL
            insert_sql = self.build_optimized_batch_sql(table_name, batch_data)
            if not insert_sql:
                return False
                
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            url = f"http://{self.clickhouse_config['host']}:{self.clickhouse_config['port']}/?database={self.clickhouse_config['database']}"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=insert_sql) as response:
                    return response.status == 200
                        
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¼‚å¸¸: {e}")
            return False
    
    def clean_data_for_table(self, table_name: str, data: Dict) -> Dict:
        """æ¸…ç†æ•°æ®ï¼Œåªä¿ç•™è¡¨ä¸­å­˜åœ¨çš„å­—æ®µ"""
        allowed_fields = self.field_mapping.get(table_name, [])
        if not allowed_fields:
            return data

        cleaned_data = {}
        for field in allowed_fields:
            if field in data:
                cleaned_data[field] = data[field]
            elif field == 'data_source':
                cleaned_data[field] = 'marketprism'  # é»˜è®¤å€¼
            elif field == 'is_maker':
                cleaned_data[field] = False  # é»˜è®¤å€¼

        return cleaned_data

    def build_optimized_batch_sql(self, table_name: str, batch_data: List[Dict]) -> Optional[str]:
        """æ„å»ºä¼˜åŒ–çš„æ‰¹é‡æ’å…¥SQL"""
        try:
            if not batch_data:
                return None

            # è·å–è¡¨çš„å­—æ®µæ˜ å°„
            allowed_fields = self.field_mapping.get(table_name, [])
            if not allowed_fields:
                return None

            # æ¸…ç†å¹¶è¿‡æ»¤å­—æ®µ
            cleaned_batch = []
            for data in batch_data:
                cleaned_data = self.clean_data_for_table(table_name, data)
                if cleaned_data:
                    cleaned_batch.append(cleaned_data)

            if not cleaned_batch:
                return None

            # æ„å»ºVALUES
            values_list = []
            for data in cleaned_batch:
                values = []
                for field in allowed_fields:
                    value = data.get(field)
                    if value is None:
                        values.append('NULL')
                    elif isinstance(value, str):
                        escaped_value = value.replace("'", "\\'").replace("\\", "\\\\")
                        values.append(f"'{escaped_value}'")
                    elif isinstance(value, (dict, list)):
                        json_str = json.dumps(value).replace("'", "\\'").replace("\\", "\\\\")
                        values.append(f"'{json_str}'")
                    elif isinstance(value, bool):
                        values.append('1' if value else '0')
                    else:
                        values.append(str(value))

                values_list.append(f"({', '.join(values)})")

            # æ„å»ºå®Œæ•´SQL
            fields_str = ', '.join(allowed_fields)
            values_str = ', '.join(values_list)
            sql = f"INSERT INTO {table_name} ({fields_str}) VALUES {values_str}"

            return sql

        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ‰¹é‡SQLå¤±è´¥: {e}")
            return None
    
    async def periodic_maintenance(self):
        """å®šæœŸç»´æŠ¤ä»»åŠ¡"""
        while self.running:
            try:
                current_time = time.time()
                
                # æ£€æŸ¥è¶…æ—¶çš„é˜Ÿåˆ—
                for data_type in list(self.data_queues.keys()):
                    config = self.cache_config.get(data_type, {'timeout': 5.0})
                    queue = self.data_queues[data_type]
                    last_flush = self.last_flush_time.get(data_type, 0)
                    
                    if len(queue) > 0 and current_time - last_flush >= config['timeout']:
                        if not self.flush_locks[data_type].locked():
                            asyncio.create_task(self.flush_batch(data_type))
                
                await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸç»´æŠ¤å¼‚å¸¸: {e}")
                await asyncio.sleep(5)
    
    async def stats_reporter(self):
        """ç»Ÿè®¡ä¿¡æ¯æŠ¥å‘Š"""
        while self.running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡
                
                queue_sizes = {dt: len(q) for dt, q in self.data_queues.items() if len(q) > 0}
                
                logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
                logger.info(f"   æ€»æ¥æ”¶: {self.stats['total_received']}")
                logger.info(f"   æ€»æ‰¹æ¬¡: {self.stats['total_batches']}")
                logger.info(f"   æ€»æ’å…¥: {self.stats['total_inserted']}")
                logger.info(f"   é”™è¯¯æ•°: {self.stats['errors']}")
                if queue_sizes:
                    logger.info(f"   é˜Ÿåˆ—å¤§å°: {queue_sizes}")
                
                # é‡ç½®è®¡æ•°å™¨
                self.stats['total_received'] = 0
                self.stats['total_batches'] = 0
                self.stats['total_inserted'] = 0
                
            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡æŠ¥å‘Šå¼‚å¸¸: {e}")
    
    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")
        
        if not await self.connect_nats():
            return False
            
        if not await self.subscribe_to_data_streams():
            return False
            
        self.running = True
        
        # å¯åŠ¨åå°ä»»åŠ¡
        tasks = [
            asyncio.create_task(self.periodic_maintenance()),
            asyncio.create_task(self.stats_reporter())
        ]
        
        logger.info("âœ… é«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å¯åŠ¨æˆåŠŸ")
        logger.info("ğŸ“‹ ç¼“å­˜é…ç½®:")
        for data_type, config in self.cache_config.items():
            logger.info(f"   {data_type}: æ‰¹æ¬¡{config['batch_size']}, è¶…æ—¶{config['timeout']}s")
        
        try:
            await asyncio.gather(*tasks)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            await self.stop()
    
    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        logger.info("ğŸ›‘ åœæ­¢ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")
        self.running = False
        
        # åˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        for data_type in list(self.data_queues.keys()):
            if self.data_queues[data_type]:
                await self.flush_batch(data_type)
        
        if self.nc:
            await self.nc.close()
        
        logger.info("âœ… ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å·²åœæ­¢")

async def main():
    service = CachedHotStorageService()
    
    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}")
        asyncio.create_task(service.stop())
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    await service.start()

if __name__ == "__main__":
    asyncio.run(main())
