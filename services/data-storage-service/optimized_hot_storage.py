#!/usr/bin/env python3
"""
MarketPrismä¼˜åŒ–çƒ­å­˜å‚¨æœåŠ¡
è§£å†³é«˜é¢‘INSERTå¯¼è‡´çš„æ€§èƒ½é—®é¢˜ï¼Œä½¿ç”¨æ‰¹å¤„ç†ä¼˜åŒ–
"""

import asyncio
import aiohttp
import json
import logging
import signal
import sys
from collections import defaultdict, deque
from datetime import datetime
from typing import Dict, List, Any, Optional
import nats
from nats.errors import TimeoutError, NoServersError

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedHotStorageService:
    """ä¼˜åŒ–çš„çƒ­å­˜å‚¨æœåŠ¡ï¼Œä½¿ç”¨æ‰¹å¤„ç†å‡å°‘ClickHouseå‹åŠ›"""
    
    def __init__(self):
        self.nc = None
        self.running = False
        
        # æ‰¹å¤„ç†é…ç½®
        self.batch_size = 100  # æ¯æ‰¹æœ€å¤š100æ¡è®°å½•
        self.batch_timeout = 5.0  # 5ç§’è¶…æ—¶
        self.max_queue_size = 10000  # æœ€å¤§é˜Ÿåˆ—å¤§å°
        
        # æ•°æ®ç¼“å­˜é˜Ÿåˆ—
        self.data_queues = defaultdict(deque)  # æŒ‰æ•°æ®ç±»å‹åˆ†ç»„
        self.last_flush_time = defaultdict(float)
        
        # ClickHouseé…ç½®
        self.clickhouse_config = {
            'host': 'localhost',
            'port': 8123,
            'database': 'marketprism_hot'
        }
        
        # è¡¨æ˜ å°„
        self.table_mapping = {
            'orderbook': 'orderbooks',
            'trade': 'trades',
            'funding_rate': 'funding_rates',
            'open_interest': 'open_interests',
            'liquidation': 'liquidations',
            'lsr_top_position': 'lsr_top_positions',
            'lsr_all_account': 'lsr_all_accounts',
            'volatility_index': 'volatility_indices'
        }

        # å­—æ®µè¿‡æ»¤ - ç§»é™¤ä¸å­˜åœ¨äºè¡¨ä¸­çš„å­—æ®µ
        self.excluded_fields = {'data_type', 'exchange_name', 'product_type'}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_received': 0,
            'total_batches': 0,
            'total_inserted': 0,
            'batch_sizes': deque(maxlen=1000),
            'errors': 0
        }
        
    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            logger.info("âœ… è¿æ¥åˆ°NATSæœåŠ¡å™¨æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            return False
    
    async def subscribe_to_data_streams(self):
        """è®¢é˜…æ‰€æœ‰æ•°æ®æµ"""
        try:
            # è®¢é˜…æ‰€æœ‰æ•°æ®ç±»å‹
            subjects = [
                "orderbook.>",
                "trade.>",
                "funding_rate.>",
                "open_interest.>",
                "liquidation.>",
                "lsr_top_position.>",
                "lsr_all_account.>",
                "volatility_index.>"
            ]
            
            for subject in subjects:
                await self.nc.subscribe(subject, cb=self.message_handler)
                logger.info(f"âœ… è®¢é˜…æˆåŠŸ: {subject}")
                
            return True
        except Exception as e:
            logger.error(f"âŒ è®¢é˜…å¤±è´¥: {e}")
            return False
    
    async def message_handler(self, msg):
        """æ¶ˆæ¯å¤„ç†å™¨ - æ·»åŠ åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        try:
            # è§£ææ¶ˆæ¯
            data = json.loads(msg.data.decode())
            subject = msg.subject
            
            # æå–æ•°æ®ç±»å‹
            data_type = self.extract_data_type(subject)
            if not data_type:
                return
                
            # æ·»åŠ åˆ°é˜Ÿåˆ—
            self.data_queues[data_type].append(data)
            self.stats['total_received'] += 1
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
            await self.check_and_flush(data_type)
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    def extract_data_type(self, subject: str) -> Optional[str]:
        """ä»ä¸»é¢˜ä¸­æå–æ•°æ®ç±»å‹"""
        try:
            if subject.startswith("orderbook-data"):
                return "orderbook"
            elif subject.startswith("trade-data"):
                return "trade"
            elif subject.startswith("funding-rate-data"):
                return "funding_rate"
            elif subject.startswith("open-interest-data"):
                return "open_interest"
            elif subject.startswith("liquidation-data"):
                return "liquidation"
            elif subject.startswith("lsr-data"):
                if "top-position" in subject:
                    return "lsr_top_position"
                elif "all-account" in subject:
                    return "lsr_all_account"
            elif subject.startswith("volatility-index-data"):
                return "volatility_index"
            return None
        except:
            return None
    
    async def check_and_flush(self, data_type: str):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°æ‰¹å¤„ç†"""
        queue = self.data_queues[data_type]
        current_time = asyncio.get_event_loop().time()
        last_flush = self.last_flush_time[data_type]
        
        # æ£€æŸ¥åˆ·æ–°æ¡ä»¶
        should_flush = (
            len(queue) >= self.batch_size or  # è¾¾åˆ°æ‰¹å¤§å°
            (len(queue) > 0 and current_time - last_flush >= self.batch_timeout) or  # è¶…æ—¶
            len(queue) >= self.max_queue_size  # é˜Ÿåˆ—è¿‡å¤§
        )
        
        if should_flush:
            await self.flush_batch(data_type)
    
    async def flush_batch(self, data_type: str):
        """åˆ·æ–°æ‰¹å¤„ç†æ•°æ®åˆ°ClickHouse"""
        queue = self.data_queues[data_type]
        if not queue:
            return
            
        try:
            # æå–æ‰¹æ•°æ®
            batch_data = []
            batch_size = min(len(queue), self.batch_size)
            
            for _ in range(batch_size):
                if queue:
                    batch_data.append(queue.popleft())
            
            if not batch_data:
                return
                
            # æ‰¹é‡æ’å…¥
            success = await self.batch_insert_to_clickhouse(data_type, batch_data)
            
            if success:
                self.stats['total_batches'] += 1
                self.stats['total_inserted'] += len(batch_data)
                self.stats['batch_sizes'].append(len(batch_data))
                logger.info(f"âœ… æ‰¹å¤„ç†æˆåŠŸ: {data_type} - {len(batch_data)}æ¡è®°å½•")
            else:
                # å¤±è´¥æ—¶é‡æ–°åŠ å…¥é˜Ÿåˆ—
                for data in reversed(batch_data):
                    queue.appendleft(data)
                logger.error(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {data_type}")
                
            self.last_flush_time[data_type] = asyncio.get_event_loop().time()
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹å¤„ç†å¼‚å¸¸: {data_type} - {e}")
            self.stats['errors'] += 1
    
    async def batch_insert_to_clickhouse(self, data_type: str, batch_data: List[Dict]) -> bool:
        """æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouse"""
        try:
            table_name = self.table_mapping.get(data_type, data_type)
            
            # æ„å»ºæ‰¹é‡æ’å…¥SQL
            insert_sql = self.build_batch_insert_sql(table_name, batch_data)
            if not insert_sql:
                return False
                
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            url = f"http://{self.clickhouse_config['host']}:{self.clickhouse_config['port']}/?database={self.clickhouse_config['database']}"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=insert_sql) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ ClickHouseé”™è¯¯: {response.status} - {error_text}")
                        return False
                        
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¼‚å¸¸: {e}")
            return False
    
    def build_batch_insert_sql(self, table_name: str, batch_data: List[Dict]) -> Optional[str]:
        """æ„å»ºæ‰¹é‡æ’å…¥SQL"""
        try:
            if not batch_data:
                return None

            # ä½¿ç”¨ç¬¬ä¸€æ¡è®°å½•ç¡®å®šå­—æ®µï¼Œè¿‡æ»¤æ‰ä¸éœ€è¦çš„å­—æ®µ
            first_record = batch_data[0]
            fields = [f for f in first_record.keys() if f not in self.excluded_fields]
            
            # æ„å»ºVALUESéƒ¨åˆ†
            values_list = []
            for data in batch_data:
                values = []
                for field in fields:
                    value = data.get(field)
                    if value is None:
                        values.append('NULL')
                    elif isinstance(value, str):
                        # è½¬ä¹‰å•å¼•å·
                        escaped_value = value.replace("'", "\\'")
                        values.append(f"'{escaped_value}'")
                    elif isinstance(value, (dict, list)):
                        # JSONå­—æ®µ
                        json_str = json.dumps(value).replace("'", "\\'")
                        values.append(f"'{json_str}'")
                    else:
                        values.append(str(value))
                
                values_list.append(f"({', '.join(values)})")
            
            # æ„å»ºå®Œæ•´SQL
            fields_str = ', '.join(fields)
            values_str = ', '.join(values_list)
            sql = f"INSERT INTO {table_name} ({fields_str}) VALUES {values_str}"
            
            return sql
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ‰¹é‡SQLå¤±è´¥: {e}")
            return None
    
    async def periodic_flush(self):
        """å®šæœŸåˆ·æ–°æ‰€æœ‰é˜Ÿåˆ—"""
        while self.running:
            try:
                current_time = asyncio.get_event_loop().time()
                
                for data_type in list(self.data_queues.keys()):
                    queue = self.data_queues[data_type]
                    last_flush = self.last_flush_time[data_type]
                    
                    # æ£€æŸ¥è¶…æ—¶çš„é˜Ÿåˆ—
                    if len(queue) > 0 and current_time - last_flush >= self.batch_timeout:
                        await self.flush_batch(data_type)
                
                await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸåˆ·æ–°å¼‚å¸¸: {e}")
                await asyncio.sleep(5)
    
    async def stats_reporter(self):
        """ç»Ÿè®¡ä¿¡æ¯æŠ¥å‘Š"""
        while self.running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡
                
                avg_batch_size = 0
                if self.stats['batch_sizes']:
                    avg_batch_size = sum(self.stats['batch_sizes']) / len(self.stats['batch_sizes'])
                
                queue_sizes = {dt: len(q) for dt, q in self.data_queues.items()}
                
                logger.info(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š:")
                logger.info(f"   æ€»æ¥æ”¶: {self.stats['total_received']}")
                logger.info(f"   æ€»æ‰¹æ¬¡: {self.stats['total_batches']}")
                logger.info(f"   æ€»æ’å…¥: {self.stats['total_inserted']}")
                logger.info(f"   å¹³å‡æ‰¹å¤§å°: {avg_batch_size:.1f}")
                logger.info(f"   é”™è¯¯æ•°: {self.stats['errors']}")
                logger.info(f"   é˜Ÿåˆ—å¤§å°: {queue_sizes}")
                
            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡æŠ¥å‘Šå¼‚å¸¸: {e}")
    
    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–çƒ­å­˜å‚¨æœåŠ¡...")
        
        # è¿æ¥NATS
        if not await self.connect_nats():
            return False
            
        # è®¢é˜…æ•°æ®æµ
        if not await self.subscribe_to_data_streams():
            return False
            
        self.running = True
        
        # å¯åŠ¨åå°ä»»åŠ¡
        tasks = [
            asyncio.create_task(self.periodic_flush()),
            asyncio.create_task(self.stats_reporter())
        ]
        
        logger.info("âœ… ä¼˜åŒ–çƒ­å­˜å‚¨æœåŠ¡å¯åŠ¨æˆåŠŸ")
        
        try:
            await asyncio.gather(*tasks)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            await self.stop()
    
    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        logger.info("ğŸ›‘ åœæ­¢ä¼˜åŒ–çƒ­å­˜å‚¨æœåŠ¡...")
        self.running = False
        
        # åˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        for data_type in list(self.data_queues.keys()):
            await self.flush_batch(data_type)
        
        if self.nc:
            await self.nc.close()
        
        logger.info("âœ… ä¼˜åŒ–çƒ­å­˜å‚¨æœåŠ¡å·²åœæ­¢")

async def main():
    """ä¸»å‡½æ•°"""
    service = OptimizedHotStorageService()
    
    # ä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}")
        asyncio.create_task(service.stop())
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    await service.start()

if __name__ == "__main__":
    asyncio.run(main())
