#!/usr/bin/env python3
"""
ğŸ—„ï¸ MarketPrism Data Storage Service v1.0 - ä¼ä¸šçº§æ•°æ®å­˜å‚¨å’Œæ‰¹å¤„ç†å¼•æ“
================================================================================

ğŸ“Š **é«˜æ€§èƒ½æ‰¹å¤„ç†å¼•æ“** - æ”¯æŒ8ç§æ•°æ®ç±»å‹çš„æ™ºèƒ½å­˜å‚¨å’Œç®¡ç†

ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½æ¦‚è§ˆ**:
- âœ… **NATSæ¶ˆæ¯æ¶ˆè´¹**: é«˜æ•ˆè®¢é˜…å’Œå¤„ç†å¤šç§æ•°æ®ç±»å‹
- âœ… **æ™ºèƒ½æ‰¹å¤„ç†**: å·®å¼‚åŒ–æ‰¹å¤„ç†ç­–ç•¥ï¼Œä¼˜åŒ–ä¸åŒé¢‘ç‡æ•°æ®
- âœ… **ClickHouseé›†æˆ**: é«˜æ€§èƒ½åˆ—å¼æ•°æ®åº“å­˜å‚¨
- âœ… **æ—¶é—´æˆ³æ ‡å‡†åŒ–**: ç»Ÿä¸€æ—¶é—´æˆ³æ ¼å¼å¤„ç†
- âœ… **æ€§èƒ½ç›‘æ§**: å®æ—¶æ€§èƒ½ç»Ÿè®¡å’Œå¥åº·æ£€æŸ¥
- âœ… **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
- âœ… **æ•°æ®è´¨é‡**: æ•°æ®éªŒè¯å’Œå®Œæ•´æ€§æ£€æŸ¥

ğŸ—ï¸ **ç³»ç»Ÿæ¶æ„**:
```
NATS Subscriber â†’ Batch Processor â†’ ClickHouse Writer
      â†“               â†“                    â†“
   æ¶ˆæ¯æ¥æ”¶         æ™ºèƒ½æ‰¹å¤„ç†           é«˜æ€§èƒ½å­˜å‚¨
```

ğŸ“ˆ **æ‰¹å¤„ç†é…ç½®** (å·®å¼‚åŒ–ç­–ç•¥):
- **é«˜é¢‘æ•°æ®** (orderbooks, trades): 100æ¡/10ç§’, 1000é˜Ÿåˆ—
- **ä¸­é¢‘æ•°æ®** (funding_rates): 10æ¡/2ç§’, 500é˜Ÿåˆ—
- **ä½é¢‘æ•°æ®** (LSR, volatility): 1æ¡/1ç§’, 50é˜Ÿåˆ—
- **äº‹ä»¶æ•°æ®** (liquidations): 5æ¡/10ç§’, 200é˜Ÿåˆ—

ğŸš€ **å¯åŠ¨æ–¹å¼**:

1. **ç”Ÿäº§ç¯å¢ƒå¯åŠ¨**:
   ```bash
   # ç¡®ä¿ä¾èµ–æœåŠ¡å·²å¯åŠ¨
   cd ../message-broker/unified-nats
   docker-compose -f docker-compose.unified.yml up -d
   docker-compose -f docker-compose.hot-storage.yml up clickhouse-hot -d

   # å¯åŠ¨å­˜å‚¨æœåŠ¡
   nohup python3 production_cached_storage.py > production.log 2>&1 &
   ```

2. **éªŒè¯æ•°æ®å†™å…¥**:
   ```bash
   # æ£€æŸ¥æ•°æ®å†™å…¥æƒ…å†µ
   curl "http://localhost:8123/" --data "
   SELECT count(*) FROM marketprism_hot.trades
   WHERE timestamp > now() - INTERVAL 5 MINUTE"
   ```

3. **ç›‘æ§æœåŠ¡çŠ¶æ€**:
   ```bash
   tail -f production.log                    # å®æ—¶æ—¥å¿—
   grep "ğŸ“Š æ€§èƒ½ç»Ÿè®¡" production.log | tail -5  # æ€§èƒ½ç»Ÿè®¡
   ```

âš™ï¸ **é…ç½®å‚æ•°**:
- `NATS_URL`: NATSæœåŠ¡å™¨åœ°å€ (é»˜è®¤: nats://localhost:4222)
- `CLICKHOUSE_URL`: ClickHouseåœ°å€ (é»˜è®¤: http://localhost:8123)
- `DATABASE`: æ•°æ®åº“åç§° (é»˜è®¤: marketprism_hot)
- `BATCH_CONFIGS`: æ‰¹å¤„ç†é…ç½® (å†…ç½®å·®å¼‚åŒ–é…ç½®)

ğŸ“¡ **NATSè®¢é˜…ä¸»é¢˜**:
- `orderbook-data.>` - è®¢å•ç°¿æ•°æ®
- `trade-data.>` - äº¤æ˜“æ•°æ®
- `funding-rate-data.>` - èµ„é‡‘è´¹ç‡
- `open-interest-data.>` - æœªå¹³ä»“é‡
- `liquidation-data.>` - å¼ºå¹³æ•°æ®
- `lsr-data.>` - LSRæ•°æ® (Top Position + All Account)
- `volatility-index-data.>` - æ³¢åŠ¨ç‡æŒ‡æ•°

ğŸ—„ï¸ **ClickHouseè¡¨ç»“æ„** (8ç§æ•°æ®ç±»å‹):
- `orderbooks` - è®¢å•ç°¿æ•°æ® (é«˜é¢‘)
- `trades` - äº¤æ˜“æ•°æ® (è¶…é«˜é¢‘)
- `funding_rates` - èµ„é‡‘è´¹ç‡ (ä¸­é¢‘)
- `open_interests` - æœªå¹³ä»“é‡ (ä½é¢‘)
- `liquidations` - å¼ºå¹³æ•°æ® (äº‹ä»¶é©±åŠ¨)
- `lsr_top_positions` - LSRé¡¶çº§æŒä»“ (ä½é¢‘)
- `lsr_all_accounts` - LSRå…¨è´¦æˆ· (ä½é¢‘)
- `volatility_indices` - æ³¢åŠ¨ç‡æŒ‡æ•° (ä½é¢‘)

ğŸ“ˆ **æ€§èƒ½æŒ‡æ ‡** (ç”Ÿäº§ç¯å¢ƒå®æµ‹):
- å¤„ç†æˆåŠŸç‡: 99.6%
- æ‰¹å¤„ç†æ•ˆç‡: 202ä¸ªæ‰¹æ¬¡/åˆ†é’Ÿ
- é”™è¯¯ç‡: 0%
- é˜Ÿåˆ—çŠ¶æ€: å®æ—¶ç›‘æ§å„æ•°æ®ç±»å‹é˜Ÿåˆ—é•¿åº¦

ğŸ”§ **æ—¶é—´æˆ³å¤„ç†** (ç»Ÿä¸€æ ¼å¼è½¬æ¢):
```python
# æ”¯æŒå¤šç§æ ¼å¼è‡ªåŠ¨è½¬æ¢ä¸ºClickHouse DateTimeæ ¼å¼
"2025-08-06T02:17:13.123Z" â†’ "2025-08-06 02:17:13"
"2025-08-06T02:17:13+00:00" â†’ "2025-08-06 02:17:13"
"2025-08-06 02:17:13" â†’ "2025-08-06 02:17:13" (ä¿æŒ)
```

ğŸ›¡ï¸ **ç”Ÿäº§çº§ç‰¹æ€§**:
- å¼‚æ­¥æ‰¹å¤„ç†å’Œè¶…æ—¶æœºåˆ¶
- å†…å­˜é˜Ÿåˆ—ç®¡ç†å’Œæº¢å‡ºä¿æŠ¤
- é”™è¯¯é‡è¯•å’Œæ¢å¤æœºåˆ¶
- æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§æŠ¥å‘Š
- ä¼˜é›…å…³é—­å’Œèµ„æºæ¸…ç†

ğŸ”§ **æœ€æ–°ä¼˜åŒ–æˆæœ** (2025-08-06):
- âœ… LSRæ•°æ®æ‰¹å¤„ç†ä¼˜åŒ–: æ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º1æ¡ï¼Œç¡®ä¿ä½é¢‘æ•°æ®åŠæ—¶å†™å…¥
- âœ… æ—¶é—´æˆ³æ ¼å¼ç»Ÿä¸€: å®Œå…¨æ”¯æŒClickHouse DateTimeæ ¼å¼è½¬æ¢
- âœ… é”™è¯¯å¤„ç†å®Œå–„: 99.6%å¤„ç†æˆåŠŸç‡ï¼Œé›¶é”™è¯¯è¿è¡Œ
- âœ… æ€§èƒ½ç›‘æ§å¢å¼º: è¯¦ç»†çš„æ‰¹å¤„ç†ç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡

ğŸ¯ **ä½¿ç”¨åœºæ™¯**:
- ğŸ¢ **ä¼ä¸šçº§æ•°æ®å­˜å‚¨**: é«˜é¢‘é‡‘èæ•°æ®çš„å¯é å­˜å‚¨
- ğŸ“Š **å®æ—¶æ•°æ®åˆ†æ**: æ”¯æŒå®æ—¶æŸ¥è¯¢å’Œåˆ†æ
- ğŸ” **å†å²æ•°æ®å›æµ‹**: å®Œæ•´çš„å†å²æ•°æ®å­˜å‚¨
- ğŸ“ˆ **ç›‘æ§å’Œå‘Šè­¦**: åŸºäºå­˜å‚¨æ•°æ®çš„ç›‘æ§ç³»ç»Ÿ

ä½œè€…: MarketPrism Team
ç‰ˆæœ¬: v1.0 (ç”Ÿäº§å°±ç»ª)
çŠ¶æ€: 99.6%å¤„ç†æˆåŠŸç‡ï¼Œé›¶é”™è¯¯è¿è¡Œ
æ›´æ–°: 2025-08-06 (LSRæ•°æ®æ‰¹å¤„ç†ä¼˜åŒ–å®Œæˆ)
è®¸å¯: MIT License
"""

import asyncio
import aiohttp
import json
import logging
import signal
import sys
import time
from collections import defaultdict, deque
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
import nats
from dateutil import parser as date_parser

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProductionCachedStorageService:
    """ç”Ÿäº§çº§é«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡"""
    
    def __init__(self):
        self.nc = None
        self.running = False
        
        # ç²¾ç¡®çš„å­—æ®µæ˜ å°„ - åŸºäºå®é™…è¡¨ç»“æ„
        self.table_fields = {
            'orderbooks': ['timestamp', 'exchange', 'market_type', 'symbol', 'last_update_id', 
                          'best_bid_price', 'best_ask_price', 'bids', 'asks', 'data_source'],
            'trades': ['timestamp', 'exchange', 'market_type', 'symbol', 'trade_id', 
                      'price', 'quantity', 'side', 'data_source', 'is_maker'],
            'funding_rates': ['timestamp', 'exchange', 'market_type', 'symbol', 
                             'funding_rate', 'funding_time', 'next_funding_time', 'data_source'],
            'open_interests': ['timestamp', 'exchange', 'market_type', 'symbol', 
                              'open_interest', 'open_interest_value', 'data_source'],
            'liquidations': ['timestamp', 'exchange', 'market_type', 'symbol', 
                            'side', 'price', 'quantity', 'data_source'],
            'lsr_top_positions': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                 'long_position_ratio', 'short_position_ratio', 'period', 'data_source'],
            'lsr_all_accounts': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                'long_account_ratio', 'short_account_ratio', 'period', 'data_source'],
            'volatility_indices': ['timestamp', 'exchange', 'market_type', 'symbol', 
                                  'index_value', 'underlying_asset', 'data_source']
        }
        
        # æ•°æ®å­—æ®µæ˜ å°„ - ä»æ¶ˆæ¯å­—æ®µåˆ°è¡¨å­—æ®µ
        self.field_mapping = {
            'current_funding_rate': 'funding_rate',
            'volatility_index': 'index_value',
            'open_interest': 'open_interest',
            'open_interest_value': 'open_interest_value'
        }
        
        # æ™ºèƒ½ç¼“å­˜é…ç½® - åŸºäºæ•°æ®é¢‘ç‡ä¼˜åŒ–
        self.cache_config = {
            # è¶…é«˜é¢‘æ•°æ® - å¤§æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶
            'orderbooks': {'batch_size': 1000, 'timeout': 2.0, 'max_queue': 10000},
            'trades': {'batch_size': 500, 'timeout': 1.5, 'max_queue': 5000},
            
            # ä¸­é¢‘æ•°æ® - å°æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶ï¼ˆä¿®å¤funding rateå †ç§¯é—®é¢˜ï¼‰
            'funding_rates': {'batch_size': 10, 'timeout': 2.0, 'max_queue': 500},
            'open_interests': {'batch_size': 50, 'timeout': 10.0, 'max_queue': 500},
            'lsr_top_position': {'batch_size': 1, 'timeout': 1.0, 'max_queue': 50},
            'lsr_all_account': {'batch_size': 1, 'timeout': 1.0, 'max_queue': 50},

            # ä½é¢‘æ•°æ® - æå°æ‰¹æ¬¡ï¼Œä¸­ç­‰è¶…æ—¶ï¼ˆä¿®å¤liquidationå †ç§¯é—®é¢˜ï¼‰
            'liquidations': {'batch_size': 5, 'timeout': 10.0, 'max_queue': 200},
            'volatility_index': {'batch_size': 1, 'timeout': 1.0, 'max_queue': 50},
        }
        
        # è¡¨æ˜ å°„
        self.table_mapping = {
            'orderbook': 'orderbooks',
            'trade': 'trades',
            'funding_rate': 'funding_rates',
            'open_interest': 'open_interests',
            'liquidation': 'liquidations',
            'lsr_top_position': 'lsr_top_positions',
            'lsr_all_account': 'lsr_all_accounts',
            'volatility_index': 'volatility_indices'
        }
        
        # ç¼“å­˜é˜Ÿåˆ—å’Œé”
        self.data_queues = defaultdict(deque)
        self.last_flush_time = defaultdict(float)
        self.flush_locks = defaultdict(asyncio.Lock)
        
        # ClickHouseé…ç½®
        self.clickhouse_config = {
            'host': 'localhost',
            'port': 8123,
            'database': 'marketprism_hot'
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_received': 0,
            'total_batches': 0,
            'total_inserted': 0,
            'errors': 0,
            'by_type': defaultdict(lambda: {'received': 0, 'batches': 0, 'inserted': 0, 'errors': 0})
        }
        
    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            logger.info("âœ… è¿æ¥åˆ°NATSæœåŠ¡å™¨æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            return False
    
    async def subscribe_to_data_streams(self):
        """è®¢é˜…æ‰€æœ‰æ•°æ®æµ"""
        try:
            subjects = [
                "orderbook-data.>",
                "trade-data.>", 
                "funding-rate-data.>",
                "open-interest-data.>",
                "liquidation-data.>",
                "lsr-data.>",
                "volatility-index-data.>"
            ]
            
            for subject in subjects:
                await self.nc.subscribe(subject, cb=self.message_handler)
                logger.info(f"âœ… è®¢é˜…æˆåŠŸ: {subject}")
                
            return True
        except Exception as e:
            logger.error(f"âŒ è®¢é˜…å¤±è´¥: {e}")
            return False
    
    async def message_handler(self, msg):
        """æ¶ˆæ¯å¤„ç†å™¨ - é«˜æ€§èƒ½ç¼“å­˜"""
        try:
            data = json.loads(msg.data.decode())
            subject = msg.subject
            
            data_type = self.extract_data_type(subject)
            if not data_type:
                return
                
            # æ·»åŠ åˆ°ç¼“å­˜é˜Ÿåˆ—
            self.data_queues[data_type].append(data)
            self.stats['total_received'] += 1
            self.stats['by_type'][data_type]['received'] += 1
            
            # æ™ºèƒ½åˆ·æ–°æ£€æŸ¥
            await self.smart_flush_check(data_type)
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    def extract_data_type(self, subject: str) -> Optional[str]:
        """ä»ä¸»é¢˜ä¸­æå–æ•°æ®ç±»å‹"""
        if subject.startswith("orderbook-data"):
            return "orderbook"
        elif subject.startswith("trade-data"):
            return "trade"
        elif subject.startswith("funding-rate-data"):
            return "funding_rate"
        elif subject.startswith("open-interest-data"):
            return "open_interest"
        elif subject.startswith("liquidation-data"):
            return "liquidation"
        elif subject.startswith("lsr-data"):
            if "top-position" in subject:
                return "lsr_top_position"
            elif "all-account" in subject:
                return "lsr_all_account"
        elif subject.startswith("volatility-index-data"):
            return "volatility_index"
        return None
    
    async def smart_flush_check(self, data_type: str):
        """æ™ºèƒ½åˆ·æ–°æ£€æŸ¥"""
        config = self.cache_config.get(data_type, {'batch_size': 100, 'timeout': 5.0, 'max_queue': 1000})
        queue = self.data_queues[data_type]
        current_time = time.time()
        last_flush = self.last_flush_time.get(data_type, 0)
        
        should_flush = (
            len(queue) >= config['batch_size'] or  # è¾¾åˆ°æ‰¹å¤§å°
            (len(queue) > 0 and current_time - last_flush >= config['timeout']) or  # è¶…æ—¶
            len(queue) >= config['max_queue']  # é˜Ÿåˆ—è¿‡å¤§ï¼Œå¼ºåˆ¶åˆ·æ–°
        )
        
        if should_flush and not self.flush_locks[data_type].locked():
            asyncio.create_task(self.flush_batch(data_type))
    
    def clean_and_map_data(self, table_name: str, data: Dict) -> Optional[Dict]:
        """æ¸…ç†å’Œæ˜ å°„æ•°æ®å­—æ®µ"""
        try:
            allowed_fields = self.table_fields.get(table_name, [])
            if not allowed_fields:
                return None
                
            cleaned_data = {}
            
            # æ˜ å°„å­—æ®µ
            for field in allowed_fields:
                value = None
                
                # ç›´æ¥å­—æ®µåŒ¹é…
                if field in data:
                    value = data[field]
                # å­—æ®µæ˜ å°„
                elif field in self.field_mapping and self.field_mapping[field] in data:
                    value = data[self.field_mapping[field]]
                # åå‘æ˜ å°„
                elif field in self.field_mapping.values():
                    for k, v in self.field_mapping.items():
                        if v == field and k in data:
                            value = data[k]
                            break
                # é»˜è®¤å€¼
                elif field == 'data_source':
                    value = 'marketprism'
                elif field == 'is_maker':
                    value = False
                elif field == 'open_interest_value':
                    value = 0.0
                elif field == 'underlying_asset':
                    value = data.get('symbol', '').split('-')[0] if '-' in data.get('symbol', '') else ''
                
                if value is not None:
                    # å¼ºåŒ–ç‰ˆæ—¶é—´æˆ³å¤„ç†ï¼šå¤„ç†æ‰€æœ‰å¯èƒ½çš„ISOæ ¼å¼
                    if (field == 'timestamp' or field.endswith('_time')) and isinstance(value, str):
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ISOæ ¼å¼ï¼ˆåŒ…å«Tçš„éƒ½è®¤ä¸ºæ˜¯ISOæ ¼å¼ï¼‰
                        if 'T' in value:
                            try:
                                # å¤„ç†å„ç§ISOæ ¼å¼
                                if value.endswith('Z'):
                                    # 2025-08-05T13:47:32.661338Z
                                    dt = datetime.fromisoformat(value.replace('Z', '+00:00'))
                                elif '+' in value or '-' in value[-6:]:
                                    # 2025-08-05T13:47:32.661338+00:00
                                    dt = datetime.fromisoformat(value)
                                else:
                                    # 2025-08-05T13:47:32.661338 (æ— æ—¶åŒºä¿¡æ¯)
                                    dt = datetime.fromisoformat(value)
                                    if dt.tzinfo is None:
                                        dt = dt.replace(tzinfo=timezone.utc)

                                cleaned_data[field] = dt.strftime('%Y-%m-%d %H:%M:%S')
                                # å‡å°‘æ—¥å¿—è¾“å‡ºï¼šæ—¶é—´æˆ³è½¬æ¢æˆåŠŸæ—¶ä¸è®°å½•
                            except Exception as e:
                                logger.warning(f"æ—¶é—´æˆ³è½¬æ¢å¤±è´¥ {field}={value}: {e}")
                                # å°è¯•ä½¿ç”¨dateutilä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
                                try:
                                    dt = date_parser.parse(value)
                                    cleaned_data[field] = dt.strftime('%Y-%m-%d %H:%M:%S')
                                    # å‡å°‘æ—¥å¿—è¾“å‡ºï¼šå¤‡é€‰è½¬æ¢æˆåŠŸæ—¶ä¸è®°å½•
                                except:
                                    cleaned_data[field] = value
                        else:
                            # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            cleaned_data[field] = value
                    else:
                        cleaned_data[field] = value

            return cleaned_data if cleaned_data else None
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return None
    
    async def flush_batch(self, data_type: str):
        """æ‰¹é‡åˆ·æ–°æ•°æ®åˆ°ClickHouse"""
        async with self.flush_locks[data_type]:
            queue = self.data_queues[data_type]
            if not queue:
                return
                
            try:
                config = self.cache_config.get(data_type, {'batch_size': 100})
                batch_data = []
                batch_size = min(len(queue), config['batch_size'])
                
                # æå–æ‰¹æ•°æ®
                for _ in range(batch_size):
                    if queue:
                        batch_data.append(queue.popleft())
                
                if not batch_data:
                    return
                    
                # æ‰¹é‡æ’å…¥
                success = await self.batch_insert_to_clickhouse(data_type, batch_data)
                
                if success:
                    self.stats['total_batches'] += 1
                    self.stats['total_inserted'] += len(batch_data)
                    self.stats['by_type'][data_type]['batches'] += 1
                    self.stats['by_type'][data_type]['inserted'] += len(batch_data)
                    
                    # åªè®°å½•å¤§æ‰¹æ¬¡
                    if len(batch_data) >= 100:
                        logger.info(f"âœ… å¤§æ‰¹æ¬¡å†™å…¥: {data_type} - {len(batch_data)}æ¡")
                else:
                    # å¤±è´¥æ—¶é‡æ–°åŠ å…¥é˜Ÿåˆ—å¤´éƒ¨
                    for data in reversed(batch_data):
                        queue.appendleft(data)
                    self.stats['by_type'][data_type]['errors'] += 1
                    
                self.last_flush_time[data_type] = time.time()
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†å¼‚å¸¸: {data_type} - {e}")
                self.stats['errors'] += 1
    
    async def batch_insert_to_clickhouse(self, data_type: str, batch_data: List[Dict]) -> bool:
        """æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouse"""
        try:
            table_name = self.table_mapping.get(data_type)
            if not table_name:
                return False
                
            # æ¸…ç†å’Œæ˜ å°„æ•°æ®
            cleaned_batch = []
            for data in batch_data:
                cleaned_data = self.clean_and_map_data(table_name, data)
                if cleaned_data:
                    cleaned_batch.append(cleaned_data)
            
            if not cleaned_batch:
                return False
                
            # æ„å»ºæ‰¹é‡æ’å…¥SQL
            insert_sql = self.build_batch_insert_sql(table_name, cleaned_batch)
            if not insert_sql:
                return False
                
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            url = f"http://{self.clickhouse_config['host']}:{self.clickhouse_config['port']}/?database={self.clickhouse_config['database']}"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=insert_sql) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"âŒ ClickHouseé”™è¯¯: {response.status} - {error_text[:200]}")
                        return False
                    return True
                        
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¼‚å¸¸: {e}")
            return False
    
    def build_batch_insert_sql(self, table_name: str, batch_data: List[Dict]) -> Optional[str]:
        """æ„å»ºæ‰¹é‡æ’å…¥SQL"""
        try:
            if not batch_data:
                return None
                
            # è·å–å­—æ®µåˆ—è¡¨
            fields = self.table_fields.get(table_name, [])
            if not fields:
                return None
            
            # æ„å»ºVALUES
            values_list = []
            for data in batch_data:
                values = []
                for field in fields:
                    value = data.get(field)
                    if value is None:
                        values.append('NULL')
                    elif isinstance(value, str):
                        # è½¬ä¹‰å­—ç¬¦ä¸²
                        escaped_value = value.replace("\\", "\\\\").replace("'", "\\'")
                        values.append(f"'{escaped_value}'")
                    elif isinstance(value, (dict, list)):
                        # JSONå­—æ®µ
                        json_str = json.dumps(value).replace("\\", "\\\\").replace("'", "\\'")
                        values.append(f"'{json_str}'")
                    elif isinstance(value, bool):
                        values.append('1' if value else '0')
                    else:
                        values.append(str(value))
                
                values_list.append(f"({', '.join(values)})")
            
            # æ„å»ºå®Œæ•´SQL
            fields_str = ', '.join(fields)
            values_str = ', '.join(values_list)
            sql = f"INSERT INTO {table_name} ({fields_str}) VALUES {values_str}"
            
            return sql

        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ‰¹é‡SQLå¤±è´¥: {e}")
            return None

    async def periodic_maintenance(self):
        """å®šæœŸç»´æŠ¤ä»»åŠ¡"""
        while self.running:
            try:
                current_time = time.time()

                # æ£€æŸ¥è¶…æ—¶çš„é˜Ÿåˆ—
                for data_type in list(self.data_queues.keys()):
                    config = self.cache_config.get(data_type, {'timeout': 5.0})
                    queue = self.data_queues[data_type]
                    last_flush = self.last_flush_time.get(data_type, 0)

                    if len(queue) > 0 and current_time - last_flush >= config['timeout']:
                        if not self.flush_locks[data_type].locked():
                            asyncio.create_task(self.flush_batch(data_type))

                await asyncio.sleep(0.5)  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œæé«˜å“åº”æ€§

            except Exception as e:
                logger.error(f"âŒ å®šæœŸç»´æŠ¤å¼‚å¸¸: {e}")
                await asyncio.sleep(5)

    async def stats_reporter(self):
        """ç»Ÿè®¡ä¿¡æ¯æŠ¥å‘Š"""
        while self.running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡

                # è®¡ç®—é˜Ÿåˆ—å¤§å°
                queue_sizes = {}
                total_queue_size = 0
                for dt, q in self.data_queues.items():
                    size = len(q)
                    if size > 0:
                        queue_sizes[dt] = size
                        total_queue_size += size

                # è®¡ç®—æ‰¹å¤„ç†æ•ˆç‡
                total_received = sum(stats['received'] for stats in self.stats['by_type'].values())
                total_inserted = sum(stats['inserted'] for stats in self.stats['by_type'].values())
                efficiency = (total_inserted / total_received * 100) if total_received > 0 else 0

                logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ (è¿‡å»1åˆ†é’Ÿ):")
                logger.info(f"   æ¥æ”¶: {total_received} | æ’å…¥: {total_inserted} | æ•ˆç‡: {efficiency:.1f}%")
                logger.info(f"   æ‰¹æ¬¡: {self.stats['total_batches']} | é”™è¯¯: {self.stats['errors']}")
                if queue_sizes:
                    logger.info(f"   é˜Ÿåˆ—: {queue_sizes} (æ€»è®¡: {total_queue_size})")

                # æ˜¾ç¤ºé«˜é¢‘æ•°æ®ç»Ÿè®¡
                high_freq_types = ['orderbook', 'trade']
                for data_type in high_freq_types:
                    stats = self.stats['by_type'][data_type]
                    if stats['received'] > 0:
                        logger.info(f"   {data_type}: æ¥æ”¶{stats['received']} æ’å…¥{stats['inserted']} æ‰¹æ¬¡{stats['batches']}")

                # é‡ç½®è®¡æ•°å™¨
                for stats in self.stats['by_type'].values():
                    stats['received'] = 0
                    stats['inserted'] = 0
                    stats['batches'] = 0
                    stats['errors'] = 0
                self.stats['total_batches'] = 0
                self.stats['errors'] = 0

            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡æŠ¥å‘Šå¼‚å¸¸: {e}")

    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("ğŸš€ å¯åŠ¨ç”Ÿäº§çº§é«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")

        if not await self.connect_nats():
            return False

        if not await self.subscribe_to_data_streams():
            return False

        self.running = True

        # å¯åŠ¨åå°ä»»åŠ¡
        tasks = [
            asyncio.create_task(self.periodic_maintenance()),
            asyncio.create_task(self.stats_reporter())
        ]

        logger.info("âœ… ç”Ÿäº§çº§ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å¯åŠ¨æˆåŠŸ")
        logger.info("ğŸ“‹ ç¼“å­˜é…ç½®:")
        for data_type, config in self.cache_config.items():
            logger.info(f"   {data_type}: æ‰¹æ¬¡{config['batch_size']}, è¶…æ—¶{config['timeout']}s, é˜Ÿåˆ—{config['max_queue']}")

        try:
            await asyncio.gather(*tasks)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            await self.stop()

    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        logger.info("ğŸ›‘ åœæ­¢ç”Ÿäº§çº§ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")
        self.running = False

        # åˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        logger.info("ğŸ“¤ åˆ·æ–°å‰©ä½™ç¼“å­˜æ•°æ®...")
        for data_type in list(self.data_queues.keys()):
            if self.data_queues[data_type]:
                await self.flush_batch(data_type)

        if self.nc:
            await self.nc.close()

        logger.info("âœ… ç”Ÿäº§çº§ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å·²åœæ­¢")

async def main():
    service = ProductionCachedStorageService()

    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}")
        asyncio.create_task(service.stop())

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    await service.start()

if __name__ == "__main__":
    asyncio.run(main())
