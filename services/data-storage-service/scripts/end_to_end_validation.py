#!/usr/bin/env python3
"""
MarketPrism åˆ†å±‚æ•°æ®å­˜å‚¨ç«¯åˆ°ç«¯éªŒè¯è„šæœ¬
éªŒè¯å®Œæ•´çš„æ•°æ®æµï¼šæ”¶é›†å™¨ â†’ NATS â†’ çƒ­ç«¯å­˜å‚¨ â†’ å†·ç«¯å­˜å‚¨
"""

import asyncio
import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any, Optional
import yaml
import structlog
import nats
from nats.js import JetStreamContext

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(project_root))

from core.storage.unified_clickhouse_writer import UnifiedClickHouseWriter
from core.storage.tiered_storage_manager import TieredStorageManager, TierConfig, StorageTier


class EndToEndValidator:
    """ç«¯åˆ°ç«¯éªŒè¯å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç«¯åˆ°ç«¯éªŒè¯å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.logger = structlog.get_logger("e2e.validator")
        
        # NATSé…ç½®
        self.nats_config = config.get('nats', {})
        self.nats_client: Optional[nats.NATS] = None
        self.jetstream: Optional[JetStreamContext] = None
        
        # å­˜å‚¨é…ç½®
        self.hot_config = config.get('hot_storage', {})
        self.cold_config = config.get('cold_storage', {})
        
        # ClickHouseå®¢æˆ·ç«¯
        self.hot_client: Optional[UnifiedClickHouseWriter] = None
        self.cold_client: Optional[UnifiedClickHouseWriter] = None
        
        # åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨
        self.storage_manager: Optional[TieredStorageManager] = None
        
        # éªŒè¯ç»“æœ
        self.validation_results = {
            "nats_connection": False,
            "hot_storage_connection": False,
            "cold_storage_connection": False,
            "data_flow_validation": {},
            "data_integrity_validation": {},
            "performance_metrics": {}
        }
    
    async def run_validation(self):
        """è¿è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯éªŒè¯"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯éªŒè¯")
            
            # 1. éªŒè¯NATSè¿æ¥
            await self._validate_nats_connection()
            
            # 2. éªŒè¯ClickHouseè¿æ¥
            await self._validate_clickhouse_connections()
            
            # 3. åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨
            await self._initialize_storage_manager()
            
            # 4. éªŒè¯æ•°æ®æµ
            await self._validate_data_flow()
            
            # 5. éªŒè¯æ•°æ®å®Œæ•´æ€§
            await self._validate_data_integrity()
            
            # 6. æ€§èƒ½æµ‹è¯•
            await self._performance_test()
            
            # 7. ç”ŸæˆéªŒè¯æŠ¥å‘Š
            await self._generate_validation_report()
            
            self.logger.info("âœ… ç«¯åˆ°ç«¯éªŒè¯å®Œæˆ")
            
        except Exception as e:
            self.logger.error("âŒ ç«¯åˆ°ç«¯éªŒè¯å¤±è´¥", error=str(e))
            raise
        finally:
            await self._cleanup()
    
    async def _validate_nats_connection(self):
        """éªŒè¯NATSè¿æ¥"""
        try:
            self.logger.info("ğŸ“¡ éªŒè¯NATSè¿æ¥")
            
            nats_url = self.nats_config.get('url', 'nats://localhost:4222')
            
            # è¿æ¥NATS
            self.nats_client = await nats.connect(
                servers=[nats_url],
                max_reconnect_attempts=3,
                reconnect_time_wait=2
            )
            
            # è·å–JetStreamä¸Šä¸‹æ–‡
            self.jetstream = self.nats_client.jetstream()
            
            # æµ‹è¯•JetStreamåŠŸèƒ½
            await self.jetstream.account_info()
            
            self.validation_results["nats_connection"] = True
            self.logger.info("âœ… NATSè¿æ¥éªŒè¯æˆåŠŸ", url=nats_url)
            
        except Exception as e:
            self.validation_results["nats_connection"] = False
            self.logger.error("âŒ NATSè¿æ¥éªŒè¯å¤±è´¥", error=str(e))
            raise
    
    async def _validate_clickhouse_connections(self):
        """éªŒè¯ClickHouseè¿æ¥"""
        try:
            self.logger.info("ğŸ—„ï¸ éªŒè¯ClickHouseè¿æ¥")
            
            # éªŒè¯çƒ­ç«¯ClickHouse
            hot_clickhouse_config = {
                'clickhouse_direct_write': True,
                'clickhouse': {
                    'host': self.hot_config.get('clickhouse_host', 'localhost'),
                    'port': self.hot_config.get('clickhouse_http_port', 8123),
                    'user': self.hot_config.get('clickhouse_user', 'default'),
                    'password': self.hot_config.get('clickhouse_password', ''),
                    'database': self.hot_config.get('clickhouse_database', 'marketprism_hot')
                }
            }

            self.hot_client = UnifiedClickHouseWriter(hot_clickhouse_config)
            await self.hot_client.start()
            # ç®€å•çš„å¥åº·æ£€æŸ¥
            await self.hot_client.execute_query("SELECT 1")
            
            self.validation_results["hot_storage_connection"] = True
            self.logger.info("âœ… çƒ­ç«¯ClickHouseè¿æ¥éªŒè¯æˆåŠŸ")
            
            # éªŒè¯å†·ç«¯ClickHouseï¼ˆå¦‚æœé…ç½®äº†ä¸åŒçš„ä¸»æœºï¼‰
            if self.cold_config.get('clickhouse_host') != self.hot_config.get('clickhouse_host'):
                cold_clickhouse_config = {
                    'clickhouse_direct_write': True,
                    'clickhouse': {
                        'host': self.cold_config.get('clickhouse_host', 'localhost'),
                        'port': self.cold_config.get('clickhouse_http_port', 8123),
                        'user': self.cold_config.get('clickhouse_user', 'default'),
                        'password': self.cold_config.get('clickhouse_password', ''),
                        'database': self.cold_config.get('clickhouse_database', 'marketprism_cold')
                    }
                }

                self.cold_client = UnifiedClickHouseWriter(cold_clickhouse_config)
                await self.cold_client.start()
                await self.cold_client.execute_query("SELECT 1")
                
                self.validation_results["cold_storage_connection"] = True
                self.logger.info("âœ… å†·ç«¯ClickHouseè¿æ¥éªŒè¯æˆåŠŸ")
            else:
                self.cold_client = self.hot_client
                self.validation_results["cold_storage_connection"] = True
                self.logger.info("ğŸ”„ å†·ç«¯ä½¿ç”¨ç›¸åŒçš„ClickHouseå®ä¾‹")
            
        except Exception as e:
            self.logger.error(f"âŒ ClickHouseè¿æ¥éªŒè¯å¤±è´¥: {e}")
            raise
    
    async def _initialize_storage_manager(self):
        """åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨"""
        try:
            self.logger.info("ğŸ—ï¸ åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨")
            
            # åˆ›å»ºçƒ­ç«¯é…ç½®
            hot_tier_config = TierConfig(
                tier=StorageTier.HOT,
                clickhouse_host=self.hot_config.get('clickhouse_host', 'localhost'),
                clickhouse_port=self.hot_config.get('clickhouse_port', 9000),
                clickhouse_user=self.hot_config.get('clickhouse_user', 'default'),
                clickhouse_password=self.hot_config.get('clickhouse_password', ''),
                clickhouse_database=self.hot_config.get('clickhouse_database', 'marketprism_hot'),
                retention_days=self.hot_config.get('retention_days', 3),
                batch_size=self.hot_config.get('batch_size', 100),  # æµ‹è¯•ç”¨å°æ‰¹æ¬¡
                flush_interval=self.hot_config.get('flush_interval', 1)
            )
            
            # åˆ›å»ºå†·ç«¯é…ç½®
            cold_tier_config = TierConfig(
                tier=StorageTier.COLD,
                clickhouse_host=self.cold_config.get('clickhouse_host', 'localhost'),
                clickhouse_port=self.cold_config.get('clickhouse_port', 9000),
                clickhouse_user=self.cold_config.get('clickhouse_user', 'default'),
                clickhouse_password=self.cold_config.get('clickhouse_password', ''),
                clickhouse_database=self.cold_config.get('clickhouse_database', 'marketprism_cold'),
                retention_days=self.cold_config.get('retention_days', 365),
                batch_size=self.cold_config.get('batch_size', 200),  # æµ‹è¯•ç”¨å°æ‰¹æ¬¡
                flush_interval=self.cold_config.get('flush_interval', 2)
            )
            
            # åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨
            self.storage_manager = TieredStorageManager(hot_tier_config, cold_tier_config)
            await self.storage_manager.initialize()
            
            self.logger.info("âœ… åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error("âŒ åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise
    
    async def _validate_data_flow(self):
        """éªŒè¯æ•°æ®æµ"""
        try:
            self.logger.info("ğŸ”„ éªŒè¯æ•°æ®æµ")
            
            # æµ‹è¯•æ•°æ®ç±»å‹
            data_types = ["orderbook", "trade", "funding_rate", "open_interest", 
                         "liquidation", "lsr", "volatility_index"]
            
            for data_type in data_types:
                try:
                    # ç”Ÿæˆæµ‹è¯•æ•°æ®
                    test_data = self._generate_test_data(data_type)
                    
                    # å‘å¸ƒåˆ°NATS
                    await self._publish_test_data(data_type, test_data)
                    
                    # ç­‰å¾…æ•°æ®å¤„ç†
                    await asyncio.sleep(2)
                    
                    # éªŒè¯çƒ­ç«¯å­˜å‚¨
                    hot_success = await self._verify_hot_storage(data_type, test_data)
                    
                    # è®°å½•ç»“æœ
                    self.validation_results["data_flow_validation"][data_type] = {
                        "nats_publish": True,
                        "hot_storage": hot_success,
                        "test_data_count": len(test_data) if isinstance(test_data, list) else 1
                    }
                    
                    self.logger.info("âœ… æ•°æ®æµéªŒè¯æˆåŠŸ", data_type=data_type)
                    
                except Exception as e:
                    self.validation_results["data_flow_validation"][data_type] = {
                        "error": str(e)
                    }
                    self.logger.error("âŒ æ•°æ®æµéªŒè¯å¤±è´¥", data_type=data_type, error=str(e))
            
        except Exception as e:
            self.logger.error("âŒ æ•°æ®æµéªŒè¯å¤±è´¥", error=str(e))
            raise
    
    def _generate_test_data(self, data_type: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        base_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "exchange": "test_exchange",
            "market_type": "spot",
            "symbol": "BTC-USDT",
            "data_source": "e2e_test"
        }
        
        if data_type == "orderbook":
            return [{
                **base_data,
                "last_update_id": 12345,
                "bids": json.dumps([["50000.00", "1.0"], ["49999.00", "2.0"]]),
                "asks": json.dumps([["50001.00", "1.5"], ["50002.00", "2.5"]]),
                "best_bid_price": 50000.00,
                "best_ask_price": 50001.00,
                "best_bid_quantity": 1.0,
                "best_ask_quantity": 1.5,
                "bids_count": 2,
                "asks_count": 2
            }]
        
        elif data_type == "trade":
            return [{
                **base_data,
                "trade_id": "test_trade_123",
                "price": 50000.50,
                "quantity": 0.1,
                "side": "buy",
                "is_maker": False,
                "trade_time": datetime.now(timezone.utc).isoformat()
            }]
        
        elif data_type == "funding_rate":
            return [{
                **base_data,
                "funding_rate": 0.0001,
                "funding_time": datetime.now(timezone.utc).isoformat(),
                "next_funding_time": (datetime.now(timezone.utc) + timedelta(hours=8)).isoformat(),
                "mark_price": 50000.00,
                "index_price": 49999.50
            }]
        
        elif data_type == "open_interest":
            return [{
                **base_data,
                "open_interest": 1000000.0,
                "open_interest_value": 50000000000.0,
                "count": 5000
            }]
        
        elif data_type == "liquidation":
            return [{
                **base_data,
                "side": "sell",
                "price": 49500.00,
                "quantity": 2.5,
                "liquidation_time": datetime.now(timezone.utc).isoformat()
            }]
        
        elif data_type == "lsr":
            return [{
                **base_data,
                "long_short_ratio": 1.25,
                "long_account": 55.5,
                "short_account": 44.5,
                "period": "1h"
            }]
        
        elif data_type == "volatility_index":
            return [{
                **base_data,
                "index_value": 75.5,
                "underlying_asset": "BTC",
                "maturity_date": (datetime.now(timezone.utc) + timedelta(days=30)).date().isoformat()
            }]
        
        else:
            return [base_data]
    
    async def _publish_test_data(self, data_type: str, test_data: List[Dict[str, Any]]):
        """å‘å¸ƒæµ‹è¯•æ•°æ®åˆ°NATS"""
        try:
            subject = f"{data_type}-data.test_exchange.spot.BTC-USDT"
            
            for data in test_data:
                message = json.dumps(data).encode()
                await self.jetstream.publish(subject, message)
            
            self.logger.debug("ğŸ“¤ æµ‹è¯•æ•°æ®å·²å‘å¸ƒ", data_type=data_type, count=len(test_data))
            
        except Exception as e:
            self.logger.error("âŒ å‘å¸ƒæµ‹è¯•æ•°æ®å¤±è´¥", data_type=data_type, error=str(e))
            raise
    
    async def _verify_hot_storage(self, data_type: str, test_data: List[Dict[str, Any]]) -> bool:
        """éªŒè¯çƒ­ç«¯å­˜å‚¨"""
        try:
            # ç­‰å¾…æ•°æ®å†™å…¥
            await asyncio.sleep(3)
            
            # æŸ¥è¯¢æ•°æ®
            table_name = self._get_table_name(data_type)
            query = f"""
                SELECT count() as count FROM {table_name}
                WHERE data_source = 'e2e_test'
                AND timestamp >= now() - INTERVAL 1 MINUTE
            """
            
            result = await self.hot_client.execute_query(query)
            
            if result and len(result) > 0:
                count = result[0].get('count', 0)
                expected_count = len(test_data)
                
                if count >= expected_count:
                    self.logger.info("âœ… çƒ­ç«¯å­˜å‚¨éªŒè¯æˆåŠŸ", 
                                   data_type=data_type, 
                                   count=count, 
                                   expected=expected_count)
                    return True
                else:
                    self.logger.warning("âš ï¸ çƒ­ç«¯å­˜å‚¨æ•°æ®ä¸å®Œæ•´", 
                                      data_type=data_type, 
                                      count=count, 
                                      expected=expected_count)
                    return False
            else:
                self.logger.error("âŒ çƒ­ç«¯å­˜å‚¨æŸ¥è¯¢æ— ç»“æœ", data_type=data_type)
                return False
            
        except Exception as e:
            self.logger.error("âŒ çƒ­ç«¯å­˜å‚¨éªŒè¯å¤±è´¥", data_type=data_type, error=str(e))
            return False
    
    def _get_table_name(self, data_type: str) -> str:
        """è·å–è¡¨å"""
        table_mapping = {
            "orderbook": "orderbooks",
            "trade": "trades",
            "funding_rate": "funding_rates",
            "open_interest": "open_interests",
            "liquidation": "liquidations",
            "lsr": "lsrs",
            "volatility_index": "volatility_indices"
        }
        return table_mapping.get(data_type, data_type)

    async def _validate_data_integrity(self):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        try:
            self.logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§")

            # éªŒè¯è¡¨ç»“æ„
            await self._validate_table_schemas()

            # éªŒè¯æ•°æ®æ ¼å¼
            await self._validate_data_formats()

            # éªŒè¯ç´¢å¼•æ€§èƒ½
            await self._validate_index_performance()

            self.logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")

        except Exception as e:
            self.logger.error("âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥", error=str(e))
            raise

    async def _validate_table_schemas(self):
        """éªŒè¯è¡¨ç»“æ„"""
        try:
            tables = ["orderbooks", "trades", "funding_rates", "open_interests",
                     "liquidations", "lsrs", "volatility_indices"]

            for table in tables:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                query = f"DESCRIBE TABLE {table}"
                result = await self.hot_client.execute_query(query)

                if result:
                    self.validation_results["data_integrity_validation"][f"{table}_schema"] = True
                    self.logger.info("âœ… è¡¨ç»“æ„éªŒè¯æˆåŠŸ", table=table, columns=len(result))
                else:
                    self.validation_results["data_integrity_validation"][f"{table}_schema"] = False
                    self.logger.error("âŒ è¡¨ç»“æ„éªŒè¯å¤±è´¥", table=table)

        except Exception as e:
            self.logger.error("âŒ è¡¨ç»“æ„éªŒè¯å¤±è´¥", error=str(e))
            raise

    async def _validate_data_formats(self):
        """éªŒè¯æ•°æ®æ ¼å¼"""
        try:
            # æ£€æŸ¥æ—¶é—´æˆ³æ ¼å¼
            query = """
                SELECT
                    toTypeName(timestamp) as timestamp_type,
                    min(timestamp) as min_timestamp,
                    max(timestamp) as max_timestamp
                FROM trades
                WHERE data_source = 'e2e_test'
                LIMIT 1
            """

            result = await self.hot_client.execute_query(query)

            if result and len(result) > 0:
                timestamp_type = result[0].get('timestamp_type')
                if 'DateTime64' in timestamp_type:
                    self.validation_results["data_integrity_validation"]["timestamp_format"] = True
                    self.logger.info("âœ… æ—¶é—´æˆ³æ ¼å¼éªŒè¯æˆåŠŸ", type=timestamp_type)
                else:
                    self.validation_results["data_integrity_validation"]["timestamp_format"] = False
                    self.logger.error("âŒ æ—¶é—´æˆ³æ ¼å¼éªŒè¯å¤±è´¥", type=timestamp_type)

            # æ£€æŸ¥æ•°æ®ç²¾åº¦
            query = """
                SELECT
                    toTypeName(price) as price_type,
                    toTypeName(quantity) as quantity_type
                FROM trades
                WHERE data_source = 'e2e_test'
                LIMIT 1
            """

            result = await self.hot_client.execute_query(query)

            if result and len(result) > 0:
                price_type = result[0].get('price_type')
                quantity_type = result[0].get('quantity_type')

                if 'Decimal64' in price_type and 'Decimal64' in quantity_type:
                    self.validation_results["data_integrity_validation"]["decimal_precision"] = True
                    self.logger.info("âœ… æ•°æ®ç²¾åº¦éªŒè¯æˆåŠŸ",
                                   price_type=price_type,
                                   quantity_type=quantity_type)
                else:
                    self.validation_results["data_integrity_validation"]["decimal_precision"] = False
                    self.logger.error("âŒ æ•°æ®ç²¾åº¦éªŒè¯å¤±è´¥",
                                    price_type=price_type,
                                    quantity_type=quantity_type)

        except Exception as e:
            self.logger.error("âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥", error=str(e))
            raise

    async def _validate_index_performance(self):
        """éªŒè¯ç´¢å¼•æ€§èƒ½"""
        try:
            # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
            start_time = time.time()

            query = """
                SELECT count()
                FROM trades
                WHERE timestamp >= now() - INTERVAL 1 HOUR
                AND exchange = 'test_exchange'
                AND symbol = 'BTC-USDT'
            """

            result = await self.hot_client.execute_query(query)

            query_time = time.time() - start_time

            self.validation_results["data_integrity_validation"]["index_performance"] = {
                "query_time_seconds": query_time,
                "result_count": result[0].get('count()') if result else 0
            }

            if query_time < 1.0:  # æŸ¥è¯¢æ—¶é—´å°äº1ç§’
                self.logger.info("âœ… ç´¢å¼•æ€§èƒ½éªŒè¯æˆåŠŸ", query_time=query_time)
            else:
                self.logger.warning("âš ï¸ ç´¢å¼•æ€§èƒ½è¾ƒæ…¢", query_time=query_time)

        except Exception as e:
            self.logger.error("âŒ ç´¢å¼•æ€§èƒ½éªŒè¯å¤±è´¥", error=str(e))
            raise

    async def _performance_test(self):
        """æ€§èƒ½æµ‹è¯•"""
        try:
            self.logger.info("âš¡ å¼€å§‹æ€§èƒ½æµ‹è¯•")

            # æ‰¹é‡å†™å…¥æµ‹è¯•
            await self._test_batch_write_performance()

            # æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
            await self._test_query_performance()

            # æ•°æ®ä¼ è¾“æ€§èƒ½æµ‹è¯•
            await self._test_transfer_performance()

            self.logger.info("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")

        except Exception as e:
            self.logger.error("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥", error=str(e))
            raise

    async def _test_batch_write_performance(self):
        """æµ‹è¯•æ‰¹é‡å†™å…¥æ€§èƒ½"""
        try:
            # ç”Ÿæˆå¤§é‡æµ‹è¯•æ•°æ®
            test_data = []
            for i in range(1000):
                test_data.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "exchange": "perf_test",
                    "market_type": "spot",
                    "symbol": "BTC-USDT",
                    "trade_id": f"perf_test_{i}",
                    "price": 50000.0 + i,
                    "quantity": 0.1,
                    "side": "buy" if i % 2 == 0 else "sell",
                    "is_maker": False,
                    "trade_time": datetime.now(timezone.utc).isoformat(),
                    "data_source": "perf_test"
                })

            # æµ‹è¯•å†™å…¥æ€§èƒ½
            start_time = time.time()

            success = await self.storage_manager.store_to_hot("trade", test_data)

            write_time = time.time() - start_time

            self.validation_results["performance_metrics"]["batch_write"] = {
                "records_count": len(test_data),
                "write_time_seconds": write_time,
                "records_per_second": len(test_data) / write_time if write_time > 0 else 0,
                "success": success
            }

            self.logger.info("âœ… æ‰¹é‡å†™å…¥æ€§èƒ½æµ‹è¯•å®Œæˆ",
                           records=len(test_data),
                           time=write_time,
                           rps=len(test_data) / write_time if write_time > 0 else 0)

        except Exception as e:
            self.logger.error("âŒ æ‰¹é‡å†™å…¥æ€§èƒ½æµ‹è¯•å¤±è´¥", error=str(e))
            raise

    async def _test_query_performance(self):
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        try:
            queries = [
                ("simple_count", "SELECT count() FROM trades WHERE data_source = 'perf_test'"),
                ("time_range", "SELECT count() FROM trades WHERE timestamp >= now() - INTERVAL 1 HOUR"),
                ("complex_filter", """
                    SELECT exchange, symbol, count() as trade_count, avg(price) as avg_price
                    FROM trades
                    WHERE data_source = 'perf_test'
                    GROUP BY exchange, symbol
                """)
            ]

            query_results = {}

            for query_name, query_sql in queries:
                start_time = time.time()
                result = await self.hot_client.execute_query(query_sql)
                query_time = time.time() - start_time

                query_results[query_name] = {
                    "query_time_seconds": query_time,
                    "result_count": len(result) if result else 0
                }

                self.logger.info("âœ… æŸ¥è¯¢æ€§èƒ½æµ‹è¯•",
                               query=query_name,
                               time=query_time,
                               results=len(result) if result else 0)

            self.validation_results["performance_metrics"]["query_performance"] = query_results

        except Exception as e:
            self.logger.error("âŒ æŸ¥è¯¢æ€§èƒ½æµ‹è¯•å¤±è´¥", error=str(e))
            raise

    async def _test_transfer_performance(self):
        """æµ‹è¯•æ•°æ®ä¼ è¾“æ€§èƒ½"""
        try:
            # è°ƒåº¦ä¸€ä¸ªå°çš„ä¼ è¾“ä»»åŠ¡
            start_time = datetime.now(timezone.utc) - timedelta(minutes=5)
            end_time = datetime.now(timezone.utc)

            transfer_start = time.time()

            task_id = await self.storage_manager.schedule_data_transfer(
                "trade", "perf_test", "BTC-USDT", start_time, end_time
            )

            # ç­‰å¾…ä¼ è¾“å®Œæˆ
            timeout = 30  # 30ç§’è¶…æ—¶
            elapsed = 0

            while elapsed < timeout:
                status = self.storage_manager.get_transfer_task_status(task_id)
                if status and status['status'] in ['completed', 'failed']:
                    break
                await asyncio.sleep(1)
                elapsed += 1

            transfer_time = time.time() - transfer_start

            # è·å–æœ€ç»ˆçŠ¶æ€
            final_status = self.storage_manager.get_transfer_task_status(task_id)

            self.validation_results["performance_metrics"]["data_transfer"] = {
                "task_id": task_id,
                "transfer_time_seconds": transfer_time,
                "status": final_status['status'] if final_status else 'timeout',
                "records_transferred": final_status['records_count'] if final_status else 0
            }

            self.logger.info("âœ… æ•°æ®ä¼ è¾“æ€§èƒ½æµ‹è¯•å®Œæˆ",
                           task_id=task_id,
                           time=transfer_time,
                           status=final_status['status'] if final_status else 'timeout')

        except Exception as e:
            self.logger.error("âŒ æ•°æ®ä¼ è¾“æ€§èƒ½æµ‹è¯•å¤±è´¥", error=str(e))
            raise

    async def _generate_validation_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        try:
            self.logger.info("ğŸ“Š ç”ŸæˆéªŒè¯æŠ¥å‘Š")

            # è®¡ç®—æ€»ä½“æˆåŠŸç‡
            total_tests = 0
            passed_tests = 0

            # è¿æ¥æµ‹è¯•
            connection_tests = ["nats_connection", "hot_storage_connection", "cold_storage_connection"]
            for test in connection_tests:
                total_tests += 1
                if self.validation_results.get(test, False):
                    passed_tests += 1

            # æ•°æ®æµæµ‹è¯•
            data_flow_results = self.validation_results.get("data_flow_validation", {})
            for data_type, result in data_flow_results.items():
                if isinstance(result, dict) and "error" not in result:
                    total_tests += 1
                    if result.get("hot_storage", False):
                        passed_tests += 1

            # æ•°æ®å®Œæ•´æ€§æµ‹è¯•
            integrity_results = self.validation_results.get("data_integrity_validation", {})
            for test_name, result in integrity_results.items():
                if isinstance(result, bool):
                    total_tests += 1
                    if result:
                        passed_tests += 1

            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

            # ç”ŸæˆæŠ¥å‘Š
            report = {
                "validation_timestamp": datetime.now(timezone.utc).isoformat(),
                "overall_success_rate": success_rate,
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": total_tests - passed_tests,
                "detailed_results": self.validation_results
            }

            # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
            report_file = Path(__file__).parent.parent / "logs" / f"validation_report_{int(time.time())}.json"
            report_file.parent.mkdir(parents=True, exist_ok=True)

            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            # æ‰“å°æ‘˜è¦
            self.logger.info("ğŸ“‹ éªŒè¯æŠ¥å‘Šæ‘˜è¦",
                           success_rate=f"{success_rate:.1f}%",
                           passed=passed_tests,
                           total=total_tests,
                           report_file=str(report_file))

            # æ‰“å°è¯¦ç»†ç»“æœ
            print("\n" + "="*80)
            print("ğŸ¯ MarketPrism åˆ†å±‚æ•°æ®å­˜å‚¨ç«¯åˆ°ç«¯éªŒè¯æŠ¥å‘Š")
            print("="*80)
            print(f"ğŸ“Š æ€»ä½“æˆåŠŸç‡: {success_rate:.1f}% ({passed_tests}/{total_tests})")
            print(f"â° éªŒè¯æ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
            print()

            # è¿æ¥æµ‹è¯•ç»“æœ
            print("ğŸ”— è¿æ¥æµ‹è¯•:")
            for test in connection_tests:
                status = "âœ… é€šè¿‡" if self.validation_results.get(test, False) else "âŒ å¤±è´¥"
                print(f"  - {test}: {status}")
            print()

            # æ•°æ®æµæµ‹è¯•ç»“æœ
            print("ğŸ”„ æ•°æ®æµæµ‹è¯•:")
            for data_type, result in data_flow_results.items():
                if isinstance(result, dict) and "error" not in result:
                    status = "âœ… é€šè¿‡" if result.get("hot_storage", False) else "âŒ å¤±è´¥"
                    count = result.get("test_data_count", 0)
                    print(f"  - {data_type}: {status} ({count} æ¡è®°å½•)")
                else:
                    print(f"  - {data_type}: âŒ å¤±è´¥ ({result.get('error', 'æœªçŸ¥é”™è¯¯')})")
            print()

            # æ€§èƒ½æµ‹è¯•ç»“æœ
            print("âš¡ æ€§èƒ½æµ‹è¯•:")
            perf_metrics = self.validation_results.get("performance_metrics", {})

            if "batch_write" in perf_metrics:
                batch_write = perf_metrics["batch_write"]
                print(f"  - æ‰¹é‡å†™å…¥: {batch_write['records_count']} æ¡è®°å½•, "
                      f"{batch_write['write_time_seconds']:.2f}s, "
                      f"{batch_write['records_per_second']:.0f} è®°å½•/ç§’")

            if "query_performance" in perf_metrics:
                query_perf = perf_metrics["query_performance"]
                for query_name, metrics in query_perf.items():
                    print(f"  - {query_name}: {metrics['query_time_seconds']:.3f}s, "
                          f"{metrics['result_count']} æ¡ç»“æœ")

            if "data_transfer" in perf_metrics:
                transfer = perf_metrics["data_transfer"]
                print(f"  - æ•°æ®ä¼ è¾“: {transfer['status']}, "
                      f"{transfer['transfer_time_seconds']:.2f}s, "
                      f"{transfer['records_transferred']} æ¡è®°å½•")

            print()
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
            print("="*80)

            return report

        except Exception as e:
            self.logger.error("âŒ ç”ŸæˆéªŒè¯æŠ¥å‘Šå¤±è´¥", error=str(e))
            raise

    async def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.logger.info("ğŸ§¹ æ¸…ç†éªŒè¯èµ„æº")

            # æ¸…ç†æµ‹è¯•æ•°æ®
            if self.hot_client:
                try:
                    cleanup_queries = [
                        "DELETE FROM trades WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM orderbooks WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM funding_rates WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM open_interests WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM liquidations WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM lsrs WHERE data_source IN ('e2e_test', 'perf_test')",
                        "DELETE FROM volatility_indices WHERE data_source IN ('e2e_test', 'perf_test')"
                    ]

                    for query in cleanup_queries:
                        try:
                            await self.hot_client.execute_query(query)
                        except Exception as e:
                            self.logger.warning("âš ï¸ æ¸…ç†æµ‹è¯•æ•°æ®å¤±è´¥", query=query[:50], error=str(e))

                    self.logger.info("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
                except Exception as e:
                    self.logger.warning("âš ï¸ æµ‹è¯•æ•°æ®æ¸…ç†å¤±è´¥", error=str(e))

            # å…³é—­è¿æ¥
            if self.storage_manager:
                await self.storage_manager.close()

            if self.hot_client and self.hot_client != self.cold_client:
                await self.hot_client.close()

            if self.cold_client:
                await self.cold_client.close()

            if self.nats_client:
                await self.nats_client.close()

            self.logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            self.logger.error("âŒ èµ„æºæ¸…ç†å¤±è´¥", error=str(e))


async def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ MarketPrism åˆ†å±‚æ•°æ®å­˜å‚¨ç«¯åˆ°ç«¯éªŒè¯")
        print("="*60)

        # åŠ è½½é…ç½®
        config_path = Path(__file__).parent.parent / "config" / "tiered_storage_config.yaml"

        if not config_path.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            sys.exit(1)

        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # è¿è¡ŒéªŒè¯
        validator = EndToEndValidator(config)
        await validator.run_validation()

        print("\nğŸ‰ ç«¯åˆ°ç«¯éªŒè¯å®Œæˆï¼")

    except KeyboardInterrupt:
        print("\nâš ï¸ éªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç«¯åˆ°ç«¯éªŒè¯å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
