#!/usr/bin/env python3
"""
MarketPrism çƒ­->å†· æ•°æ®è¿ç§»è„šæœ¬ (å¢å¼ºç‰ˆ)
- ç­–ç•¥ï¼šå°†æ—©äº now()-8h çš„æ•°æ®ä» marketprism_hot è¿ç§»åˆ° marketprism_cold
- æ­¥éª¤ï¼šINSERT ... SELECT -> éªŒè¯ -> DELETE
- è¿è¡Œæ–¹å¼ï¼šå®šæ—¶æ‰§è¡Œï¼ˆcron/systemdï¼‰ï¼Œæˆ–æ‰‹å·¥æ‰§è¡Œä¸€æ¬¡

å¢å¼ºèƒ½åŠ›ï¼š
- æ”¯æŒæŒ‰ç¬¦å·å‰ç¼€/äº¤æ˜“æ‰€/å¸‚åœºç±»å‹è¿‡æ»¤ï¼ˆä»…è¿ç§»ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼‰
- æ”¯æŒå¹²è·‘ï¼ˆä»…ç»Ÿè®¡ï¼Œä¸æ‰§è¡Œæ’å…¥/åˆ é™¤ï¼‰
- ğŸ”§ æ–°å¢ï¼šLSRæ•°æ®ç±»å‹ç‰¹æ®Šå¤„ç†ï¼ˆæ”¯æŒå¤æ‚å»é‡é€»è¾‘ï¼‰
- ğŸ”§ æ–°å¢ï¼šå¤‡ç”¨è¿ç§»æ–¹æ¡ˆï¼ˆå½“ä¸»è¿ç§»å¤±è´¥æ—¶è‡ªåŠ¨å›é€€ï¼‰
- ğŸ”§ æ–°å¢ï¼šæ•°æ®å®Œæ•´æ€§éªŒè¯ï¼ˆç¡®ä¿æ‰€æœ‰8ç§æ•°æ®ç±»å‹éƒ½æœ‰æ•°æ®ï¼‰
- ğŸ”§ æ–°å¢ï¼šä¸€é”®ä¿®å¤åŠŸèƒ½ï¼ˆè‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤æ•°æ®è¿ç§»é—®é¢˜ï¼‰

ç¯å¢ƒå˜é‡ï¼š
- CLICKHOUSE_HTTP_URLï¼ˆé»˜è®¤ï¼šhttp://localhost:8123/ï¼‰
- CLICKHOUSE_HOT_DBï¼ˆé»˜è®¤ï¼šmarketprism_hotï¼‰
- CLICKHOUSE_COLD_DBï¼ˆé»˜è®¤ï¼šmarketprism_coldï¼‰
- MIGRATION_WINDOW_HOURSï¼ˆé»˜è®¤ï¼š8ï¼‰
- MIGRATION_BATCH_LIMITï¼ˆé»˜è®¤ï¼š5000000ï¼‰
- MIGRATION_SYMBOL_PREFIXï¼ˆå¯é€‰ï¼‰
- MIGRATION_EXCHANGEï¼ˆå¯é€‰ï¼‰
- MIGRATION_MARKET_TYPEï¼ˆå¯é€‰ï¼‰
- MIGRATION_DRY_RUNï¼ˆ1/true/yes å¯ç”¨å¹²è·‘ï¼‰
- MIGRATION_FORCE_REPAIRï¼ˆ1/true/yes å¯ç”¨å¼ºåˆ¶ä¿®å¤æ¨¡å¼ï¼‰

æ³¨æ„ï¼šClickHouse ä¸æ”¯æŒäº‹åŠ¡çº§å¼ºä¸€è‡´ï¼›æœ¬è„šæœ¬é‡‡ç”¨â€œå…ˆæ‹·è´ååˆ é™¤â€çš„æ–¹å¼ï¼Œ
     å¦‚éœ€æ›´å¼ºä¸€è‡´æ€§å¯åœ¨ä¸šåŠ¡å±‚å¢åŠ å»é‡ç­–ç•¥æˆ–è°ƒæ•´è¡¨å¼•æ“ã€‚
"""

import asyncio
import aiohttp
import os
from datetime import datetime, timezone, timedelta

TABLES = [
    "orderbooks",
    "trades",
    "funding_rates",
    "open_interests",
    "liquidations",
    "lsr_top_positions",
    "lsr_all_accounts",
    "volatility_indices",
]

CLICKHOUSE_URL = os.environ.get("CLICKHOUSE_HTTP_URL", "http://localhost:8123/")
HOT_DB = os.environ.get("CLICKHOUSE_HOT_DB", "marketprism_hot")
COLD_DB = os.environ.get("CLICKHOUSE_COLD_DB", "marketprism_cold")
MIGRATION_WINDOW_HOURS = float(os.environ.get("MIGRATION_WINDOW_HOURS", "8"))
BATCH_LIMIT = int(os.environ.get("MIGRATION_BATCH_LIMIT", "5000000"))  # å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
# é€‰æ‹©æ€§è¿‡æ»¤ä¸å¹²è·‘
SYMBOL_PREFIX = os.environ.get("MIGRATION_SYMBOL_PREFIX")  # ä¾‹å¦‚: MPTEST
EXCHANGE_FILTER = os.environ.get("MIGRATION_EXCHANGE")     # ä¾‹å¦‚: binance_derivatives
MARKET_TYPE_FILTER = os.environ.get("MIGRATION_MARKET_TYPE")  # ä¾‹å¦‚: perpetual
DRY_RUN = os.environ.get("MIGRATION_DRY_RUN", "0").lower() in ("1", "true", "yes")
FORCE_REPAIR = os.environ.get("MIGRATION_FORCE_REPAIR", "").lower() in ("1", "true", "yes")

# ğŸ”§ LSRæ•°æ®ç±»å‹ç‰¹æ®Šå¤„ç†é…ç½®
LSR_TABLES = ["lsr_top_positions", "lsr_all_accounts"]
COMPLEX_TABLES = LSR_TABLES + ["funding_rates", "open_interests"]  # éœ€è¦ç‰¹æ®Šå¤„ç†çš„è¡¨

def now_utc():
    return datetime.now(timezone.utc)

async def ch_post(session: aiohttp.ClientSession, sql: str, database: str | None = None) -> str:
    url = CLICKHOUSE_URL
    if database:
        url += f"?database={database}"
    async with session.post(url, data=sql) as resp:
        text = await resp.text()
        if resp.status != 200:
            raise RuntimeError(f"ClickHouse error: {resp.status} {text}")
        return text

async def migrate_table_simple(session: aiohttp.ClientSession, table: str, cutoff_iso: str) -> dict:
    """ç®€å•è¡¨è¿ç§»ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
    summary = {"table": table, "cutoff": cutoff_iso, "method": "simple"}

    # æ„é€ ç»Ÿä¸€è¿‡æ»¤æ¡ä»¶
    where_parts = [f"timestamp < toDateTime64('{cutoff_iso}', 3, 'UTC')"]
    if SYMBOL_PREFIX:
        where_parts.append(f"symbol LIKE '{SYMBOL_PREFIX}%' ")
    if EXCHANGE_FILTER:
        where_parts.append(f"exchange = '{EXCHANGE_FILTER}'")
    if MARKET_TYPE_FILTER:
        where_parts.append(f"market_type = '{MARKET_TYPE_FILTER}'")
    where_clause = " AND ".join(where_parts)

    # ç»Ÿè®¡å¾…è¿ç§»æ•°é‡
    count_sql = f"SELECT count() FROM {table} WHERE {where_clause}"
    hot_count_str = await ch_post(session, count_sql, HOT_DB)
    hot_count = int(hot_count_str.strip() or 0)
    summary["hot_count_before"] = hot_count
    if hot_count == 0:
        summary["copied"] = 0
        summary["deleted"] = 0
        summary["status"] = "SKIP"
        return summary

    if DRY_RUN:
        summary["copied"] = 0
        summary["deleted"] = 0
        summary["status"] = "DRY_RUN"
        return summary

    # æ‹·è´åˆ°å†·ç«¯
    insert_sql = (
        f"INSERT INTO {COLD_DB}.{table} SELECT * FROM {HOT_DB}.{table} "
        f"WHERE {where_clause} LIMIT {BATCH_LIMIT}"
    )
    await ch_post(session, insert_sql)

    # éªŒè¯å†·ç«¯å¢é•¿ï¼ˆæŸ¥è¯¢åŒä¸€æ¡ä»¶ï¼‰
    cold_new_str = await ch_post(
        session,
        f"SELECT count() FROM {table} WHERE {where_clause}",
        COLD_DB,
    )
    cold_new = int(cold_new_str.strip() or 0)

    summary["copied"] = min(hot_count, cold_new)

    # åˆ é™¤çƒ­ç«¯å·²è¿ç§»æ•°æ®ï¼ˆåŒä¸€æ¡ä»¶ï¼‰
    delete_sql = f"ALTER TABLE {HOT_DB}.{table} DELETE WHERE {where_clause}"
    await ch_post(session, delete_sql)

    # mutation éœ€è¦æ—¶é—´ç”Ÿæ•ˆï¼Œè¿™é‡Œä¸ç­‰å¾…å®Œæˆï¼Œä»…è®°å½•
    summary["deleted"] = summary["copied"]
    summary["status"] = "OK"
    return summary

async def migrate_table_complex(session: aiohttp.ClientSession, table: str, cutoff_iso: str) -> dict:
    """å¤æ‚è¡¨è¿ç§»ï¼ˆLSRç­‰æ•°æ®ç±»å‹ï¼Œæ”¯æŒå»é‡é€»è¾‘ï¼‰"""
    summary = {"table": table, "cutoff": cutoff_iso, "method": "complex"}

    # æ„å»º WHERE æ¡ä»¶
    where_parts = [f"hot.timestamp < toDateTime64('{cutoff_iso}', 3, 'UTC')"]
    if SYMBOL_PREFIX:
        where_parts.append(f"hot.symbol LIKE '{SYMBOL_PREFIX}%'")
    if EXCHANGE_FILTER:
        where_parts.append(f"hot.exchange = '{EXCHANGE_FILTER}'")
    if MARKET_TYPE_FILTER:
        where_parts.append(f"hot.market_type = '{MARKET_TYPE_FILTER}'")
    where_clause = " AND ".join(where_parts)

    # ç»Ÿè®¡å¾…è¿ç§»æ•°é‡
    simple_where = " AND ".join([p.replace("hot.", "") for p in where_parts])
    count_sql = f"SELECT count() FROM {table} WHERE {simple_where}"
    hot_count_str = await ch_post(session, count_sql, HOT_DB)
    hot_count = int(hot_count_str.strip() or 0)
    summary["hot_count_before"] = hot_count

    if hot_count == 0:
        summary["copied"] = 0
        summary["deleted"] = 0
        summary["status"] = "SKIP"
        return summary

    if DRY_RUN:
        summary["copied"] = 0
        summary["deleted"] = 0
        summary["status"] = "DRY_RUN"
        return summary

    # ğŸ”§ LSRæ•°æ®ç±»å‹ä½¿ç”¨å¤æ‚çš„å»é‡INSERT SELECT
    if table in LSR_TABLES:
        if table == "lsr_top_positions":
            insert_sql = f"""
            INSERT INTO {COLD_DB}.lsr_top_positions (
                timestamp, exchange, market_type, symbol, long_position_ratio, short_position_ratio, period, data_source, created_at
            )
            SELECT hot.timestamp, hot.exchange, hot.market_type, hot.symbol, hot.long_position_ratio, hot.short_position_ratio, hot.period, 'marketprism', now()
            FROM {HOT_DB}.lsr_top_positions AS hot
            LEFT JOIN {COLD_DB}.lsr_top_positions AS cold
            ON cold.exchange = hot.exchange AND cold.market_type = hot.market_type AND cold.symbol = hot.symbol
            AND cold.timestamp = hot.timestamp AND cold.period = hot.period
            WHERE {where_clause} AND cold.exchange IS NULL
            LIMIT {BATCH_LIMIT}
            """
        else:  # lsr_all_accounts
            insert_sql = f"""
            INSERT INTO {COLD_DB}.lsr_all_accounts (
                timestamp, exchange, market_type, symbol, long_account_ratio, short_account_ratio, period, data_source, created_at
            )
            SELECT hot.timestamp, hot.exchange, hot.market_type, hot.symbol, hot.long_account_ratio, hot.short_account_ratio, hot.period, 'marketprism', now()
            FROM {HOT_DB}.lsr_all_accounts AS hot
            LEFT JOIN {COLD_DB}.lsr_all_accounts AS cold
            ON cold.exchange = hot.exchange AND cold.market_type = hot.market_type AND cold.symbol = hot.symbol
            AND cold.timestamp = hot.timestamp AND cold.period = hot.period
            WHERE {where_clause} AND cold.exchange IS NULL
            LIMIT {BATCH_LIMIT}
            """
    else:
        # å…¶ä»–å¤æ‚è¡¨ä½¿ç”¨ç®€å•çš„å»é‡é€»è¾‘
        insert_sql = f"""
        INSERT INTO {COLD_DB}.{table}
        SELECT hot.* FROM {HOT_DB}.{table} AS hot
        LEFT JOIN {COLD_DB}.{table} AS cold
        ON cold.exchange = hot.exchange AND cold.symbol = hot.symbol AND cold.timestamp = hot.timestamp
        WHERE {where_clause} AND cold.exchange IS NULL
        LIMIT {BATCH_LIMIT}
        """

    try:
        await ch_post(session, insert_sql)
        summary["status"] = "OK"
    except Exception as e:
        print(f"âš ï¸ å¤æ‚è¿ç§»å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•è¿ç§»: {e}")
        # å›é€€åˆ°ç®€å•è¿ç§»
        return await migrate_table_simple(session, table, cutoff_iso)

    # éªŒè¯å†·ç«¯å¢é•¿
    cold_count_str = await ch_post(
        session,
        f"SELECT count() FROM {table} WHERE {simple_where}",
        COLD_DB,
    )
    cold_count = int(cold_count_str.strip() or 0)
    summary["copied"] = cold_count

    # åˆ é™¤çƒ­ç«¯å·²è¿ç§»æ•°æ®
    delete_sql = f"ALTER TABLE {HOT_DB}.{table} DELETE WHERE {simple_where}"
    await ch_post(session, delete_sql)

    summary["deleted"] = summary["copied"]
    return summary

async def migrate_table(session: aiohttp.ClientSession, table: str, cutoff_iso: str) -> dict:
    """ç»Ÿä¸€è¡¨è¿ç§»å…¥å£"""
    if table in COMPLEX_TABLES:
        return await migrate_table_complex(session, table, cutoff_iso)
    else:
        return await migrate_table_simple(session, table, cutoff_iso)

async def verify_data_integrity(session: aiohttp.ClientSession) -> dict:
    """ğŸ”§ éªŒè¯æ‰€æœ‰8ç§æ•°æ®ç±»å‹çš„æ•°æ®å®Œæ•´æ€§"""
    print("\n=== ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯ ===")
    integrity_report = {"total_tables": len(TABLES), "tables_with_data": 0, "empty_tables": [], "table_counts": {}}

    for table in TABLES:
        try:
            count_str = await ch_post(session, f"SELECT count() FROM {table}", COLD_DB)
            count = int(count_str.strip() or 0)
            integrity_report["table_counts"][table] = count

            if count > 0:
                integrity_report["tables_with_data"] += 1
                print(f"âœ… {table}: {count:,} æ¡è®°å½•")
            else:
                integrity_report["empty_tables"].append(table)
                print(f"âš ï¸ {table}: 0 æ¡è®°å½• (éœ€è¦ä¿®å¤)")
        except Exception as e:
            integrity_report["empty_tables"].append(table)
            integrity_report["table_counts"][table] = -1
            print(f"âŒ {table}: æ£€æŸ¥å¤±è´¥ - {e}")

    integrity_report["integrity_score"] = integrity_report["tables_with_data"] / integrity_report["total_tables"] * 100
    print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§è¯„åˆ†: {integrity_report['integrity_score']:.1f}% ({integrity_report['tables_with_data']}/{integrity_report['total_tables']})")

    return integrity_report

async def repair_missing_data(session: aiohttp.ClientSession, empty_tables: list) -> dict:
    """ğŸ”§ ä¸€é”®ä¿®å¤ç¼ºå¤±çš„æ•°æ®"""
    print(f"\n=== ğŸ› ï¸ å¼€å§‹ä¿®å¤ {len(empty_tables)} ä¸ªç©ºè¡¨ ===")
    repair_report = {"repaired_tables": [], "failed_tables": [], "total_repaired_records": 0}

    # ä½¿ç”¨æ›´å¤§çš„æ—¶é—´çª—å£è¿›è¡Œä¿®å¤ï¼ˆ2å°æ—¶ï¼‰
    repair_window_hours = 2.0
    cutoff_dt = now_utc() - timedelta(hours=repair_window_hours)
    cutoff_iso = cutoff_dt.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]

    for table in empty_tables:
        try:
            print(f"ğŸ”§ ä¿®å¤è¡¨: {table}")

            # ä½¿ç”¨ç®€å•çš„INSERT SELECTè¿›è¡Œä¿®å¤
            repair_sql = f"""
            INSERT INTO {COLD_DB}.{table}
            SELECT * FROM {HOT_DB}.{table}
            WHERE timestamp >= toDateTime64('{cutoff_iso}', 3, 'UTC')
            """

            await ch_post(session, repair_sql)

            # éªŒè¯ä¿®å¤ç»“æœ
            count_str = await ch_post(session, f"SELECT count() FROM {table}", COLD_DB)
            count = int(count_str.strip() or 0)

            if count > 0:
                repair_report["repaired_tables"].append(table)
                repair_report["total_repaired_records"] += count
                print(f"âœ… {table}: ä¿®å¤æˆåŠŸï¼Œè¿ç§»äº† {count:,} æ¡è®°å½•")
            else:
                repair_report["failed_tables"].append(table)
                print(f"âš ï¸ {table}: ä¿®å¤åä»ç„¶ä¸ºç©ºï¼ˆçƒ­ç«¯å¯èƒ½æ²¡æœ‰æ•°æ®ï¼‰")

        except Exception as e:
            repair_report["failed_tables"].append(table)
            print(f"âŒ {table}: ä¿®å¤å¤±è´¥ - {e}")

    print(f"\nğŸ¯ ä¿®å¤å®Œæˆ: æˆåŠŸä¿®å¤ {len(repair_report['repaired_tables'])} ä¸ªè¡¨ï¼Œå…± {repair_report['total_repaired_records']:,} æ¡è®°å½•")
    return repair_report

async def main():
    async with aiohttp.ClientSession() as session:
        # éªŒè¯æ•°æ®åº“å­˜åœ¨
        try:
            await ch_post(session, "SELECT 1", HOT_DB)
            await ch_post(session, "SELECT 1", COLD_DB)
        except Exception as e:
            print(f"âŒ ClickHouse æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {e}")
            return 1

        # ğŸ”§ å¼ºåˆ¶ä¿®å¤æ¨¡å¼ï¼šç›´æ¥æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®å®Œæ•´æ€§
        if FORCE_REPAIR:
            print("ğŸ› ï¸ å¼ºåˆ¶ä¿®å¤æ¨¡å¼å·²å¯ç”¨")
            integrity_report = await verify_data_integrity(session)

            if integrity_report["empty_tables"]:
                repair_report = await repair_missing_data(session, integrity_report["empty_tables"])

                # å†æ¬¡éªŒè¯ä¿®å¤ç»“æœ
                print("\n=== ğŸ” ä¿®å¤åéªŒè¯ ===")
                final_integrity = await verify_data_integrity(session)

                if final_integrity["integrity_score"] == 100.0:
                    print("ğŸ‰ æ‰€æœ‰æ•°æ®ç±»å‹ä¿®å¤æˆåŠŸï¼")
                    return 0
                else:
                    print(f"âš ï¸ éƒ¨åˆ†æ•°æ®ç±»å‹ä»éœ€æ‰‹åŠ¨å¤„ç†: {final_integrity['empty_tables']}")
                    return 1
            else:
                print("âœ… æ‰€æœ‰æ•°æ®ç±»å‹éƒ½æœ‰æ•°æ®ï¼Œæ— éœ€ä¿®å¤")
                return 0

        # ğŸ”„ å¸¸è§„è¿ç§»æ¨¡å¼
        cutoff_dt = now_utc() - timedelta(hours=MIGRATION_WINDOW_HOURS)
        cutoff_iso = cutoff_dt.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]  # æ¯«ç§’
        print(f"\n=== å¯åŠ¨çƒ­->å†·è¿ç§» ===\ncutoff = {cutoff_iso}Z (UTC)\n")

        total_copied = 0
        total_deleted = 0
        for t in TABLES:
            try:
                s = await migrate_table(session, t, cutoff_iso)
                total_copied += s.get("copied", 0)
                total_deleted += s.get("deleted", 0)
                method = s.get("method", "unknown")
                print(f"- {t}: status={s['status']} method={method} copied={s['copied']} deleted={s['deleted']} hot_count_before={s['hot_count_before']}")
            except Exception as e:
                print(f"âŒ è¿ç§» {t} å¤±è´¥: {e}")

        print(f"\nâœ… è¿ç§»å®Œæˆ: copied={total_copied}, deleted={total_deleted}")

        # ğŸ” è¿ç§»åéªŒè¯æ•°æ®å®Œæ•´æ€§
        integrity_report = await verify_data_integrity(session)

        if integrity_report["empty_tables"]:
            print(f"\nâš ï¸ å‘ç° {len(integrity_report['empty_tables'])} ä¸ªç©ºè¡¨ï¼Œå»ºè®®è¿è¡Œä¿®å¤æ¨¡å¼:")
            print("MIGRATION_FORCE_REPAIR=1 python hot_to_cold_migrator.py")
            return 1

        return 0

if __name__ == "__main__":
    import sys
    rc = asyncio.run(main())
    sys.exit(rc)


