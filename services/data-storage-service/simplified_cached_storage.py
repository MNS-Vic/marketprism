#!/usr/bin/env python3
"""
MarketPrismç®€åŒ–ç‰ˆé«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡
åŸºäºä¿®å¤åçš„Normalizerï¼Œç§»é™¤å¤æ‚çš„å­—æ®µè½¬æ¢é€»è¾‘
"""

import asyncio
import aiohttp
import json
import logging
import signal
import time
from collections import defaultdict, deque
from typing import Dict, List, Any, Optional
import nats

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimplifiedCachedStorageService:
    """ç®€åŒ–ç‰ˆé«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡"""
    
    def __init__(self):
        self.nc = None
        self.running = False
        
        # æ™ºèƒ½ç¼“å­˜é…ç½® - åŸºäºæ•°æ®é¢‘ç‡ä¼˜åŒ–
        self.cache_config = {
            # è¶…é«˜é¢‘æ•°æ® - å¤§æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶
            'orderbooks': {'batch_size': 1000, 'timeout': 2.0, 'max_queue': 10000},
            'trades': {'batch_size': 500, 'timeout': 1.5, 'max_queue': 5000},
            
            # ä¸­é¢‘æ•°æ® - å°æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶ï¼ˆä¿®å¤funding rateé—®é¢˜ï¼‰
            'funding_rates': {'batch_size': 10, 'timeout': 2.0, 'max_queue': 500},
            'open_interests': {'batch_size': 20, 'timeout': 5.0, 'max_queue': 500},
            'lsr_top_positions': {'batch_size': 50, 'timeout': 5.0, 'max_queue': 1000},
            'lsr_all_accounts': {'batch_size': 50, 'timeout': 5.0, 'max_queue': 1000},
            
            # ä½é¢‘æ•°æ® - å°æ‰¹æ¬¡ï¼ŒçŸ­è¶…æ—¶ï¼ˆä¿®å¤liquidationé—®é¢˜ï¼‰
            'liquidations': {'batch_size': 5, 'timeout': 10.0, 'max_queue': 200},
            'volatility_indices': {'batch_size': 10, 'timeout': 10.0, 'max_queue': 300},
        }
        
        # è¡¨æ˜ å°„
        self.table_mapping = {
            'orderbook': 'orderbooks',
            'trade': 'trades',
            'funding_rate': 'funding_rates',
            'open_interest': 'open_interests',
            'liquidation': 'liquidations',
            'lsr_top_position': 'lsr_top_positions',
            'lsr_all_account': 'lsr_all_accounts',
            'volatility_index': 'volatility_indices'
        }
        
        # ç¼“å­˜é˜Ÿåˆ—å’Œé”
        self.data_queues = defaultdict(deque)
        self.last_flush_time = defaultdict(float)
        self.flush_locks = defaultdict(asyncio.Lock)
        
        # ClickHouseé…ç½®
        self.clickhouse_config = {
            'host': 'localhost',
            'port': 8123,
            'database': 'marketprism_hot'
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_received': 0,
            'total_batches': 0,
            'total_inserted': 0,
            'errors': 0,
            'by_type': defaultdict(lambda: {'received': 0, 'batches': 0, 'inserted': 0, 'errors': 0})
        }
        
    async def connect_nats(self):
        """è¿æ¥åˆ°NATSæœåŠ¡å™¨"""
        try:
            self.nc = await nats.connect("nats://localhost:4222")
            logger.info("âœ… è¿æ¥åˆ°NATSæœåŠ¡å™¨æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥NATSå¤±è´¥: {e}")
            return False
    
    async def subscribe_to_data_streams(self):
        """è®¢é˜…æ‰€æœ‰æ•°æ®æµ"""
        try:
            subjects = [
                "orderbook-data.>",
                "trade-data.>", 
                "funding-rate-data.>",
                "open-interest-data.>",
                "liquidation-data.>",
                "lsr-data.>",
                "volatility-index-data.>"
            ]
            
            for subject in subjects:
                await self.nc.subscribe(subject, cb=self.message_handler)
                logger.info(f"âœ… è®¢é˜…æˆåŠŸ: {subject}")
                
            return True
        except Exception as e:
            logger.error(f"âŒ è®¢é˜…å¤±è´¥: {e}")
            return False
    
    async def message_handler(self, msg):
        """æ¶ˆæ¯å¤„ç†å™¨ - ç®€åŒ–ç‰ˆï¼Œå‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–"""
        try:
            data = json.loads(msg.data.decode())
            subject = msg.subject
            
            data_type = self.extract_data_type(subject)
            if not data_type:
                return
                
            # ç›´æ¥æ·»åŠ åˆ°ç¼“å­˜é˜Ÿåˆ—ï¼ˆå‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–ï¼‰
            self.data_queues[data_type].append(data)
            self.stats['total_received'] += 1
            self.stats['by_type'][data_type]['received'] += 1
            
            # æ™ºèƒ½åˆ·æ–°æ£€æŸ¥
            await self.smart_flush_check(data_type)
            
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            self.stats['errors'] += 1
    
    def extract_data_type(self, subject: str) -> Optional[str]:
        """ä»ä¸»é¢˜ä¸­æå–æ•°æ®ç±»å‹"""
        if subject.startswith("orderbook-data"):
            return "orderbook"
        elif subject.startswith("trade-data"):
            return "trade"
        elif subject.startswith("funding-rate-data"):
            return "funding_rate"
        elif subject.startswith("open-interest-data"):
            return "open_interest"
        elif subject.startswith("liquidation-data"):
            return "liquidation"
        elif subject.startswith("lsr-data"):
            if "top-position" in subject:
                return "lsr_top_position"
            elif "all-account" in subject:
                return "lsr_all_account"
        elif subject.startswith("volatility-index-data"):
            return "volatility_index"
        return None
    
    async def smart_flush_check(self, data_type: str):
        """æ™ºèƒ½åˆ·æ–°æ£€æŸ¥"""
        config = self.cache_config.get(data_type, {'batch_size': 100, 'timeout': 5.0, 'max_queue': 1000})
        queue = self.data_queues[data_type]
        current_time = time.time()
        last_flush = self.last_flush_time.get(data_type, 0)
        
        should_flush = (
            len(queue) >= config['batch_size'] or  # è¾¾åˆ°æ‰¹å¤§å°
            (len(queue) > 0 and current_time - last_flush >= config['timeout']) or  # è¶…æ—¶
            len(queue) >= config['max_queue']  # é˜Ÿåˆ—è¿‡å¤§ï¼Œå¼ºåˆ¶åˆ·æ–°
        )
        
        if should_flush and not self.flush_locks[data_type].locked():
            asyncio.create_task(self.flush_batch(data_type))
    
    async def flush_batch(self, data_type: str):
        """æ‰¹é‡åˆ·æ–°æ•°æ®åˆ°ClickHouse"""
        async with self.flush_locks[data_type]:
            queue = self.data_queues[data_type]
            if not queue:
                return
                
            try:
                config = self.cache_config.get(data_type, {'batch_size': 100})
                batch_data = []
                batch_size = min(len(queue), config['batch_size'])
                
                # æå–æ‰¹æ•°æ®
                for _ in range(batch_size):
                    if queue:
                        batch_data.append(queue.popleft())
                
                if not batch_data:
                    return
                    
                # æ‰¹é‡æ’å…¥
                success = await self.batch_insert_to_clickhouse(data_type, batch_data)
                
                if success:
                    self.stats['total_batches'] += 1
                    self.stats['total_inserted'] += len(batch_data)
                    self.stats['by_type'][data_type]['batches'] += 1
                    self.stats['by_type'][data_type]['inserted'] += len(batch_data)
                    
                    # è®°å½•æ‰¹å¤„ç†æˆåŠŸ
                    if len(batch_data) >= 10:  # è®°å½•è¾ƒå¤§æ‰¹æ¬¡
                        logger.info(f"âœ… æ‰¹å¤„ç†æˆåŠŸ: {data_type} - {len(batch_data)}æ¡")
                else:
                    # å¤±è´¥æ—¶é‡æ–°åŠ å…¥é˜Ÿåˆ—å¤´éƒ¨
                    for data in reversed(batch_data):
                        queue.appendleft(data)
                    self.stats['by_type'][data_type]['errors'] += 1
                    logger.error(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {data_type} - {len(batch_data)}æ¡")
                    
                self.last_flush_time[data_type] = time.time()
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†å¼‚å¸¸: {data_type} - {e}")
                self.stats['errors'] += 1
    
    async def batch_insert_to_clickhouse(self, data_type: str, batch_data: List[Dict]) -> bool:
        """æ‰¹é‡æ’å…¥æ•°æ®åˆ°ClickHouse - ç®€åŒ–ç‰ˆï¼Œå‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–"""
        try:
            table_name = self.table_mapping.get(data_type)
            if not table_name:
                return False
                
            # æ„å»ºæ‰¹é‡æ’å…¥SQL - ç®€åŒ–ç‰ˆ
            insert_sql = self.build_simple_batch_sql(table_name, batch_data)
            if not insert_sql:
                return False
                
            # æ‰§è¡Œæ‰¹é‡æ’å…¥
            url = f"http://{self.clickhouse_config['host']}:{self.clickhouse_config['port']}/?database={self.clickhouse_config['database']}"
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=insert_sql) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"âŒ ClickHouseé”™è¯¯: {response.status} - {error_text[:200]}")
                        return False
                    return True
                        
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥å¼‚å¸¸: {e}")
            return False
    
    def build_simple_batch_sql(self, table_name: str, batch_data: List[Dict]) -> Optional[str]:
        """æ„å»ºç®€åŒ–çš„æ‰¹é‡æ’å…¥SQL - å‡è®¾æ•°æ®å·²ç»æ ‡å‡†åŒ–"""
        try:
            if not batch_data:
                return None
                
            # ä½¿ç”¨ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µä½œä¸ºæ¨¡æ¿
            first_record = batch_data[0]
            fields = list(first_record.keys())
            
            # æ„å»ºVALUES
            values_list = []
            for data in batch_data:
                values = []
                for field in fields:
                    value = data.get(field)
                    if value is None:
                        values.append('NULL')
                    elif isinstance(value, str):
                        # è½¬ä¹‰å­—ç¬¦ä¸²
                        escaped_value = value.replace("\\", "\\\\").replace("'", "\\'")
                        values.append(f"'{escaped_value}'")
                    elif isinstance(value, (dict, list)):
                        # JSONå­—æ®µ
                        json_str = json.dumps(value).replace("\\", "\\\\").replace("'", "\\'")
                        values.append(f"'{json_str}'")
                    elif isinstance(value, bool):
                        values.append('1' if value else '0')
                    else:
                        values.append(str(value))
                
                values_list.append(f"({', '.join(values)})")
            
            # æ„å»ºå®Œæ•´SQL
            fields_str = ', '.join(fields)
            values_str = ', '.join(values_list)
            sql = f"INSERT INTO {table_name} ({fields_str}) VALUES {values_str}"
            
            return sql
            
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæ‰¹é‡SQLå¤±è´¥: {e}")
            return None
    
    async def periodic_maintenance(self):
        """å®šæœŸç»´æŠ¤ä»»åŠ¡"""
        while self.running:
            try:
                current_time = time.time()
                
                # æ£€æŸ¥è¶…æ—¶çš„é˜Ÿåˆ—
                for data_type in list(self.data_queues.keys()):
                    config = self.cache_config.get(data_type, {'timeout': 5.0})
                    queue = self.data_queues[data_type]
                    last_flush = self.last_flush_time.get(data_type, 0)
                    
                    if len(queue) > 0 and current_time - last_flush >= config['timeout']:
                        if not self.flush_locks[data_type].locked():
                            asyncio.create_task(self.flush_batch(data_type))
                
                await asyncio.sleep(0.5)  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸç»´æŠ¤å¼‚å¸¸: {e}")
                await asyncio.sleep(5)
    
    async def stats_reporter(self):
        """ç»Ÿè®¡ä¿¡æ¯æŠ¥å‘Š"""
        while self.running:
            try:
                await asyncio.sleep(60)  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡
                
                # è®¡ç®—é˜Ÿåˆ—å¤§å°
                queue_sizes = {}
                total_queue_size = 0
                for dt, q in self.data_queues.items():
                    size = len(q)
                    if size > 0:
                        queue_sizes[dt] = size
                        total_queue_size += size
                
                # è®¡ç®—æ‰¹å¤„ç†æ•ˆç‡
                total_received = sum(stats['received'] for stats in self.stats['by_type'].values())
                total_inserted = sum(stats['inserted'] for stats in self.stats['by_type'].values())
                efficiency = (total_inserted / total_received * 100) if total_received > 0 else 0
                
                logger.info(f"ğŸ“Š ç®€åŒ–ç‰ˆæ€§èƒ½ç»Ÿè®¡ (è¿‡å»1åˆ†é’Ÿ):")
                logger.info(f"   æ¥æ”¶: {total_received} | æ’å…¥: {total_inserted} | æ•ˆç‡: {efficiency:.1f}%")
                logger.info(f"   æ‰¹æ¬¡: {self.stats['total_batches']} | é”™è¯¯: {self.stats['errors']}")
                if queue_sizes:
                    logger.info(f"   é˜Ÿåˆ—: {queue_sizes} (æ€»è®¡: {total_queue_size})")
                
                # æ˜¾ç¤ºå„æ•°æ®ç±»å‹ç»Ÿè®¡
                for data_type, stats in self.stats['by_type'].items():
                    if stats['received'] > 0:
                        logger.info(f"   {data_type}: æ¥æ”¶{stats['received']} æ’å…¥{stats['inserted']} æ‰¹æ¬¡{stats['batches']} é”™è¯¯{stats['errors']}")
                
                # é‡ç½®è®¡æ•°å™¨
                for stats in self.stats['by_type'].values():
                    stats['received'] = 0
                    stats['inserted'] = 0
                    stats['batches'] = 0
                    stats['errors'] = 0
                self.stats['total_batches'] = 0
                self.stats['errors'] = 0
                
            except Exception as e:
                logger.error(f"âŒ ç»Ÿè®¡æŠ¥å‘Šå¼‚å¸¸: {e}")
    
    async def start(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info("ğŸš€ å¯åŠ¨ç®€åŒ–ç‰ˆé«˜æ€§èƒ½ç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")
        
        if not await self.connect_nats():
            return False
            
        if not await self.subscribe_to_data_streams():
            return False
            
        self.running = True
        
        # å¯åŠ¨åå°ä»»åŠ¡
        tasks = [
            asyncio.create_task(self.periodic_maintenance()),
            asyncio.create_task(self.stats_reporter())
        ]
        
        logger.info("âœ… ç®€åŒ–ç‰ˆç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å¯åŠ¨æˆåŠŸ")
        logger.info("ğŸ“‹ ç¼“å­˜é…ç½®:")
        for data_type, config in self.cache_config.items():
            logger.info(f"   {data_type}: æ‰¹æ¬¡{config['batch_size']}, è¶…æ—¶{config['timeout']}s")
        
        try:
            await asyncio.gather(*tasks)
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            await self.stop()
    
    async def stop(self):
        """åœæ­¢æœåŠ¡"""
        logger.info("ğŸ›‘ åœæ­¢ç®€åŒ–ç‰ˆç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡...")
        self.running = False
        
        # åˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        logger.info("ğŸ“¤ åˆ·æ–°å‰©ä½™ç¼“å­˜æ•°æ®...")
        for data_type in list(self.data_queues.keys()):
            if self.data_queues[data_type]:
                await self.flush_batch(data_type)
        
        if self.nc:
            await self.nc.close()
        
        logger.info("âœ… ç®€åŒ–ç‰ˆç¼“å­˜çƒ­å­˜å‚¨æœåŠ¡å·²åœæ­¢")

async def main():
    service = SimplifiedCachedStorageService()
    
    def signal_handler(signum, frame):
        logger.info(f"æ”¶åˆ°ä¿¡å· {signum}")
        asyncio.create_task(service.stop())
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    await service.start()

if __name__ == "__main__":
    asyncio.run(main())
