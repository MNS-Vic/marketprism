"""
MarketPrism æ•°æ®åŒæ­¥ä»»åŠ¡
ä¸task-workeræœåŠ¡é›†æˆï¼Œå®ç°å®šæ—¶æ•°æ®åŒæ­¥è°ƒåº¦
"""

import asyncio
import json
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any
import structlog

from core.storage.tiered_storage_manager import TieredStorageManager, TierConfig, StorageTier


class DataSyncTask:
    """æ•°æ®åŒæ­¥ä»»åŠ¡"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®åŒæ­¥ä»»åŠ¡
        
        Args:
            config: ä»»åŠ¡é…ç½®
        """
        self.config = config
        self.logger = structlog.get_logger("services.data_storage.tasks.data_sync")
        
        # å­˜å‚¨ç®¡ç†å™¨
        self.storage_manager: Optional[TieredStorageManager] = None
        
        # ä»»åŠ¡é…ç½®
        self.sync_config = config.get('sync', {})
        self.hot_storage_config = config.get('hot_storage', {})
        self.cold_storage_config = config.get('cold_storage', {})
    
    async def initialize(self):
        """åˆå§‹åŒ–ä»»åŠ¡"""
        try:
            self.logger.info("ğŸš€ åˆå§‹åŒ–æ•°æ®åŒæ­¥ä»»åŠ¡")
            
            # åˆ›å»ºçƒ­ç«¯é…ç½®
            hot_config = TierConfig(
                tier=StorageTier.HOT,
                clickhouse_host=self.hot_storage_config.get('clickhouse_host', 'localhost'),
                clickhouse_port=self.hot_storage_config.get('clickhouse_port', 9000),
                clickhouse_user=self.hot_storage_config.get('clickhouse_user', 'default'),
                clickhouse_password=self.hot_storage_config.get('clickhouse_password', ''),
                clickhouse_database=self.hot_storage_config.get('clickhouse_database', 'marketprism_hot'),
                retention_days=self.hot_storage_config.get('retention_days', 3),
                batch_size=self.hot_storage_config.get('batch_size', 1000),
                flush_interval=self.hot_storage_config.get('flush_interval', 5)
            )
            
            # åˆ›å»ºå†·ç«¯é…ç½®
            cold_config = TierConfig(
                tier=StorageTier.COLD,
                clickhouse_host=self.cold_storage_config.get('clickhouse_host', 'localhost'),
                clickhouse_port=self.cold_storage_config.get('clickhouse_port', 9000),
                clickhouse_user=self.cold_storage_config.get('clickhouse_user', 'default'),
                clickhouse_password=self.cold_storage_config.get('clickhouse_password', ''),
                clickhouse_database=self.cold_storage_config.get('clickhouse_database', 'marketprism_cold'),
                retention_days=self.cold_storage_config.get('retention_days', 365),
                batch_size=self.cold_storage_config.get('batch_size', 5000),
                flush_interval=self.cold_storage_config.get('flush_interval', 30)
            )
            
            # åˆå§‹åŒ–åˆ†å±‚å­˜å‚¨ç®¡ç†å™¨
            self.storage_manager = TieredStorageManager(hot_config, cold_config)
            await self.storage_manager.initialize()
            
            self.logger.info("âœ… æ•°æ®åŒæ­¥ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error("âŒ æ•°æ®åŒæ­¥ä»»åŠ¡åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise
    
    async def execute_sync_task(self, task_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡
        
        Args:
            task_params: ä»»åŠ¡å‚æ•°
                - data_type: æ•°æ®ç±»å‹
                - exchange: äº¤æ˜“æ‰€
                - symbol: äº¤æ˜“å¯¹
                - start_time: å¼€å§‹æ—¶é—´
                - end_time: ç»“æŸæ—¶é—´
                - task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        try:
            task_id = task_params.get('task_id', f"sync_{int(datetime.now().timestamp())}")
            data_type = task_params['data_type']
            exchange = task_params['exchange']
            symbol = task_params['symbol']
            start_time = datetime.fromisoformat(task_params['start_time'])
            end_time = datetime.fromisoformat(task_params['end_time'])
            
            self.logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡",
                           task_id=task_id,
                           data_type=data_type,
                           exchange=exchange,
                           symbol=symbol)
            
            # è°ƒåº¦æ•°æ®ä¼ è¾“ä»»åŠ¡
            transfer_task_id = await self.storage_manager.schedule_data_transfer(
                data_type, exchange, symbol, start_time, end_time
            )
            
            # ç­‰å¾…ä¼ è¾“å®Œæˆ
            await self._wait_for_transfer_completion(transfer_task_id)
            
            # è·å–ä¼ è¾“ç»“æœ
            transfer_status = self.storage_manager.get_transfer_task_status(transfer_task_id)
            
            if transfer_status and transfer_status['status'] == 'completed':
                result = {
                    "status": "success",
                    "task_id": task_id,
                    "transfer_task_id": transfer_task_id,
                    "records_synced": transfer_status['records_count'],
                    "completed_at": datetime.now(timezone.utc).isoformat()
                }
                
                self.logger.info("âœ… æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆ",
                               task_id=task_id,
                               records_synced=transfer_status['records_count'])
            else:
                result = {
                    "status": "failed",
                    "task_id": task_id,
                    "transfer_task_id": transfer_task_id,
                    "error": transfer_status.get('error_message', 'ä¼ è¾“ä»»åŠ¡å¤±è´¥') if transfer_status else 'ä¼ è¾“ä»»åŠ¡çŠ¶æ€æœªçŸ¥',
                    "completed_at": datetime.now(timezone.utc).isoformat()
                }
                
                self.logger.error("âŒ æ•°æ®åŒæ­¥ä»»åŠ¡å¤±è´¥",
                                task_id=task_id,
                                error=result['error'])
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error("âŒ æ‰§è¡Œæ•°æ®åŒæ­¥ä»»åŠ¡å¼‚å¸¸", 
                            task_id=task_params.get('task_id', 'unknown'),
                            error=error_msg)
            
            return {
                "status": "error",
                "task_id": task_params.get('task_id', 'unknown'),
                "error": error_msg,
                "completed_at": datetime.now(timezone.utc).isoformat()
            }
    
    async def execute_cleanup_task(self, task_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®æ¸…ç†ä»»åŠ¡
        
        Args:
            task_params: ä»»åŠ¡å‚æ•°
                - task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        try:
            task_id = task_params.get('task_id', f"cleanup_{int(datetime.now().timestamp())}")
            
            self.logger.info("ğŸ§¹ å¼€å§‹æ‰§è¡Œæ•°æ®æ¸…ç†ä»»åŠ¡", task_id=task_id)
            
            # æ‰§è¡Œçƒ­ç«¯æ•°æ®æ¸…ç†
            cleanup_summary = await self.storage_manager.cleanup_expired_hot_data()
            
            result = {
                "status": "success",
                "task_id": task_id,
                "records_deleted": cleanup_summary.get("total_records_deleted", 0),
                "tables_cleaned": cleanup_summary.get("tables_cleaned", {}),
                "completed_at": datetime.now(timezone.utc).isoformat()
            }
            
            self.logger.info("âœ… æ•°æ®æ¸…ç†ä»»åŠ¡å®Œæˆ",
                           task_id=task_id,
                           records_deleted=result['records_deleted'])
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error("âŒ æ‰§è¡Œæ•°æ®æ¸…ç†ä»»åŠ¡å¼‚å¸¸",
                            task_id=task_params.get('task_id', 'unknown'),
                            error=error_msg)
            
            return {
                "status": "error",
                "task_id": task_params.get('task_id', 'unknown'),
                "error": error_msg,
                "completed_at": datetime.now(timezone.utc).isoformat()
            }
    
    async def execute_batch_sync_task(self, task_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ‰¹é‡æ•°æ®åŒæ­¥ä»»åŠ¡
        
        Args:
            task_params: ä»»åŠ¡å‚æ•°
                - lookback_hours: å›æº¯æ—¶é—´ï¼ˆå°æ—¶ï¼‰
                - data_types: æ•°æ®ç±»å‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                - exchanges: äº¤æ˜“æ‰€åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                - task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        try:
            task_id = task_params.get('task_id', f"batch_sync_{int(datetime.now().timestamp())}")
            lookback_hours = task_params.get('lookback_hours', 24)
            data_types = task_params.get('data_types')
            exchanges = task_params.get('exchanges')
            
            self.logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œæ‰¹é‡æ•°æ®åŒæ­¥ä»»åŠ¡",
                           task_id=task_id,
                           lookback_hours=lookback_hours)
            
            # è°ƒåº¦è‡ªåŠ¨ä¼ è¾“ä»»åŠ¡
            transfer_task_ids = await self.storage_manager.auto_schedule_transfers(
                data_types=data_types,
                exchanges=exchanges,
                lookback_hours=lookback_hours
            )
            
            # ç­‰å¾…æ‰€æœ‰ä¼ è¾“å®Œæˆ
            await self._wait_for_multiple_transfers_completion(transfer_task_ids)
            
            # ç»Ÿè®¡ç»“æœ
            completed_count = 0
            failed_count = 0
            total_records = 0
            
            for transfer_task_id in transfer_task_ids:
                status = self.storage_manager.get_transfer_task_status(transfer_task_id)
                if status:
                    if status['status'] == 'completed':
                        completed_count += 1
                        total_records += status['records_count']
                    elif status['status'] == 'failed':
                        failed_count += 1
            
            result = {
                "status": "success" if failed_count == 0 else "partial",
                "task_id": task_id,
                "total_transfers": len(transfer_task_ids),
                "completed_transfers": completed_count,
                "failed_transfers": failed_count,
                "total_records_synced": total_records,
                "transfer_task_ids": transfer_task_ids,
                "completed_at": datetime.now(timezone.utc).isoformat()
            }
            
            self.logger.info("âœ… æ‰¹é‡æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆ",
                           task_id=task_id,
                           completed_transfers=completed_count,
                           failed_transfers=failed_count,
                           total_records=total_records)
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            self.logger.error("âŒ æ‰§è¡Œæ‰¹é‡æ•°æ®åŒæ­¥ä»»åŠ¡å¼‚å¸¸",
                            task_id=task_params.get('task_id', 'unknown'),
                            error=error_msg)
            
            return {
                "status": "error",
                "task_id": task_params.get('task_id', 'unknown'),
                "error": error_msg,
                "completed_at": datetime.now(timezone.utc).isoformat()
            }
    
    async def _wait_for_transfer_completion(self, transfer_task_id: str, timeout_minutes: int = 30):
        """ç­‰å¾…å•ä¸ªä¼ è¾“ä»»åŠ¡å®Œæˆ"""
        timeout_seconds = timeout_minutes * 60
        start_time = datetime.now(timezone.utc)
        
        while (datetime.now(timezone.utc) - start_time).total_seconds() < timeout_seconds:
            status = self.storage_manager.get_transfer_task_status(transfer_task_id)
            if status and status['status'] in ['completed', 'failed']:
                return
            
            await asyncio.sleep(10)  # 10ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # è¶…æ—¶
        self.logger.warning("âš ï¸ ä¼ è¾“ä»»åŠ¡ç­‰å¾…è¶…æ—¶",
                          transfer_task_id=transfer_task_id,
                          timeout_minutes=timeout_minutes)
    
    async def _wait_for_multiple_transfers_completion(self, transfer_task_ids: List[str], timeout_minutes: int = 60):
        """ç­‰å¾…å¤šä¸ªä¼ è¾“ä»»åŠ¡å®Œæˆ"""
        timeout_seconds = timeout_minutes * 60
        start_time = datetime.now(timezone.utc)
        
        while (datetime.now(timezone.utc) - start_time).total_seconds() < timeout_seconds:
            pending_tasks = []
            
            for task_id in transfer_task_ids:
                status = self.storage_manager.get_transfer_task_status(task_id)
                if status and status['status'] in ['pending', 'running']:
                    pending_tasks.append(task_id)
            
            if not pending_tasks:
                return  # æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†
            
            await asyncio.sleep(30)  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
        
        # è¶…æ—¶
        self.logger.warning("âš ï¸ æ‰¹é‡ä¼ è¾“ä»»åŠ¡ç­‰å¾…è¶…æ—¶",
                          total_tasks=len(transfer_task_ids),
                          timeout_minutes=timeout_minutes)
    
    async def close(self):
        """å…³é—­ä»»åŠ¡"""
        try:
            if self.storage_manager:
                await self.storage_manager.close()
                self.logger.info("âœ… æ•°æ®åŒæ­¥ä»»åŠ¡å·²å…³é—­")
        except Exception as e:
            self.logger.error("âŒ å…³é—­æ•°æ®åŒæ­¥ä»»åŠ¡å¤±è´¥", error=str(e))
